{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNKTImrOGqPhWVsc0akNwK4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Hackathon - The Next Purchasek\n","### Recommendation System\n","<hr style=\" border:none; height:3px;\">"],"metadata":{"id":"S5Cu_txRvwU5"}},{"cell_type":"markdown","source":["## Stage 1: Multi-Channel Recall"],"metadata":{"id":"o2g2iI9rzIGD"}},{"cell_type":"markdown","source":["Retrieval employs multiple parallel strategies—collaborative filtering, embedding-based matching, sequence modeling, and popularity-based methods—to rapidly surface thousands of candidates from millions of items, balancing precision with diversity by leveraging different signals of user-item affinity."],"metadata":{"id":"9KDts1rgzLrq"}},{"cell_type":"markdown","source":["### 1) Embedding-based Recall (Two-Tower Model)"],"metadata":{"id":"js_aJtpP1qxm"}},{"cell_type":"markdown","source":["The two-tower architecture separately encodes users and items into dense vector representations within a shared embedding space, enabling efficient similarity matching through approximate nearest neighbor search to identify items whose learned embeddings exhibit high affinity with the user's latent preference vector.\n","\n","user features + item features → separate neural networks → user vector & item vectors → cosine similarity in shared space → retrieve top-K nearest items → candidates matching learned preference patterns."],"metadata":{"id":"rJYC0bJsFJzz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive/Hackathon/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7ewuOFJ-wXa","executionInfo":{"status":"ok","timestamp":1770750983659,"user_tz":-60,"elapsed":14650,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"75f207d5-9dea-4059-c30c-53536d517dac"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Hackathon\n"]}]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","\n","# Load environment variables\n","load_dotenv()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lO02rc2S_WX9","executionInfo":{"status":"ok","timestamp":1770751096896,"user_tz":-60,"elapsed":313,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"466ebce9-b81b-46b6-e217-161a49bc6916"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics.pairwise import cosine_similarity\n","from typing import List, Dict, Tuple\n","import psycopg2\n","from psycopg2.extras import execute_values\n","import os\n","\n","class TwoTowerRecall:\n","\n","    def __init__(self, user_feature_dim: int, item_feature_dim: int, embedding_dim: int = 64):\n","        \"\"\"\n","        Initialize two-tower model.\n","        Args:\n","            user_feature_dim: Dimension of user features\n","            item_feature_dim: Dimension of item features\n","            embedding_dim: Output embedding dimension (default: 64)\n","        \"\"\"\n","\n","        self.embedding_dim = embedding_dim\n","        self.user_tower = self._build_tower(user_feature_dim, embedding_dim)\n","        self.item_tower = self._build_tower(item_feature_dim, embedding_dim)\n","        self.optimizer = torch.optim.Adam(\n","            list(self.user_tower.parameters()) + list(self.item_tower.parameters()),\n","            lr=0.001\n","        )\n","\n","    def _build_tower(self, input_dim: int, output_dim: int) -> nn.Module:\n","        \"\"\"\n","        Build tower network.\n","        Args:\n","            input_dim: Input feature dimension\n","            output_dim: Output embedding dimension\n","        Returns:\n","            Neural network tower\n","        \"\"\"\n","\n","        return nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, output_dim)\n","        )\n","\n","    def forward(self, user_features: torch.Tensor, item_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Forward pass through both towers.\n","        Args:\n","            user_features: User feature tensor [batch_size, user_feature_dim]\n","            item_features: Item feature tensor [batch_size, item_feature_dim]\n","        Returns:\n","            Tuple of (user_embeddings, item_embeddings), both L2-normalized\n","        \"\"\"\n","\n","        user_emb = self.user_tower(user_features)\n","        item_emb = self.item_tower(item_features)\n","\n","        # L2 normalization\n","        user_emb = torch.nn.functional.normalize(user_emb, p=2, dim=1)\n","        item_emb = torch.nn.functional.normalize(item_emb, p=2, dim=1)\n","\n","        return user_emb, item_emb\n","\n","    def train_step(self, user_features: torch.Tensor, item_features: torch.Tensor, labels: torch.Tensor) -> float:\n","        \"\"\"\n","        Single training step.\n","        Args:\n","            user_features: User features [batch_size, user_feature_dim]\n","            item_features: Item features [batch_size, item_feature_dim]\n","            labels: Binary labels [batch_size] (1 for positive, 0 for negative)\n","        Returns:\n","            Loss value\n","        \"\"\"\n","\n","        self.user_tower.train()\n","        self.item_tower.train()\n","        self.optimizer.zero_grad()\n","\n","        user_emb, item_emb = self.forward(user_features, item_features)\n","\n","        # Compute similarity scores\n","        scores = (user_emb * item_emb).sum(dim=1)\n","\n","        # Binary cross-entropy loss\n","        loss = nn.BCEWithLogitsLoss()(scores, labels.float())\n","\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","    def encode_users(self, user_features: torch.Tensor) -> np.ndarray:\n","        \"\"\"\n","        Encode users into embeddings.\n","        Args:\n","            user_features: User features [num_users, user_feature_dim]\n","        Returns:\n","            User embeddings [num_users, embedding_dim]\n","        \"\"\"\n","\n","        self.user_tower.eval()\n","        with torch.no_grad():\n","            user_emb = self.user_tower(user_features)\n","            user_emb = torch.nn.functional.normalize(user_emb, p=2, dim=1)\n","        self.user_tower.train()\n","        return user_emb.cpu().numpy()\n","\n","    def encode_items(self, item_features: torch.Tensor) -> np.ndarray:\n","        \"\"\"\n","        Encode items into embeddings.\n","        Args:\n","            item_features: Item features [num_items, item_feature_dim]\n","        Returns:\n","            Item embeddings [num_items, embedding_dim]\n","        \"\"\"\n","\n","        self.item_tower.eval()\n","        with torch.no_grad():\n","            item_emb = self.item_tower(item_features)\n","            item_emb = torch.nn.functional.normalize(item_emb, p=2, dim=1)\n","        self.item_tower.train()\n","        return item_emb.cpu().numpy()\n","\n","    def recall(self, user_embedding: np.ndarray, item_embeddings: np.ndarray, top_k: int = 500) -> List[int]:\n","        \"\"\"\n","        Recall top-K items for user.\n","        Args:\n","            user_embedding: User embedding vector [embedding_dim]\n","            item_embeddings: All item embeddings [num_items, embedding_dim]\n","            top_k: Number of items to recall\n","        Returns:\n","            List of top-K item indices\n","        \"\"\"\n","\n","        similarities = np.dot(item_embeddings, user_embedding)\n","        top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n","        return top_k_indices.tolist()"],"metadata":{"id":"2BT02RYF0zKx","executionInfo":{"status":"ok","timestamp":1770753283625,"user_tz":-60,"elapsed":4,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","# 0. Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# 1. Load training data from database\n","try:\n","    with psycopg2.connect(\n","        host=os.getenv('DB_HOST'),\n","        port=os.getenv('DB_PORT'),\n","        dbname=os.getenv('DB_NAME'),\n","        user=os.getenv('DB_USER'),\n","        password=os.getenv('DB_PASSWORD'),\n","        sslmode='require'\n","    ) as conn:\n","        with conn.cursor() as cursor:\n","            cursor.execute(\"\"\"\n","            SELECT \"ClientID\", \"ProductID\"\n","            FROM user_item_interactions\n","            WHERE \"PurchaseCount\" > 0\n","            \"\"\")\n","            positive_samples = cursor.fetchall()\n","\n","            cursor.execute(\"\"\"\n","            SELECT \"ClientID\", \"TotalPurchases\", \"TotalSpendEuro\", \"AvgOrderValue\",\n","                   \"DaysSinceLastPurchase\", \"PurchaseFrequency\", \"UniqueProductsBought\",\n","                   \"TotalQuantity\", \"Age\"\n","            FROM clients\n","            \"\"\")\n","            user_features_data = cursor.fetchall()\n","\n","            cursor.execute(\"\"\"\n","            SELECT \"ProductID\", \"TotalSales\", \"TotalQuantitySold\", \"Sales7d\",\n","                   \"Sales30d\", \"AvgPrice\", \"TotalRevenue\", \"UniqueBuyers\",\n","                   \"AvgQuantityPerOrder\", \"TotalStockQuantity\", \"StockCountries\"\n","            FROM products\n","            \"\"\")\n","            item_features_data = cursor.fetchall()\n","            print(\"Training data loaded successfully!\")\n","except Exception as e:\n","    print(f\"Error loading training data: {e}\")\n","    raise\n","\n","# 2. Prepare features\n","user_feature_dict = {uid: [0 if x is None else float(x) for x in features]\n","                     for uid, *features in user_features_data}\n","item_feature_dict = {iid: [0 if x is None else float(x) for x in features]\n","                     for iid, *features in item_features_data}\n","\n","user_feature_dim = len(user_features_data[0]) - 1\n","item_feature_dim = len(item_features_data[0]) - 1\n","\n","print(f\"Total positive samples: {len(positive_samples)}\")\n","\n","# 3. Generate negative samples\n","print(\"Generating negative samples...\")\n","all_item_ids = np.array(list(item_feature_dict.keys()))\n","positive_set = set(positive_samples)\n","training_samples = []\n","\n","# Pre-generate 10 candidate negative samples for each positive sample\n","neg_candidates = np.random.choice(all_item_ids, size=(len(positive_samples), 10), replace=True)\n","\n","for idx, (user_id, item_id) in enumerate(positive_samples):\n","    if user_id not in user_feature_dict or item_id not in item_feature_dict:\n","        continue\n","\n","    training_samples.append((user_id, item_id, 1))\n","\n","    # Find first candidate not in positive_set\n","    for neg_item in neg_candidates[idx]:\n","        if (user_id, neg_item) not in positive_set:\n","            training_samples.append((user_id, neg_item, 0))\n","            break\n","\n","print(f\"Total training samples: {len(training_samples)}\")\n","\n","# 4. Prepare tensors\n","user_tensor_all = torch.FloatTensor([user_feature_dict[u] for u, _, _ in training_samples])\n","item_tensor_all = torch.FloatTensor([item_feature_dict[i] for _, i, _ in training_samples])\n","label_tensor_all = torch.FloatTensor([l for _, _, l in training_samples])\n","\n","dataset = TensorDataset(user_tensor_all, item_tensor_all, label_tensor_all)\n","loader = DataLoader(\n","    dataset,\n","    batch_size=256,\n","    shuffle=True\n",")\n","\n","# 5. Initialize model\n","model = TwoTowerRecall(user_feature_dim, item_feature_dim, embedding_dim=64)\n","model.user_tower = model.user_tower.to(device)\n","model.item_tower = model.item_tower.to(device)\n","\n","# 6. Training loop\n","epochs = 50\n","patience = 5\n","best_loss = float('inf')\n","patience_counter = 0\n","\n","for epoch in range(epochs):\n","    epoch_loss = 0\n","    num_batches = 0\n","\n","    for batch_idx, (user_tensor, item_tensor, labels_tensor) in enumerate(loader):\n","        user_tensor = user_tensor.to(device)\n","        item_tensor = item_tensor.to(device)\n","        labels_tensor = labels_tensor.to(device)\n","\n","        loss = model.train_step(user_tensor, item_tensor, labels_tensor)\n","        epoch_loss += loss\n","        num_batches += 1\n","\n","        if batch_idx % 100 == 0:\n","            print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx}/{len(loader)}\")\n","\n","    avg_loss = epoch_loss / num_batches\n","    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","    # Early stopping\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        patience_counter = 0\n","        # Save best model\n","        torch.save({\n","            'user_tower': model.user_tower.state_dict(),\n","            'item_tower': model.item_tower.state_dict()\n","        }, 'two_tower_model_best.pth')\n","        print(f\"Best model saved with loss: {best_loss:.4f}\")\n","    else:\n","        patience_counter += 1\n","        print(f\"No improvement for {patience_counter} epoch(s)\")\n","\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n","            break\n","\n","# 7. Load best model\n","checkpoint = torch.load('two_tower_model_best.pth')\n","model.user_tower.load_state_dict(checkpoint['user_tower'])\n","model.item_tower.load_state_dict(checkpoint['item_tower'])\n","print(\"Best model loaded!\")\n","\n","# 8. Save final model\n","torch.save({\n","    'user_tower': model.user_tower.state_dict(),\n","    'item_tower': model.item_tower.state_dict()\n","}, 'two_tower_model.pth')\n","print(\"Model saved successfully!\")\n","\n","# 9. Generate embeddings\n","all_user_ids = list(user_feature_dict.keys())\n","all_item_ids = list(item_feature_dict.keys())\n","\n","user_features_tensor = torch.FloatTensor([user_feature_dict[uid] for uid in all_user_ids]).to(device)\n","item_features_tensor = torch.FloatTensor([item_feature_dict[iid] for iid in all_item_ids]).to(device)\n","\n","user_embeddings = model.encode_users(user_features_tensor)\n","item_embeddings = model.encode_items(item_features_tensor)\n","\n","# 10. Save embeddings to database\n","try:\n","    with psycopg2.connect(\n","        host=os.getenv('DB_HOST'),\n","        port=os.getenv('DB_PORT'),\n","        dbname=os.getenv('DB_NAME'),\n","        user=os.getenv('DB_USER'),\n","        password=os.getenv('DB_PASSWORD'),\n","        sslmode='require'\n","    ) as conn:\n","        with conn.cursor() as cursor:\n","            cursor.execute(\"TRUNCATE TABLE user_embeddings\")\n","            cursor.execute(\"TRUNCATE TABLE item_embeddings\")\n","\n","            user_data = [(int(uid), user_embeddings[i].tolist(), 'NOW()')\n","                        for i, uid in enumerate(all_user_ids)]\n","            execute_values(cursor, \"\"\"\n","            INSERT INTO user_embeddings (\"ClientID\", \"Embedding\", \"UpdatedAt\")\n","            VALUES %s\n","            \"\"\", user_data)\n","\n","            item_data = [(int(iid), item_embeddings[i].tolist(), 'NOW()')\n","                        for i, iid in enumerate(all_item_ids)]\n","            execute_values(cursor, \"\"\"\n","            INSERT INTO item_embeddings (\"ProductID\", \"Embedding\", \"UpdatedAt\")\n","            VALUES %s\n","            \"\"\", item_data)\n","\n","            conn.commit()\n","            print(\"Embeddings saved successfully!\")\n","except Exception as e:\n","    print(f\"Error saving embeddings: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwO9UlLD44qX","executionInfo":{"status":"ok","timestamp":1770760509455,"user_tz":-60,"elapsed":2595852,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"b167b24c-50b8-428f-cff8-be875b22010a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training data loaded successfully!\n","Total positive samples: 976274\n","Generating negative samples...\n","Total training samples: 1952548\n","Epoch 1/50, Batch 0/7628\n","Epoch 1/50, Batch 100/7628\n","Epoch 1/50, Batch 200/7628\n","Epoch 1/50, Batch 300/7628\n","Epoch 1/50, Batch 400/7628\n","Epoch 1/50, Batch 500/7628\n","Epoch 1/50, Batch 600/7628\n","Epoch 1/50, Batch 700/7628\n","Epoch 1/50, Batch 800/7628\n","Epoch 1/50, Batch 900/7628\n","Epoch 1/50, Batch 1000/7628\n","Epoch 1/50, Batch 1100/7628\n","Epoch 1/50, Batch 1200/7628\n","Epoch 1/50, Batch 1300/7628\n","Epoch 1/50, Batch 1400/7628\n","Epoch 1/50, Batch 1500/7628\n","Epoch 1/50, Batch 1600/7628\n","Epoch 1/50, Batch 1700/7628\n","Epoch 1/50, Batch 1800/7628\n","Epoch 1/50, Batch 1900/7628\n","Epoch 1/50, Batch 2000/7628\n","Epoch 1/50, Batch 2100/7628\n","Epoch 1/50, Batch 2200/7628\n","Epoch 1/50, Batch 2300/7628\n","Epoch 1/50, Batch 2400/7628\n","Epoch 1/50, Batch 2500/7628\n","Epoch 1/50, Batch 2600/7628\n","Epoch 1/50, Batch 2700/7628\n","Epoch 1/50, Batch 2800/7628\n","Epoch 1/50, Batch 2900/7628\n","Epoch 1/50, Batch 3000/7628\n","Epoch 1/50, Batch 3100/7628\n","Epoch 1/50, Batch 3200/7628\n","Epoch 1/50, Batch 3300/7628\n","Epoch 1/50, Batch 3400/7628\n","Epoch 1/50, Batch 3500/7628\n","Epoch 1/50, Batch 3600/7628\n","Epoch 1/50, Batch 3700/7628\n","Epoch 1/50, Batch 3800/7628\n","Epoch 1/50, Batch 3900/7628\n","Epoch 1/50, Batch 4000/7628\n","Epoch 1/50, Batch 4100/7628\n","Epoch 1/50, Batch 4200/7628\n","Epoch 1/50, Batch 4300/7628\n","Epoch 1/50, Batch 4400/7628\n","Epoch 1/50, Batch 4500/7628\n","Epoch 1/50, Batch 4600/7628\n","Epoch 1/50, Batch 4700/7628\n","Epoch 1/50, Batch 4800/7628\n","Epoch 1/50, Batch 4900/7628\n","Epoch 1/50, Batch 5000/7628\n","Epoch 1/50, Batch 5100/7628\n","Epoch 1/50, Batch 5200/7628\n","Epoch 1/50, Batch 5300/7628\n","Epoch 1/50, Batch 5400/7628\n","Epoch 1/50, Batch 5500/7628\n","Epoch 1/50, Batch 5600/7628\n","Epoch 1/50, Batch 5700/7628\n","Epoch 1/50, Batch 5800/7628\n","Epoch 1/50, Batch 5900/7628\n","Epoch 1/50, Batch 6000/7628\n","Epoch 1/50, Batch 6100/7628\n","Epoch 1/50, Batch 6200/7628\n","Epoch 1/50, Batch 6300/7628\n","Epoch 1/50, Batch 6400/7628\n","Epoch 1/50, Batch 6500/7628\n","Epoch 1/50, Batch 6600/7628\n","Epoch 1/50, Batch 6700/7628\n","Epoch 1/50, Batch 6800/7628\n","Epoch 1/50, Batch 6900/7628\n","Epoch 1/50, Batch 7000/7628\n","Epoch 1/50, Batch 7100/7628\n","Epoch 1/50, Batch 7200/7628\n","Epoch 1/50, Batch 7300/7628\n","Epoch 1/50, Batch 7400/7628\n","Epoch 1/50, Batch 7500/7628\n","Epoch 1/50, Batch 7600/7628\n","Epoch 1/50, Loss: 0.4492\n","Best model saved with loss: 0.4492\n","Epoch 2/50, Batch 0/7628\n","Epoch 2/50, Batch 100/7628\n","Epoch 2/50, Batch 200/7628\n","Epoch 2/50, Batch 300/7628\n","Epoch 2/50, Batch 400/7628\n","Epoch 2/50, Batch 500/7628\n","Epoch 2/50, Batch 600/7628\n","Epoch 2/50, Batch 700/7628\n","Epoch 2/50, Batch 800/7628\n","Epoch 2/50, Batch 900/7628\n","Epoch 2/50, Batch 1000/7628\n","Epoch 2/50, Batch 1100/7628\n","Epoch 2/50, Batch 1200/7628\n","Epoch 2/50, Batch 1300/7628\n","Epoch 2/50, Batch 1400/7628\n","Epoch 2/50, Batch 1500/7628\n","Epoch 2/50, Batch 1600/7628\n","Epoch 2/50, Batch 1700/7628\n","Epoch 2/50, Batch 1800/7628\n","Epoch 2/50, Batch 1900/7628\n","Epoch 2/50, Batch 2000/7628\n","Epoch 2/50, Batch 2100/7628\n","Epoch 2/50, Batch 2200/7628\n","Epoch 2/50, Batch 2300/7628\n","Epoch 2/50, Batch 2400/7628\n","Epoch 2/50, Batch 2500/7628\n","Epoch 2/50, Batch 2600/7628\n","Epoch 2/50, Batch 2700/7628\n","Epoch 2/50, Batch 2800/7628\n","Epoch 2/50, Batch 2900/7628\n","Epoch 2/50, Batch 3000/7628\n","Epoch 2/50, Batch 3100/7628\n","Epoch 2/50, Batch 3200/7628\n","Epoch 2/50, Batch 3300/7628\n","Epoch 2/50, Batch 3400/7628\n","Epoch 2/50, Batch 3500/7628\n","Epoch 2/50, Batch 3600/7628\n","Epoch 2/50, Batch 3700/7628\n","Epoch 2/50, Batch 3800/7628\n","Epoch 2/50, Batch 3900/7628\n","Epoch 2/50, Batch 4000/7628\n","Epoch 2/50, Batch 4100/7628\n","Epoch 2/50, Batch 4200/7628\n","Epoch 2/50, Batch 4300/7628\n","Epoch 2/50, Batch 4400/7628\n","Epoch 2/50, Batch 4500/7628\n","Epoch 2/50, Batch 4600/7628\n","Epoch 2/50, Batch 4700/7628\n","Epoch 2/50, Batch 4800/7628\n","Epoch 2/50, Batch 4900/7628\n","Epoch 2/50, Batch 5000/7628\n","Epoch 2/50, Batch 5100/7628\n","Epoch 2/50, Batch 5200/7628\n","Epoch 2/50, Batch 5300/7628\n","Epoch 2/50, Batch 5400/7628\n","Epoch 2/50, Batch 5500/7628\n","Epoch 2/50, Batch 5600/7628\n","Epoch 2/50, Batch 5700/7628\n","Epoch 2/50, Batch 5800/7628\n","Epoch 2/50, Batch 5900/7628\n","Epoch 2/50, Batch 6000/7628\n","Epoch 2/50, Batch 6100/7628\n","Epoch 2/50, Batch 6200/7628\n","Epoch 2/50, Batch 6300/7628\n","Epoch 2/50, Batch 6400/7628\n","Epoch 2/50, Batch 6500/7628\n","Epoch 2/50, Batch 6600/7628\n","Epoch 2/50, Batch 6700/7628\n","Epoch 2/50, Batch 6800/7628\n","Epoch 2/50, Batch 6900/7628\n","Epoch 2/50, Batch 7000/7628\n","Epoch 2/50, Batch 7100/7628\n","Epoch 2/50, Batch 7200/7628\n","Epoch 2/50, Batch 7300/7628\n","Epoch 2/50, Batch 7400/7628\n","Epoch 2/50, Batch 7500/7628\n","Epoch 2/50, Batch 7600/7628\n","Epoch 2/50, Loss: 0.4464\n","Best model saved with loss: 0.4464\n","Epoch 3/50, Batch 0/7628\n","Epoch 3/50, Batch 100/7628\n","Epoch 3/50, Batch 200/7628\n","Epoch 3/50, Batch 300/7628\n","Epoch 3/50, Batch 400/7628\n","Epoch 3/50, Batch 500/7628\n","Epoch 3/50, Batch 600/7628\n","Epoch 3/50, Batch 700/7628\n","Epoch 3/50, Batch 800/7628\n","Epoch 3/50, Batch 900/7628\n","Epoch 3/50, Batch 1000/7628\n","Epoch 3/50, Batch 1100/7628\n","Epoch 3/50, Batch 1200/7628\n","Epoch 3/50, Batch 1300/7628\n","Epoch 3/50, Batch 1400/7628\n","Epoch 3/50, Batch 1500/7628\n","Epoch 3/50, Batch 1600/7628\n","Epoch 3/50, Batch 1700/7628\n","Epoch 3/50, Batch 1800/7628\n","Epoch 3/50, Batch 1900/7628\n","Epoch 3/50, Batch 2000/7628\n","Epoch 3/50, Batch 2100/7628\n","Epoch 3/50, Batch 2200/7628\n","Epoch 3/50, Batch 2300/7628\n","Epoch 3/50, Batch 2400/7628\n","Epoch 3/50, Batch 2500/7628\n","Epoch 3/50, Batch 2600/7628\n","Epoch 3/50, Batch 2700/7628\n","Epoch 3/50, Batch 2800/7628\n","Epoch 3/50, Batch 2900/7628\n","Epoch 3/50, Batch 3000/7628\n","Epoch 3/50, Batch 3100/7628\n","Epoch 3/50, Batch 3200/7628\n","Epoch 3/50, Batch 3300/7628\n","Epoch 3/50, Batch 3400/7628\n","Epoch 3/50, Batch 3500/7628\n","Epoch 3/50, Batch 3600/7628\n","Epoch 3/50, Batch 3700/7628\n","Epoch 3/50, Batch 3800/7628\n","Epoch 3/50, Batch 3900/7628\n","Epoch 3/50, Batch 4000/7628\n","Epoch 3/50, Batch 4100/7628\n","Epoch 3/50, Batch 4200/7628\n","Epoch 3/50, Batch 4300/7628\n","Epoch 3/50, Batch 4400/7628\n","Epoch 3/50, Batch 4500/7628\n","Epoch 3/50, Batch 4600/7628\n","Epoch 3/50, Batch 4700/7628\n","Epoch 3/50, Batch 4800/7628\n","Epoch 3/50, Batch 4900/7628\n","Epoch 3/50, Batch 5000/7628\n","Epoch 3/50, Batch 5100/7628\n","Epoch 3/50, Batch 5200/7628\n","Epoch 3/50, Batch 5300/7628\n","Epoch 3/50, Batch 5400/7628\n","Epoch 3/50, Batch 5500/7628\n","Epoch 3/50, Batch 5600/7628\n","Epoch 3/50, Batch 5700/7628\n","Epoch 3/50, Batch 5800/7628\n","Epoch 3/50, Batch 5900/7628\n","Epoch 3/50, Batch 6000/7628\n","Epoch 3/50, Batch 6100/7628\n","Epoch 3/50, Batch 6200/7628\n","Epoch 3/50, Batch 6300/7628\n","Epoch 3/50, Batch 6400/7628\n","Epoch 3/50, Batch 6500/7628\n","Epoch 3/50, Batch 6600/7628\n","Epoch 3/50, Batch 6700/7628\n","Epoch 3/50, Batch 6800/7628\n","Epoch 3/50, Batch 6900/7628\n","Epoch 3/50, Batch 7000/7628\n","Epoch 3/50, Batch 7100/7628\n","Epoch 3/50, Batch 7200/7628\n","Epoch 3/50, Batch 7300/7628\n","Epoch 3/50, Batch 7400/7628\n","Epoch 3/50, Batch 7500/7628\n","Epoch 3/50, Batch 7600/7628\n","Epoch 3/50, Loss: 0.4460\n","Best model saved with loss: 0.4460\n","Epoch 4/50, Batch 0/7628\n","Epoch 4/50, Batch 100/7628\n","Epoch 4/50, Batch 200/7628\n","Epoch 4/50, Batch 300/7628\n","Epoch 4/50, Batch 400/7628\n","Epoch 4/50, Batch 500/7628\n","Epoch 4/50, Batch 600/7628\n","Epoch 4/50, Batch 700/7628\n","Epoch 4/50, Batch 800/7628\n","Epoch 4/50, Batch 900/7628\n","Epoch 4/50, Batch 1000/7628\n","Epoch 4/50, Batch 1100/7628\n","Epoch 4/50, Batch 1200/7628\n","Epoch 4/50, Batch 1300/7628\n","Epoch 4/50, Batch 1400/7628\n","Epoch 4/50, Batch 1500/7628\n","Epoch 4/50, Batch 1600/7628\n","Epoch 4/50, Batch 1700/7628\n","Epoch 4/50, Batch 1800/7628\n","Epoch 4/50, Batch 1900/7628\n","Epoch 4/50, Batch 2000/7628\n","Epoch 4/50, Batch 2100/7628\n","Epoch 4/50, Batch 2200/7628\n","Epoch 4/50, Batch 2300/7628\n","Epoch 4/50, Batch 2400/7628\n","Epoch 4/50, Batch 2500/7628\n","Epoch 4/50, Batch 2600/7628\n","Epoch 4/50, Batch 2700/7628\n","Epoch 4/50, Batch 2800/7628\n","Epoch 4/50, Batch 2900/7628\n","Epoch 4/50, Batch 3000/7628\n","Epoch 4/50, Batch 3100/7628\n","Epoch 4/50, Batch 3200/7628\n","Epoch 4/50, Batch 3300/7628\n","Epoch 4/50, Batch 3400/7628\n","Epoch 4/50, Batch 3500/7628\n","Epoch 4/50, Batch 3600/7628\n","Epoch 4/50, Batch 3700/7628\n","Epoch 4/50, Batch 3800/7628\n","Epoch 4/50, Batch 3900/7628\n","Epoch 4/50, Batch 4000/7628\n","Epoch 4/50, Batch 4100/7628\n","Epoch 4/50, Batch 4200/7628\n","Epoch 4/50, Batch 4300/7628\n","Epoch 4/50, Batch 4400/7628\n","Epoch 4/50, Batch 4500/7628\n","Epoch 4/50, Batch 4600/7628\n","Epoch 4/50, Batch 4700/7628\n","Epoch 4/50, Batch 4800/7628\n","Epoch 4/50, Batch 4900/7628\n","Epoch 4/50, Batch 5000/7628\n","Epoch 4/50, Batch 5100/7628\n","Epoch 4/50, Batch 5200/7628\n","Epoch 4/50, Batch 5300/7628\n","Epoch 4/50, Batch 5400/7628\n","Epoch 4/50, Batch 5500/7628\n","Epoch 4/50, Batch 5600/7628\n","Epoch 4/50, Batch 5700/7628\n","Epoch 4/50, Batch 5800/7628\n","Epoch 4/50, Batch 5900/7628\n","Epoch 4/50, Batch 6000/7628\n","Epoch 4/50, Batch 6100/7628\n","Epoch 4/50, Batch 6200/7628\n","Epoch 4/50, Batch 6300/7628\n","Epoch 4/50, Batch 6400/7628\n","Epoch 4/50, Batch 6500/7628\n","Epoch 4/50, Batch 6600/7628\n","Epoch 4/50, Batch 6700/7628\n","Epoch 4/50, Batch 6800/7628\n","Epoch 4/50, Batch 6900/7628\n","Epoch 4/50, Batch 7000/7628\n","Epoch 4/50, Batch 7100/7628\n","Epoch 4/50, Batch 7200/7628\n","Epoch 4/50, Batch 7300/7628\n","Epoch 4/50, Batch 7400/7628\n","Epoch 4/50, Batch 7500/7628\n","Epoch 4/50, Batch 7600/7628\n","Epoch 4/50, Loss: 0.4459\n","Best model saved with loss: 0.4459\n","Epoch 5/50, Batch 0/7628\n","Epoch 5/50, Batch 100/7628\n","Epoch 5/50, Batch 200/7628\n","Epoch 5/50, Batch 300/7628\n","Epoch 5/50, Batch 400/7628\n","Epoch 5/50, Batch 500/7628\n","Epoch 5/50, Batch 600/7628\n","Epoch 5/50, Batch 700/7628\n","Epoch 5/50, Batch 800/7628\n","Epoch 5/50, Batch 900/7628\n","Epoch 5/50, Batch 1000/7628\n","Epoch 5/50, Batch 1100/7628\n","Epoch 5/50, Batch 1200/7628\n","Epoch 5/50, Batch 1300/7628\n","Epoch 5/50, Batch 1400/7628\n","Epoch 5/50, Batch 1500/7628\n","Epoch 5/50, Batch 1600/7628\n","Epoch 5/50, Batch 1700/7628\n","Epoch 5/50, Batch 1800/7628\n","Epoch 5/50, Batch 1900/7628\n","Epoch 5/50, Batch 2000/7628\n","Epoch 5/50, Batch 2100/7628\n","Epoch 5/50, Batch 2200/7628\n","Epoch 5/50, Batch 2300/7628\n","Epoch 5/50, Batch 2400/7628\n","Epoch 5/50, Batch 2500/7628\n","Epoch 5/50, Batch 2600/7628\n","Epoch 5/50, Batch 2700/7628\n","Epoch 5/50, Batch 2800/7628\n","Epoch 5/50, Batch 2900/7628\n","Epoch 5/50, Batch 3000/7628\n","Epoch 5/50, Batch 3100/7628\n","Epoch 5/50, Batch 3200/7628\n","Epoch 5/50, Batch 3300/7628\n","Epoch 5/50, Batch 3400/7628\n","Epoch 5/50, Batch 3500/7628\n","Epoch 5/50, Batch 3600/7628\n","Epoch 5/50, Batch 3700/7628\n","Epoch 5/50, Batch 3800/7628\n","Epoch 5/50, Batch 3900/7628\n","Epoch 5/50, Batch 4000/7628\n","Epoch 5/50, Batch 4100/7628\n","Epoch 5/50, Batch 4200/7628\n","Epoch 5/50, Batch 4300/7628\n","Epoch 5/50, Batch 4400/7628\n","Epoch 5/50, Batch 4500/7628\n","Epoch 5/50, Batch 4600/7628\n","Epoch 5/50, Batch 4700/7628\n","Epoch 5/50, Batch 4800/7628\n","Epoch 5/50, Batch 4900/7628\n","Epoch 5/50, Batch 5000/7628\n","Epoch 5/50, Batch 5100/7628\n","Epoch 5/50, Batch 5200/7628\n","Epoch 5/50, Batch 5300/7628\n","Epoch 5/50, Batch 5400/7628\n","Epoch 5/50, Batch 5500/7628\n","Epoch 5/50, Batch 5600/7628\n","Epoch 5/50, Batch 5700/7628\n","Epoch 5/50, Batch 5800/7628\n","Epoch 5/50, Batch 5900/7628\n","Epoch 5/50, Batch 6000/7628\n","Epoch 5/50, Batch 6100/7628\n","Epoch 5/50, Batch 6200/7628\n","Epoch 5/50, Batch 6300/7628\n","Epoch 5/50, Batch 6400/7628\n","Epoch 5/50, Batch 6500/7628\n","Epoch 5/50, Batch 6600/7628\n","Epoch 5/50, Batch 6700/7628\n","Epoch 5/50, Batch 6800/7628\n","Epoch 5/50, Batch 6900/7628\n","Epoch 5/50, Batch 7000/7628\n","Epoch 5/50, Batch 7100/7628\n","Epoch 5/50, Batch 7200/7628\n","Epoch 5/50, Batch 7300/7628\n","Epoch 5/50, Batch 7400/7628\n","Epoch 5/50, Batch 7500/7628\n","Epoch 5/50, Batch 7600/7628\n","Epoch 5/50, Loss: 0.4458\n","Best model saved with loss: 0.4458\n","Epoch 6/50, Batch 0/7628\n","Epoch 6/50, Batch 100/7628\n","Epoch 6/50, Batch 200/7628\n","Epoch 6/50, Batch 300/7628\n","Epoch 6/50, Batch 400/7628\n","Epoch 6/50, Batch 500/7628\n","Epoch 6/50, Batch 600/7628\n","Epoch 6/50, Batch 700/7628\n","Epoch 6/50, Batch 800/7628\n","Epoch 6/50, Batch 900/7628\n","Epoch 6/50, Batch 1000/7628\n","Epoch 6/50, Batch 1100/7628\n","Epoch 6/50, Batch 1200/7628\n","Epoch 6/50, Batch 1300/7628\n","Epoch 6/50, Batch 1400/7628\n","Epoch 6/50, Batch 1500/7628\n","Epoch 6/50, Batch 1600/7628\n","Epoch 6/50, Batch 1700/7628\n","Epoch 6/50, Batch 1800/7628\n","Epoch 6/50, Batch 1900/7628\n","Epoch 6/50, Batch 2000/7628\n","Epoch 6/50, Batch 2100/7628\n","Epoch 6/50, Batch 2200/7628\n","Epoch 6/50, Batch 2300/7628\n","Epoch 6/50, Batch 2400/7628\n","Epoch 6/50, Batch 2500/7628\n","Epoch 6/50, Batch 2600/7628\n","Epoch 6/50, Batch 2700/7628\n","Epoch 6/50, Batch 2800/7628\n","Epoch 6/50, Batch 2900/7628\n","Epoch 6/50, Batch 3000/7628\n","Epoch 6/50, Batch 3100/7628\n","Epoch 6/50, Batch 3200/7628\n","Epoch 6/50, Batch 3300/7628\n","Epoch 6/50, Batch 3400/7628\n","Epoch 6/50, Batch 3500/7628\n","Epoch 6/50, Batch 3600/7628\n","Epoch 6/50, Batch 3700/7628\n","Epoch 6/50, Batch 3800/7628\n","Epoch 6/50, Batch 3900/7628\n","Epoch 6/50, Batch 4000/7628\n","Epoch 6/50, Batch 4100/7628\n","Epoch 6/50, Batch 4200/7628\n","Epoch 6/50, Batch 4300/7628\n","Epoch 6/50, Batch 4400/7628\n","Epoch 6/50, Batch 4500/7628\n","Epoch 6/50, Batch 4600/7628\n","Epoch 6/50, Batch 4700/7628\n","Epoch 6/50, Batch 4800/7628\n","Epoch 6/50, Batch 4900/7628\n","Epoch 6/50, Batch 5000/7628\n","Epoch 6/50, Batch 5100/7628\n","Epoch 6/50, Batch 5200/7628\n","Epoch 6/50, Batch 5300/7628\n","Epoch 6/50, Batch 5400/7628\n","Epoch 6/50, Batch 5500/7628\n","Epoch 6/50, Batch 5600/7628\n","Epoch 6/50, Batch 5700/7628\n","Epoch 6/50, Batch 5800/7628\n","Epoch 6/50, Batch 5900/7628\n","Epoch 6/50, Batch 6000/7628\n","Epoch 6/50, Batch 6100/7628\n","Epoch 6/50, Batch 6200/7628\n","Epoch 6/50, Batch 6300/7628\n","Epoch 6/50, Batch 6400/7628\n","Epoch 6/50, Batch 6500/7628\n","Epoch 6/50, Batch 6600/7628\n","Epoch 6/50, Batch 6700/7628\n","Epoch 6/50, Batch 6800/7628\n","Epoch 6/50, Batch 6900/7628\n","Epoch 6/50, Batch 7000/7628\n","Epoch 6/50, Batch 7100/7628\n","Epoch 6/50, Batch 7200/7628\n","Epoch 6/50, Batch 7300/7628\n","Epoch 6/50, Batch 7400/7628\n","Epoch 6/50, Batch 7500/7628\n","Epoch 6/50, Batch 7600/7628\n","Epoch 6/50, Loss: 0.4457\n","Best model saved with loss: 0.4457\n","Epoch 7/50, Batch 0/7628\n","Epoch 7/50, Batch 100/7628\n","Epoch 7/50, Batch 200/7628\n","Epoch 7/50, Batch 300/7628\n","Epoch 7/50, Batch 400/7628\n","Epoch 7/50, Batch 500/7628\n","Epoch 7/50, Batch 600/7628\n","Epoch 7/50, Batch 700/7628\n","Epoch 7/50, Batch 800/7628\n","Epoch 7/50, Batch 900/7628\n","Epoch 7/50, Batch 1000/7628\n","Epoch 7/50, Batch 1100/7628\n","Epoch 7/50, Batch 1200/7628\n","Epoch 7/50, Batch 1300/7628\n","Epoch 7/50, Batch 1400/7628\n","Epoch 7/50, Batch 1500/7628\n","Epoch 7/50, Batch 1600/7628\n","Epoch 7/50, Batch 1700/7628\n","Epoch 7/50, Batch 1800/7628\n","Epoch 7/50, Batch 1900/7628\n","Epoch 7/50, Batch 2000/7628\n","Epoch 7/50, Batch 2100/7628\n","Epoch 7/50, Batch 2200/7628\n","Epoch 7/50, Batch 2300/7628\n","Epoch 7/50, Batch 2400/7628\n","Epoch 7/50, Batch 2500/7628\n","Epoch 7/50, Batch 2600/7628\n","Epoch 7/50, Batch 2700/7628\n","Epoch 7/50, Batch 2800/7628\n","Epoch 7/50, Batch 2900/7628\n","Epoch 7/50, Batch 3000/7628\n","Epoch 7/50, Batch 3100/7628\n","Epoch 7/50, Batch 3200/7628\n","Epoch 7/50, Batch 3300/7628\n","Epoch 7/50, Batch 3400/7628\n","Epoch 7/50, Batch 3500/7628\n","Epoch 7/50, Batch 3600/7628\n","Epoch 7/50, Batch 3700/7628\n","Epoch 7/50, Batch 3800/7628\n","Epoch 7/50, Batch 3900/7628\n","Epoch 7/50, Batch 4000/7628\n","Epoch 7/50, Batch 4100/7628\n","Epoch 7/50, Batch 4200/7628\n","Epoch 7/50, Batch 4300/7628\n","Epoch 7/50, Batch 4400/7628\n","Epoch 7/50, Batch 4500/7628\n","Epoch 7/50, Batch 4600/7628\n","Epoch 7/50, Batch 4700/7628\n","Epoch 7/50, Batch 4800/7628\n","Epoch 7/50, Batch 4900/7628\n","Epoch 7/50, Batch 5000/7628\n","Epoch 7/50, Batch 5100/7628\n","Epoch 7/50, Batch 5200/7628\n","Epoch 7/50, Batch 5300/7628\n","Epoch 7/50, Batch 5400/7628\n","Epoch 7/50, Batch 5500/7628\n","Epoch 7/50, Batch 5600/7628\n","Epoch 7/50, Batch 5700/7628\n","Epoch 7/50, Batch 5800/7628\n","Epoch 7/50, Batch 5900/7628\n","Epoch 7/50, Batch 6000/7628\n","Epoch 7/50, Batch 6100/7628\n","Epoch 7/50, Batch 6200/7628\n","Epoch 7/50, Batch 6300/7628\n","Epoch 7/50, Batch 6400/7628\n","Epoch 7/50, Batch 6500/7628\n","Epoch 7/50, Batch 6600/7628\n","Epoch 7/50, Batch 6700/7628\n","Epoch 7/50, Batch 6800/7628\n","Epoch 7/50, Batch 6900/7628\n","Epoch 7/50, Batch 7000/7628\n","Epoch 7/50, Batch 7100/7628\n","Epoch 7/50, Batch 7200/7628\n","Epoch 7/50, Batch 7300/7628\n","Epoch 7/50, Batch 7400/7628\n","Epoch 7/50, Batch 7500/7628\n","Epoch 7/50, Batch 7600/7628\n","Epoch 7/50, Loss: 0.4456\n","Best model saved with loss: 0.4456\n","Epoch 8/50, Batch 0/7628\n","Epoch 8/50, Batch 100/7628\n","Epoch 8/50, Batch 200/7628\n","Epoch 8/50, Batch 300/7628\n","Epoch 8/50, Batch 400/7628\n","Epoch 8/50, Batch 500/7628\n","Epoch 8/50, Batch 600/7628\n","Epoch 8/50, Batch 700/7628\n","Epoch 8/50, Batch 800/7628\n","Epoch 8/50, Batch 900/7628\n","Epoch 8/50, Batch 1000/7628\n","Epoch 8/50, Batch 1100/7628\n","Epoch 8/50, Batch 1200/7628\n","Epoch 8/50, Batch 1300/7628\n","Epoch 8/50, Batch 1400/7628\n","Epoch 8/50, Batch 1500/7628\n","Epoch 8/50, Batch 1600/7628\n","Epoch 8/50, Batch 1700/7628\n","Epoch 8/50, Batch 1800/7628\n","Epoch 8/50, Batch 1900/7628\n","Epoch 8/50, Batch 2000/7628\n","Epoch 8/50, Batch 2100/7628\n","Epoch 8/50, Batch 2200/7628\n","Epoch 8/50, Batch 2300/7628\n","Epoch 8/50, Batch 2400/7628\n","Epoch 8/50, Batch 2500/7628\n","Epoch 8/50, Batch 2600/7628\n","Epoch 8/50, Batch 2700/7628\n","Epoch 8/50, Batch 2800/7628\n","Epoch 8/50, Batch 2900/7628\n","Epoch 8/50, Batch 3000/7628\n","Epoch 8/50, Batch 3100/7628\n","Epoch 8/50, Batch 3200/7628\n","Epoch 8/50, Batch 3300/7628\n","Epoch 8/50, Batch 3400/7628\n","Epoch 8/50, Batch 3500/7628\n","Epoch 8/50, Batch 3600/7628\n","Epoch 8/50, Batch 3700/7628\n","Epoch 8/50, Batch 3800/7628\n","Epoch 8/50, Batch 3900/7628\n","Epoch 8/50, Batch 4000/7628\n","Epoch 8/50, Batch 4100/7628\n","Epoch 8/50, Batch 4200/7628\n","Epoch 8/50, Batch 4300/7628\n","Epoch 8/50, Batch 4400/7628\n","Epoch 8/50, Batch 4500/7628\n","Epoch 8/50, Batch 4600/7628\n","Epoch 8/50, Batch 4700/7628\n","Epoch 8/50, Batch 4800/7628\n","Epoch 8/50, Batch 4900/7628\n","Epoch 8/50, Batch 5000/7628\n","Epoch 8/50, Batch 5100/7628\n","Epoch 8/50, Batch 5200/7628\n","Epoch 8/50, Batch 5300/7628\n","Epoch 8/50, Batch 5400/7628\n","Epoch 8/50, Batch 5500/7628\n","Epoch 8/50, Batch 5600/7628\n","Epoch 8/50, Batch 5700/7628\n","Epoch 8/50, Batch 5800/7628\n","Epoch 8/50, Batch 5900/7628\n","Epoch 8/50, Batch 6000/7628\n","Epoch 8/50, Batch 6100/7628\n","Epoch 8/50, Batch 6200/7628\n","Epoch 8/50, Batch 6300/7628\n","Epoch 8/50, Batch 6400/7628\n","Epoch 8/50, Batch 6500/7628\n","Epoch 8/50, Batch 6600/7628\n","Epoch 8/50, Batch 6700/7628\n","Epoch 8/50, Batch 6800/7628\n","Epoch 8/50, Batch 6900/7628\n","Epoch 8/50, Batch 7000/7628\n","Epoch 8/50, Batch 7100/7628\n","Epoch 8/50, Batch 7200/7628\n","Epoch 8/50, Batch 7300/7628\n","Epoch 8/50, Batch 7400/7628\n","Epoch 8/50, Batch 7500/7628\n","Epoch 8/50, Batch 7600/7628\n","Epoch 8/50, Loss: 0.4456\n","Best model saved with loss: 0.4456\n","Epoch 9/50, Batch 0/7628\n","Epoch 9/50, Batch 100/7628\n","Epoch 9/50, Batch 200/7628\n","Epoch 9/50, Batch 300/7628\n","Epoch 9/50, Batch 400/7628\n","Epoch 9/50, Batch 500/7628\n","Epoch 9/50, Batch 600/7628\n","Epoch 9/50, Batch 700/7628\n","Epoch 9/50, Batch 800/7628\n","Epoch 9/50, Batch 900/7628\n","Epoch 9/50, Batch 1000/7628\n","Epoch 9/50, Batch 1100/7628\n","Epoch 9/50, Batch 1200/7628\n","Epoch 9/50, Batch 1300/7628\n","Epoch 9/50, Batch 1400/7628\n","Epoch 9/50, Batch 1500/7628\n","Epoch 9/50, Batch 1600/7628\n","Epoch 9/50, Batch 1700/7628\n","Epoch 9/50, Batch 1800/7628\n","Epoch 9/50, Batch 1900/7628\n","Epoch 9/50, Batch 2000/7628\n","Epoch 9/50, Batch 2100/7628\n","Epoch 9/50, Batch 2200/7628\n","Epoch 9/50, Batch 2300/7628\n","Epoch 9/50, Batch 2400/7628\n","Epoch 9/50, Batch 2500/7628\n","Epoch 9/50, Batch 2600/7628\n","Epoch 9/50, Batch 2700/7628\n","Epoch 9/50, Batch 2800/7628\n","Epoch 9/50, Batch 2900/7628\n","Epoch 9/50, Batch 3000/7628\n","Epoch 9/50, Batch 3100/7628\n","Epoch 9/50, Batch 3200/7628\n","Epoch 9/50, Batch 3300/7628\n","Epoch 9/50, Batch 3400/7628\n","Epoch 9/50, Batch 3500/7628\n","Epoch 9/50, Batch 3600/7628\n","Epoch 9/50, Batch 3700/7628\n","Epoch 9/50, Batch 3800/7628\n","Epoch 9/50, Batch 3900/7628\n","Epoch 9/50, Batch 4000/7628\n","Epoch 9/50, Batch 4100/7628\n","Epoch 9/50, Batch 4200/7628\n","Epoch 9/50, Batch 4300/7628\n","Epoch 9/50, Batch 4400/7628\n","Epoch 9/50, Batch 4500/7628\n","Epoch 9/50, Batch 4600/7628\n","Epoch 9/50, Batch 4700/7628\n","Epoch 9/50, Batch 4800/7628\n","Epoch 9/50, Batch 4900/7628\n","Epoch 9/50, Batch 5000/7628\n","Epoch 9/50, Batch 5100/7628\n","Epoch 9/50, Batch 5200/7628\n","Epoch 9/50, Batch 5300/7628\n","Epoch 9/50, Batch 5400/7628\n","Epoch 9/50, Batch 5500/7628\n","Epoch 9/50, Batch 5600/7628\n","Epoch 9/50, Batch 5700/7628\n","Epoch 9/50, Batch 5800/7628\n","Epoch 9/50, Batch 5900/7628\n","Epoch 9/50, Batch 6000/7628\n","Epoch 9/50, Batch 6100/7628\n","Epoch 9/50, Batch 6200/7628\n","Epoch 9/50, Batch 6300/7628\n","Epoch 9/50, Batch 6400/7628\n","Epoch 9/50, Batch 6500/7628\n","Epoch 9/50, Batch 6600/7628\n","Epoch 9/50, Batch 6700/7628\n","Epoch 9/50, Batch 6800/7628\n","Epoch 9/50, Batch 6900/7628\n","Epoch 9/50, Batch 7000/7628\n","Epoch 9/50, Batch 7100/7628\n","Epoch 9/50, Batch 7200/7628\n","Epoch 9/50, Batch 7300/7628\n","Epoch 9/50, Batch 7400/7628\n","Epoch 9/50, Batch 7500/7628\n","Epoch 9/50, Batch 7600/7628\n","Epoch 9/50, Loss: 0.4456\n","Best model saved with loss: 0.4456\n","Epoch 10/50, Batch 0/7628\n","Epoch 10/50, Batch 100/7628\n","Epoch 10/50, Batch 200/7628\n","Epoch 10/50, Batch 300/7628\n","Epoch 10/50, Batch 400/7628\n","Epoch 10/50, Batch 500/7628\n","Epoch 10/50, Batch 600/7628\n","Epoch 10/50, Batch 700/7628\n","Epoch 10/50, Batch 800/7628\n","Epoch 10/50, Batch 900/7628\n","Epoch 10/50, Batch 1000/7628\n","Epoch 10/50, Batch 1100/7628\n","Epoch 10/50, Batch 1200/7628\n","Epoch 10/50, Batch 1300/7628\n","Epoch 10/50, Batch 1400/7628\n","Epoch 10/50, Batch 1500/7628\n","Epoch 10/50, Batch 1600/7628\n","Epoch 10/50, Batch 1700/7628\n","Epoch 10/50, Batch 1800/7628\n","Epoch 10/50, Batch 1900/7628\n","Epoch 10/50, Batch 2000/7628\n","Epoch 10/50, Batch 2100/7628\n","Epoch 10/50, Batch 2200/7628\n","Epoch 10/50, Batch 2300/7628\n","Epoch 10/50, Batch 2400/7628\n","Epoch 10/50, Batch 2500/7628\n","Epoch 10/50, Batch 2600/7628\n","Epoch 10/50, Batch 2700/7628\n","Epoch 10/50, Batch 2800/7628\n","Epoch 10/50, Batch 2900/7628\n","Epoch 10/50, Batch 3000/7628\n","Epoch 10/50, Batch 3100/7628\n","Epoch 10/50, Batch 3200/7628\n","Epoch 10/50, Batch 3300/7628\n","Epoch 10/50, Batch 3400/7628\n","Epoch 10/50, Batch 3500/7628\n","Epoch 10/50, Batch 3600/7628\n","Epoch 10/50, Batch 3700/7628\n","Epoch 10/50, Batch 3800/7628\n","Epoch 10/50, Batch 3900/7628\n","Epoch 10/50, Batch 4000/7628\n","Epoch 10/50, Batch 4100/7628\n","Epoch 10/50, Batch 4200/7628\n","Epoch 10/50, Batch 4300/7628\n","Epoch 10/50, Batch 4400/7628\n","Epoch 10/50, Batch 4500/7628\n","Epoch 10/50, Batch 4600/7628\n","Epoch 10/50, Batch 4700/7628\n","Epoch 10/50, Batch 4800/7628\n","Epoch 10/50, Batch 4900/7628\n","Epoch 10/50, Batch 5000/7628\n","Epoch 10/50, Batch 5100/7628\n","Epoch 10/50, Batch 5200/7628\n","Epoch 10/50, Batch 5300/7628\n","Epoch 10/50, Batch 5400/7628\n","Epoch 10/50, Batch 5500/7628\n","Epoch 10/50, Batch 5600/7628\n","Epoch 10/50, Batch 5700/7628\n","Epoch 10/50, Batch 5800/7628\n","Epoch 10/50, Batch 5900/7628\n","Epoch 10/50, Batch 6000/7628\n","Epoch 10/50, Batch 6100/7628\n","Epoch 10/50, Batch 6200/7628\n","Epoch 10/50, Batch 6300/7628\n","Epoch 10/50, Batch 6400/7628\n","Epoch 10/50, Batch 6500/7628\n","Epoch 10/50, Batch 6600/7628\n","Epoch 10/50, Batch 6700/7628\n","Epoch 10/50, Batch 6800/7628\n","Epoch 10/50, Batch 6900/7628\n","Epoch 10/50, Batch 7000/7628\n","Epoch 10/50, Batch 7100/7628\n","Epoch 10/50, Batch 7200/7628\n","Epoch 10/50, Batch 7300/7628\n","Epoch 10/50, Batch 7400/7628\n","Epoch 10/50, Batch 7500/7628\n","Epoch 10/50, Batch 7600/7628\n","Epoch 10/50, Loss: 0.4455\n","Best model saved with loss: 0.4455\n","Epoch 11/50, Batch 0/7628\n","Epoch 11/50, Batch 100/7628\n","Epoch 11/50, Batch 200/7628\n","Epoch 11/50, Batch 300/7628\n","Epoch 11/50, Batch 400/7628\n","Epoch 11/50, Batch 500/7628\n","Epoch 11/50, Batch 600/7628\n","Epoch 11/50, Batch 700/7628\n","Epoch 11/50, Batch 800/7628\n","Epoch 11/50, Batch 900/7628\n","Epoch 11/50, Batch 1000/7628\n","Epoch 11/50, Batch 1100/7628\n","Epoch 11/50, Batch 1200/7628\n","Epoch 11/50, Batch 1300/7628\n","Epoch 11/50, Batch 1400/7628\n","Epoch 11/50, Batch 1500/7628\n","Epoch 11/50, Batch 1600/7628\n","Epoch 11/50, Batch 1700/7628\n","Epoch 11/50, Batch 1800/7628\n","Epoch 11/50, Batch 1900/7628\n","Epoch 11/50, Batch 2000/7628\n","Epoch 11/50, Batch 2100/7628\n","Epoch 11/50, Batch 2200/7628\n","Epoch 11/50, Batch 2300/7628\n","Epoch 11/50, Batch 2400/7628\n","Epoch 11/50, Batch 2500/7628\n","Epoch 11/50, Batch 2600/7628\n","Epoch 11/50, Batch 2700/7628\n","Epoch 11/50, Batch 2800/7628\n","Epoch 11/50, Batch 2900/7628\n","Epoch 11/50, Batch 3000/7628\n","Epoch 11/50, Batch 3100/7628\n","Epoch 11/50, Batch 3200/7628\n","Epoch 11/50, Batch 3300/7628\n","Epoch 11/50, Batch 3400/7628\n","Epoch 11/50, Batch 3500/7628\n","Epoch 11/50, Batch 3600/7628\n","Epoch 11/50, Batch 3700/7628\n","Epoch 11/50, Batch 3800/7628\n","Epoch 11/50, Batch 3900/7628\n","Epoch 11/50, Batch 4000/7628\n","Epoch 11/50, Batch 4100/7628\n","Epoch 11/50, Batch 4200/7628\n","Epoch 11/50, Batch 4300/7628\n","Epoch 11/50, Batch 4400/7628\n","Epoch 11/50, Batch 4500/7628\n","Epoch 11/50, Batch 4600/7628\n","Epoch 11/50, Batch 4700/7628\n","Epoch 11/50, Batch 4800/7628\n","Epoch 11/50, Batch 4900/7628\n","Epoch 11/50, Batch 5000/7628\n","Epoch 11/50, Batch 5100/7628\n","Epoch 11/50, Batch 5200/7628\n","Epoch 11/50, Batch 5300/7628\n","Epoch 11/50, Batch 5400/7628\n","Epoch 11/50, Batch 5500/7628\n","Epoch 11/50, Batch 5600/7628\n","Epoch 11/50, Batch 5700/7628\n","Epoch 11/50, Batch 5800/7628\n","Epoch 11/50, Batch 5900/7628\n","Epoch 11/50, Batch 6000/7628\n","Epoch 11/50, Batch 6100/7628\n","Epoch 11/50, Batch 6200/7628\n","Epoch 11/50, Batch 6300/7628\n","Epoch 11/50, Batch 6400/7628\n","Epoch 11/50, Batch 6500/7628\n","Epoch 11/50, Batch 6600/7628\n","Epoch 11/50, Batch 6700/7628\n","Epoch 11/50, Batch 6800/7628\n","Epoch 11/50, Batch 6900/7628\n","Epoch 11/50, Batch 7000/7628\n","Epoch 11/50, Batch 7100/7628\n","Epoch 11/50, Batch 7200/7628\n","Epoch 11/50, Batch 7300/7628\n","Epoch 11/50, Batch 7400/7628\n","Epoch 11/50, Batch 7500/7628\n","Epoch 11/50, Batch 7600/7628\n","Epoch 11/50, Loss: 0.4455\n","Best model saved with loss: 0.4455\n","Epoch 12/50, Batch 0/7628\n","Epoch 12/50, Batch 100/7628\n","Epoch 12/50, Batch 200/7628\n","Epoch 12/50, Batch 300/7628\n","Epoch 12/50, Batch 400/7628\n","Epoch 12/50, Batch 500/7628\n","Epoch 12/50, Batch 600/7628\n","Epoch 12/50, Batch 700/7628\n","Epoch 12/50, Batch 800/7628\n","Epoch 12/50, Batch 900/7628\n","Epoch 12/50, Batch 1000/7628\n","Epoch 12/50, Batch 1100/7628\n","Epoch 12/50, Batch 1200/7628\n","Epoch 12/50, Batch 1300/7628\n","Epoch 12/50, Batch 1400/7628\n","Epoch 12/50, Batch 1500/7628\n","Epoch 12/50, Batch 1600/7628\n","Epoch 12/50, Batch 1700/7628\n","Epoch 12/50, Batch 1800/7628\n","Epoch 12/50, Batch 1900/7628\n","Epoch 12/50, Batch 2000/7628\n","Epoch 12/50, Batch 2100/7628\n","Epoch 12/50, Batch 2200/7628\n","Epoch 12/50, Batch 2300/7628\n","Epoch 12/50, Batch 2400/7628\n","Epoch 12/50, Batch 2500/7628\n","Epoch 12/50, Batch 2600/7628\n","Epoch 12/50, Batch 2700/7628\n","Epoch 12/50, Batch 2800/7628\n","Epoch 12/50, Batch 2900/7628\n","Epoch 12/50, Batch 3000/7628\n","Epoch 12/50, Batch 3100/7628\n","Epoch 12/50, Batch 3200/7628\n","Epoch 12/50, Batch 3300/7628\n","Epoch 12/50, Batch 3400/7628\n","Epoch 12/50, Batch 3500/7628\n","Epoch 12/50, Batch 3600/7628\n","Epoch 12/50, Batch 3700/7628\n","Epoch 12/50, Batch 3800/7628\n","Epoch 12/50, Batch 3900/7628\n","Epoch 12/50, Batch 4000/7628\n","Epoch 12/50, Batch 4100/7628\n","Epoch 12/50, Batch 4200/7628\n","Epoch 12/50, Batch 4300/7628\n","Epoch 12/50, Batch 4400/7628\n","Epoch 12/50, Batch 4500/7628\n","Epoch 12/50, Batch 4600/7628\n","Epoch 12/50, Batch 4700/7628\n","Epoch 12/50, Batch 4800/7628\n","Epoch 12/50, Batch 4900/7628\n","Epoch 12/50, Batch 5000/7628\n","Epoch 12/50, Batch 5100/7628\n","Epoch 12/50, Batch 5200/7628\n","Epoch 12/50, Batch 5300/7628\n","Epoch 12/50, Batch 5400/7628\n","Epoch 12/50, Batch 5500/7628\n","Epoch 12/50, Batch 5600/7628\n","Epoch 12/50, Batch 5700/7628\n","Epoch 12/50, Batch 5800/7628\n","Epoch 12/50, Batch 5900/7628\n","Epoch 12/50, Batch 6000/7628\n","Epoch 12/50, Batch 6100/7628\n","Epoch 12/50, Batch 6200/7628\n","Epoch 12/50, Batch 6300/7628\n","Epoch 12/50, Batch 6400/7628\n","Epoch 12/50, Batch 6500/7628\n","Epoch 12/50, Batch 6600/7628\n","Epoch 12/50, Batch 6700/7628\n","Epoch 12/50, Batch 6800/7628\n","Epoch 12/50, Batch 6900/7628\n","Epoch 12/50, Batch 7000/7628\n","Epoch 12/50, Batch 7100/7628\n","Epoch 12/50, Batch 7200/7628\n","Epoch 12/50, Batch 7300/7628\n","Epoch 12/50, Batch 7400/7628\n","Epoch 12/50, Batch 7500/7628\n","Epoch 12/50, Batch 7600/7628\n","Epoch 12/50, Loss: 0.4455\n","Best model saved with loss: 0.4455\n","Epoch 13/50, Batch 0/7628\n","Epoch 13/50, Batch 100/7628\n","Epoch 13/50, Batch 200/7628\n","Epoch 13/50, Batch 300/7628\n","Epoch 13/50, Batch 400/7628\n","Epoch 13/50, Batch 500/7628\n","Epoch 13/50, Batch 600/7628\n","Epoch 13/50, Batch 700/7628\n","Epoch 13/50, Batch 800/7628\n","Epoch 13/50, Batch 900/7628\n","Epoch 13/50, Batch 1000/7628\n","Epoch 13/50, Batch 1100/7628\n","Epoch 13/50, Batch 1200/7628\n","Epoch 13/50, Batch 1300/7628\n","Epoch 13/50, Batch 1400/7628\n","Epoch 13/50, Batch 1500/7628\n","Epoch 13/50, Batch 1600/7628\n","Epoch 13/50, Batch 1700/7628\n","Epoch 13/50, Batch 1800/7628\n","Epoch 13/50, Batch 1900/7628\n","Epoch 13/50, Batch 2000/7628\n","Epoch 13/50, Batch 2100/7628\n","Epoch 13/50, Batch 2200/7628\n","Epoch 13/50, Batch 2300/7628\n","Epoch 13/50, Batch 2400/7628\n","Epoch 13/50, Batch 2500/7628\n","Epoch 13/50, Batch 2600/7628\n","Epoch 13/50, Batch 2700/7628\n","Epoch 13/50, Batch 2800/7628\n","Epoch 13/50, Batch 2900/7628\n","Epoch 13/50, Batch 3000/7628\n","Epoch 13/50, Batch 3100/7628\n","Epoch 13/50, Batch 3200/7628\n","Epoch 13/50, Batch 3300/7628\n","Epoch 13/50, Batch 3400/7628\n","Epoch 13/50, Batch 3500/7628\n","Epoch 13/50, Batch 3600/7628\n","Epoch 13/50, Batch 3700/7628\n","Epoch 13/50, Batch 3800/7628\n","Epoch 13/50, Batch 3900/7628\n","Epoch 13/50, Batch 4000/7628\n","Epoch 13/50, Batch 4100/7628\n","Epoch 13/50, Batch 4200/7628\n","Epoch 13/50, Batch 4300/7628\n","Epoch 13/50, Batch 4400/7628\n","Epoch 13/50, Batch 4500/7628\n","Epoch 13/50, Batch 4600/7628\n","Epoch 13/50, Batch 4700/7628\n","Epoch 13/50, Batch 4800/7628\n","Epoch 13/50, Batch 4900/7628\n","Epoch 13/50, Batch 5000/7628\n","Epoch 13/50, Batch 5100/7628\n","Epoch 13/50, Batch 5200/7628\n","Epoch 13/50, Batch 5300/7628\n","Epoch 13/50, Batch 5400/7628\n","Epoch 13/50, Batch 5500/7628\n","Epoch 13/50, Batch 5600/7628\n","Epoch 13/50, Batch 5700/7628\n","Epoch 13/50, Batch 5800/7628\n","Epoch 13/50, Batch 5900/7628\n","Epoch 13/50, Batch 6000/7628\n","Epoch 13/50, Batch 6100/7628\n","Epoch 13/50, Batch 6200/7628\n","Epoch 13/50, Batch 6300/7628\n","Epoch 13/50, Batch 6400/7628\n","Epoch 13/50, Batch 6500/7628\n","Epoch 13/50, Batch 6600/7628\n","Epoch 13/50, Batch 6700/7628\n","Epoch 13/50, Batch 6800/7628\n","Epoch 13/50, Batch 6900/7628\n","Epoch 13/50, Batch 7000/7628\n","Epoch 13/50, Batch 7100/7628\n","Epoch 13/50, Batch 7200/7628\n","Epoch 13/50, Batch 7300/7628\n","Epoch 13/50, Batch 7400/7628\n","Epoch 13/50, Batch 7500/7628\n","Epoch 13/50, Batch 7600/7628\n","Epoch 13/50, Loss: 0.4455\n","Best model saved with loss: 0.4455\n","Epoch 14/50, Batch 0/7628\n","Epoch 14/50, Batch 100/7628\n","Epoch 14/50, Batch 200/7628\n","Epoch 14/50, Batch 300/7628\n","Epoch 14/50, Batch 400/7628\n","Epoch 14/50, Batch 500/7628\n","Epoch 14/50, Batch 600/7628\n","Epoch 14/50, Batch 700/7628\n","Epoch 14/50, Batch 800/7628\n","Epoch 14/50, Batch 900/7628\n","Epoch 14/50, Batch 1000/7628\n","Epoch 14/50, Batch 1100/7628\n","Epoch 14/50, Batch 1200/7628\n","Epoch 14/50, Batch 1300/7628\n","Epoch 14/50, Batch 1400/7628\n","Epoch 14/50, Batch 1500/7628\n","Epoch 14/50, Batch 1600/7628\n","Epoch 14/50, Batch 1700/7628\n","Epoch 14/50, Batch 1800/7628\n","Epoch 14/50, Batch 1900/7628\n","Epoch 14/50, Batch 2000/7628\n","Epoch 14/50, Batch 2100/7628\n","Epoch 14/50, Batch 2200/7628\n","Epoch 14/50, Batch 2300/7628\n","Epoch 14/50, Batch 2400/7628\n","Epoch 14/50, Batch 2500/7628\n","Epoch 14/50, Batch 2600/7628\n","Epoch 14/50, Batch 2700/7628\n","Epoch 14/50, Batch 2800/7628\n","Epoch 14/50, Batch 2900/7628\n","Epoch 14/50, Batch 3000/7628\n","Epoch 14/50, Batch 3100/7628\n","Epoch 14/50, Batch 3200/7628\n","Epoch 14/50, Batch 3300/7628\n","Epoch 14/50, Batch 3400/7628\n","Epoch 14/50, Batch 3500/7628\n","Epoch 14/50, Batch 3600/7628\n","Epoch 14/50, Batch 3700/7628\n","Epoch 14/50, Batch 3800/7628\n","Epoch 14/50, Batch 3900/7628\n","Epoch 14/50, Batch 4000/7628\n","Epoch 14/50, Batch 4100/7628\n","Epoch 14/50, Batch 4200/7628\n","Epoch 14/50, Batch 4300/7628\n","Epoch 14/50, Batch 4400/7628\n","Epoch 14/50, Batch 4500/7628\n","Epoch 14/50, Batch 4600/7628\n","Epoch 14/50, Batch 4700/7628\n","Epoch 14/50, Batch 4800/7628\n","Epoch 14/50, Batch 4900/7628\n","Epoch 14/50, Batch 5000/7628\n","Epoch 14/50, Batch 5100/7628\n","Epoch 14/50, Batch 5200/7628\n","Epoch 14/50, Batch 5300/7628\n","Epoch 14/50, Batch 5400/7628\n","Epoch 14/50, Batch 5500/7628\n","Epoch 14/50, Batch 5600/7628\n","Epoch 14/50, Batch 5700/7628\n","Epoch 14/50, Batch 5800/7628\n","Epoch 14/50, Batch 5900/7628\n","Epoch 14/50, Batch 6000/7628\n","Epoch 14/50, Batch 6100/7628\n","Epoch 14/50, Batch 6200/7628\n","Epoch 14/50, Batch 6300/7628\n","Epoch 14/50, Batch 6400/7628\n","Epoch 14/50, Batch 6500/7628\n","Epoch 14/50, Batch 6600/7628\n","Epoch 14/50, Batch 6700/7628\n","Epoch 14/50, Batch 6800/7628\n","Epoch 14/50, Batch 6900/7628\n","Epoch 14/50, Batch 7000/7628\n","Epoch 14/50, Batch 7100/7628\n","Epoch 14/50, Batch 7200/7628\n","Epoch 14/50, Batch 7300/7628\n","Epoch 14/50, Batch 7400/7628\n","Epoch 14/50, Batch 7500/7628\n","Epoch 14/50, Batch 7600/7628\n","Epoch 14/50, Loss: 0.4454\n","Best model saved with loss: 0.4454\n","Epoch 15/50, Batch 0/7628\n","Epoch 15/50, Batch 100/7628\n","Epoch 15/50, Batch 200/7628\n","Epoch 15/50, Batch 300/7628\n","Epoch 15/50, Batch 400/7628\n","Epoch 15/50, Batch 500/7628\n","Epoch 15/50, Batch 600/7628\n","Epoch 15/50, Batch 700/7628\n","Epoch 15/50, Batch 800/7628\n","Epoch 15/50, Batch 900/7628\n","Epoch 15/50, Batch 1000/7628\n","Epoch 15/50, Batch 1100/7628\n","Epoch 15/50, Batch 1200/7628\n","Epoch 15/50, Batch 1300/7628\n","Epoch 15/50, Batch 1400/7628\n","Epoch 15/50, Batch 1500/7628\n","Epoch 15/50, Batch 1600/7628\n","Epoch 15/50, Batch 1700/7628\n","Epoch 15/50, Batch 1800/7628\n","Epoch 15/50, Batch 1900/7628\n","Epoch 15/50, Batch 2000/7628\n","Epoch 15/50, Batch 2100/7628\n","Epoch 15/50, Batch 2200/7628\n","Epoch 15/50, Batch 2300/7628\n","Epoch 15/50, Batch 2400/7628\n","Epoch 15/50, Batch 2500/7628\n","Epoch 15/50, Batch 2600/7628\n","Epoch 15/50, Batch 2700/7628\n","Epoch 15/50, Batch 2800/7628\n","Epoch 15/50, Batch 2900/7628\n","Epoch 15/50, Batch 3000/7628\n","Epoch 15/50, Batch 3100/7628\n","Epoch 15/50, Batch 3200/7628\n","Epoch 15/50, Batch 3300/7628\n","Epoch 15/50, Batch 3400/7628\n","Epoch 15/50, Batch 3500/7628\n","Epoch 15/50, Batch 3600/7628\n","Epoch 15/50, Batch 3700/7628\n","Epoch 15/50, Batch 3800/7628\n","Epoch 15/50, Batch 3900/7628\n","Epoch 15/50, Batch 4000/7628\n","Epoch 15/50, Batch 4100/7628\n","Epoch 15/50, Batch 4200/7628\n","Epoch 15/50, Batch 4300/7628\n","Epoch 15/50, Batch 4400/7628\n","Epoch 15/50, Batch 4500/7628\n","Epoch 15/50, Batch 4600/7628\n","Epoch 15/50, Batch 4700/7628\n","Epoch 15/50, Batch 4800/7628\n","Epoch 15/50, Batch 4900/7628\n","Epoch 15/50, Batch 5000/7628\n","Epoch 15/50, Batch 5100/7628\n","Epoch 15/50, Batch 5200/7628\n","Epoch 15/50, Batch 5300/7628\n","Epoch 15/50, Batch 5400/7628\n","Epoch 15/50, Batch 5500/7628\n","Epoch 15/50, Batch 5600/7628\n","Epoch 15/50, Batch 5700/7628\n","Epoch 15/50, Batch 5800/7628\n","Epoch 15/50, Batch 5900/7628\n","Epoch 15/50, Batch 6000/7628\n","Epoch 15/50, Batch 6100/7628\n","Epoch 15/50, Batch 6200/7628\n","Epoch 15/50, Batch 6300/7628\n","Epoch 15/50, Batch 6400/7628\n","Epoch 15/50, Batch 6500/7628\n","Epoch 15/50, Batch 6600/7628\n","Epoch 15/50, Batch 6700/7628\n","Epoch 15/50, Batch 6800/7628\n","Epoch 15/50, Batch 6900/7628\n","Epoch 15/50, Batch 7000/7628\n","Epoch 15/50, Batch 7100/7628\n","Epoch 15/50, Batch 7200/7628\n","Epoch 15/50, Batch 7300/7628\n","Epoch 15/50, Batch 7400/7628\n","Epoch 15/50, Batch 7500/7628\n","Epoch 15/50, Batch 7600/7628\n","Epoch 15/50, Loss: 0.4454\n","Best model saved with loss: 0.4454\n","Epoch 16/50, Batch 0/7628\n","Epoch 16/50, Batch 100/7628\n","Epoch 16/50, Batch 200/7628\n","Epoch 16/50, Batch 300/7628\n","Epoch 16/50, Batch 400/7628\n","Epoch 16/50, Batch 500/7628\n","Epoch 16/50, Batch 600/7628\n","Epoch 16/50, Batch 700/7628\n","Epoch 16/50, Batch 800/7628\n","Epoch 16/50, Batch 900/7628\n","Epoch 16/50, Batch 1000/7628\n","Epoch 16/50, Batch 1100/7628\n","Epoch 16/50, Batch 1200/7628\n","Epoch 16/50, Batch 1300/7628\n","Epoch 16/50, Batch 1400/7628\n","Epoch 16/50, Batch 1500/7628\n","Epoch 16/50, Batch 1600/7628\n","Epoch 16/50, Batch 1700/7628\n","Epoch 16/50, Batch 1800/7628\n","Epoch 16/50, Batch 1900/7628\n","Epoch 16/50, Batch 2000/7628\n","Epoch 16/50, Batch 2100/7628\n","Epoch 16/50, Batch 2200/7628\n","Epoch 16/50, Batch 2300/7628\n","Epoch 16/50, Batch 2400/7628\n","Epoch 16/50, Batch 2500/7628\n","Epoch 16/50, Batch 2600/7628\n","Epoch 16/50, Batch 2700/7628\n","Epoch 16/50, Batch 2800/7628\n","Epoch 16/50, Batch 2900/7628\n","Epoch 16/50, Batch 3000/7628\n","Epoch 16/50, Batch 3100/7628\n","Epoch 16/50, Batch 3200/7628\n","Epoch 16/50, Batch 3300/7628\n","Epoch 16/50, Batch 3400/7628\n","Epoch 16/50, Batch 3500/7628\n","Epoch 16/50, Batch 3600/7628\n","Epoch 16/50, Batch 3700/7628\n","Epoch 16/50, Batch 3800/7628\n","Epoch 16/50, Batch 3900/7628\n","Epoch 16/50, Batch 4000/7628\n","Epoch 16/50, Batch 4100/7628\n","Epoch 16/50, Batch 4200/7628\n","Epoch 16/50, Batch 4300/7628\n","Epoch 16/50, Batch 4400/7628\n","Epoch 16/50, Batch 4500/7628\n","Epoch 16/50, Batch 4600/7628\n","Epoch 16/50, Batch 4700/7628\n","Epoch 16/50, Batch 4800/7628\n","Epoch 16/50, Batch 4900/7628\n","Epoch 16/50, Batch 5000/7628\n","Epoch 16/50, Batch 5100/7628\n","Epoch 16/50, Batch 5200/7628\n","Epoch 16/50, Batch 5300/7628\n","Epoch 16/50, Batch 5400/7628\n","Epoch 16/50, Batch 5500/7628\n","Epoch 16/50, Batch 5600/7628\n","Epoch 16/50, Batch 5700/7628\n","Epoch 16/50, Batch 5800/7628\n","Epoch 16/50, Batch 5900/7628\n","Epoch 16/50, Batch 6000/7628\n","Epoch 16/50, Batch 6100/7628\n","Epoch 16/50, Batch 6200/7628\n","Epoch 16/50, Batch 6300/7628\n","Epoch 16/50, Batch 6400/7628\n","Epoch 16/50, Batch 6500/7628\n","Epoch 16/50, Batch 6600/7628\n","Epoch 16/50, Batch 6700/7628\n","Epoch 16/50, Batch 6800/7628\n","Epoch 16/50, Batch 6900/7628\n","Epoch 16/50, Batch 7000/7628\n","Epoch 16/50, Batch 7100/7628\n","Epoch 16/50, Batch 7200/7628\n","Epoch 16/50, Batch 7300/7628\n","Epoch 16/50, Batch 7400/7628\n","Epoch 16/50, Batch 7500/7628\n","Epoch 16/50, Batch 7600/7628\n","Epoch 16/50, Loss: 0.4454\n","Best model saved with loss: 0.4454\n","Epoch 17/50, Batch 0/7628\n","Epoch 17/50, Batch 100/7628\n","Epoch 17/50, Batch 200/7628\n","Epoch 17/50, Batch 300/7628\n","Epoch 17/50, Batch 400/7628\n","Epoch 17/50, Batch 500/7628\n","Epoch 17/50, Batch 600/7628\n","Epoch 17/50, Batch 700/7628\n","Epoch 17/50, Batch 800/7628\n","Epoch 17/50, Batch 900/7628\n","Epoch 17/50, Batch 1000/7628\n","Epoch 17/50, Batch 1100/7628\n","Epoch 17/50, Batch 1200/7628\n","Epoch 17/50, Batch 1300/7628\n","Epoch 17/50, Batch 1400/7628\n","Epoch 17/50, Batch 1500/7628\n","Epoch 17/50, Batch 1600/7628\n","Epoch 17/50, Batch 1700/7628\n","Epoch 17/50, Batch 1800/7628\n","Epoch 17/50, Batch 1900/7628\n","Epoch 17/50, Batch 2000/7628\n","Epoch 17/50, Batch 2100/7628\n","Epoch 17/50, Batch 2200/7628\n","Epoch 17/50, Batch 2300/7628\n","Epoch 17/50, Batch 2400/7628\n","Epoch 17/50, Batch 2500/7628\n","Epoch 17/50, Batch 2600/7628\n","Epoch 17/50, Batch 2700/7628\n","Epoch 17/50, Batch 2800/7628\n","Epoch 17/50, Batch 2900/7628\n","Epoch 17/50, Batch 3000/7628\n","Epoch 17/50, Batch 3100/7628\n","Epoch 17/50, Batch 3200/7628\n","Epoch 17/50, Batch 3300/7628\n","Epoch 17/50, Batch 3400/7628\n","Epoch 17/50, Batch 3500/7628\n","Epoch 17/50, Batch 3600/7628\n","Epoch 17/50, Batch 3700/7628\n","Epoch 17/50, Batch 3800/7628\n","Epoch 17/50, Batch 3900/7628\n","Epoch 17/50, Batch 4000/7628\n","Epoch 17/50, Batch 4100/7628\n","Epoch 17/50, Batch 4200/7628\n","Epoch 17/50, Batch 4300/7628\n","Epoch 17/50, Batch 4400/7628\n","Epoch 17/50, Batch 4500/7628\n","Epoch 17/50, Batch 4600/7628\n","Epoch 17/50, Batch 4700/7628\n","Epoch 17/50, Batch 4800/7628\n","Epoch 17/50, Batch 4900/7628\n","Epoch 17/50, Batch 5000/7628\n","Epoch 17/50, Batch 5100/7628\n","Epoch 17/50, Batch 5200/7628\n","Epoch 17/50, Batch 5300/7628\n","Epoch 17/50, Batch 5400/7628\n","Epoch 17/50, Batch 5500/7628\n","Epoch 17/50, Batch 5600/7628\n","Epoch 17/50, Batch 5700/7628\n","Epoch 17/50, Batch 5800/7628\n","Epoch 17/50, Batch 5900/7628\n","Epoch 17/50, Batch 6000/7628\n","Epoch 17/50, Batch 6100/7628\n","Epoch 17/50, Batch 6200/7628\n","Epoch 17/50, Batch 6300/7628\n","Epoch 17/50, Batch 6400/7628\n","Epoch 17/50, Batch 6500/7628\n","Epoch 17/50, Batch 6600/7628\n","Epoch 17/50, Batch 6700/7628\n","Epoch 17/50, Batch 6800/7628\n","Epoch 17/50, Batch 6900/7628\n","Epoch 17/50, Batch 7000/7628\n","Epoch 17/50, Batch 7100/7628\n","Epoch 17/50, Batch 7200/7628\n","Epoch 17/50, Batch 7300/7628\n","Epoch 17/50, Batch 7400/7628\n","Epoch 17/50, Batch 7500/7628\n","Epoch 17/50, Batch 7600/7628\n","Epoch 17/50, Loss: 0.4454\n","Best model saved with loss: 0.4454\n","Epoch 18/50, Batch 0/7628\n","Epoch 18/50, Batch 100/7628\n","Epoch 18/50, Batch 200/7628\n","Epoch 18/50, Batch 300/7628\n","Epoch 18/50, Batch 400/7628\n","Epoch 18/50, Batch 500/7628\n","Epoch 18/50, Batch 600/7628\n","Epoch 18/50, Batch 700/7628\n","Epoch 18/50, Batch 800/7628\n","Epoch 18/50, Batch 900/7628\n","Epoch 18/50, Batch 1000/7628\n","Epoch 18/50, Batch 1100/7628\n","Epoch 18/50, Batch 1200/7628\n","Epoch 18/50, Batch 1300/7628\n","Epoch 18/50, Batch 1400/7628\n","Epoch 18/50, Batch 1500/7628\n","Epoch 18/50, Batch 1600/7628\n","Epoch 18/50, Batch 1700/7628\n","Epoch 18/50, Batch 1800/7628\n","Epoch 18/50, Batch 1900/7628\n","Epoch 18/50, Batch 2000/7628\n","Epoch 18/50, Batch 2100/7628\n","Epoch 18/50, Batch 2200/7628\n","Epoch 18/50, Batch 2300/7628\n","Epoch 18/50, Batch 2400/7628\n","Epoch 18/50, Batch 2500/7628\n","Epoch 18/50, Batch 2600/7628\n","Epoch 18/50, Batch 2700/7628\n","Epoch 18/50, Batch 2800/7628\n","Epoch 18/50, Batch 2900/7628\n","Epoch 18/50, Batch 3000/7628\n","Epoch 18/50, Batch 3100/7628\n","Epoch 18/50, Batch 3200/7628\n","Epoch 18/50, Batch 3300/7628\n","Epoch 18/50, Batch 3400/7628\n","Epoch 18/50, Batch 3500/7628\n","Epoch 18/50, Batch 3600/7628\n","Epoch 18/50, Batch 3700/7628\n","Epoch 18/50, Batch 3800/7628\n","Epoch 18/50, Batch 3900/7628\n","Epoch 18/50, Batch 4000/7628\n","Epoch 18/50, Batch 4100/7628\n","Epoch 18/50, Batch 4200/7628\n","Epoch 18/50, Batch 4300/7628\n","Epoch 18/50, Batch 4400/7628\n","Epoch 18/50, Batch 4500/7628\n","Epoch 18/50, Batch 4600/7628\n","Epoch 18/50, Batch 4700/7628\n","Epoch 18/50, Batch 4800/7628\n","Epoch 18/50, Batch 4900/7628\n","Epoch 18/50, Batch 5000/7628\n","Epoch 18/50, Batch 5100/7628\n","Epoch 18/50, Batch 5200/7628\n","Epoch 18/50, Batch 5300/7628\n","Epoch 18/50, Batch 5400/7628\n","Epoch 18/50, Batch 5500/7628\n","Epoch 18/50, Batch 5600/7628\n","Epoch 18/50, Batch 5700/7628\n","Epoch 18/50, Batch 5800/7628\n","Epoch 18/50, Batch 5900/7628\n","Epoch 18/50, Batch 6000/7628\n","Epoch 18/50, Batch 6100/7628\n","Epoch 18/50, Batch 6200/7628\n","Epoch 18/50, Batch 6300/7628\n","Epoch 18/50, Batch 6400/7628\n","Epoch 18/50, Batch 6500/7628\n","Epoch 18/50, Batch 6600/7628\n","Epoch 18/50, Batch 6700/7628\n","Epoch 18/50, Batch 6800/7628\n","Epoch 18/50, Batch 6900/7628\n","Epoch 18/50, Batch 7000/7628\n","Epoch 18/50, Batch 7100/7628\n","Epoch 18/50, Batch 7200/7628\n","Epoch 18/50, Batch 7300/7628\n","Epoch 18/50, Batch 7400/7628\n","Epoch 18/50, Batch 7500/7628\n","Epoch 18/50, Batch 7600/7628\n","Epoch 18/50, Loss: 0.4453\n","Best model saved with loss: 0.4453\n","Epoch 19/50, Batch 0/7628\n","Epoch 19/50, Batch 100/7628\n","Epoch 19/50, Batch 200/7628\n","Epoch 19/50, Batch 300/7628\n","Epoch 19/50, Batch 400/7628\n","Epoch 19/50, Batch 500/7628\n","Epoch 19/50, Batch 600/7628\n","Epoch 19/50, Batch 700/7628\n","Epoch 19/50, Batch 800/7628\n","Epoch 19/50, Batch 900/7628\n","Epoch 19/50, Batch 1000/7628\n","Epoch 19/50, Batch 1100/7628\n","Epoch 19/50, Batch 1200/7628\n","Epoch 19/50, Batch 1300/7628\n","Epoch 19/50, Batch 1400/7628\n","Epoch 19/50, Batch 1500/7628\n","Epoch 19/50, Batch 1600/7628\n","Epoch 19/50, Batch 1700/7628\n","Epoch 19/50, Batch 1800/7628\n","Epoch 19/50, Batch 1900/7628\n","Epoch 19/50, Batch 2000/7628\n","Epoch 19/50, Batch 2100/7628\n","Epoch 19/50, Batch 2200/7628\n","Epoch 19/50, Batch 2300/7628\n","Epoch 19/50, Batch 2400/7628\n","Epoch 19/50, Batch 2500/7628\n","Epoch 19/50, Batch 2600/7628\n","Epoch 19/50, Batch 2700/7628\n","Epoch 19/50, Batch 2800/7628\n","Epoch 19/50, Batch 2900/7628\n","Epoch 19/50, Batch 3000/7628\n","Epoch 19/50, Batch 3100/7628\n","Epoch 19/50, Batch 3200/7628\n","Epoch 19/50, Batch 3300/7628\n","Epoch 19/50, Batch 3400/7628\n","Epoch 19/50, Batch 3500/7628\n","Epoch 19/50, Batch 3600/7628\n","Epoch 19/50, Batch 3700/7628\n","Epoch 19/50, Batch 3800/7628\n","Epoch 19/50, Batch 3900/7628\n","Epoch 19/50, Batch 4000/7628\n","Epoch 19/50, Batch 4100/7628\n","Epoch 19/50, Batch 4200/7628\n","Epoch 19/50, Batch 4300/7628\n","Epoch 19/50, Batch 4400/7628\n","Epoch 19/50, Batch 4500/7628\n","Epoch 19/50, Batch 4600/7628\n","Epoch 19/50, Batch 4700/7628\n","Epoch 19/50, Batch 4800/7628\n","Epoch 19/50, Batch 4900/7628\n","Epoch 19/50, Batch 5000/7628\n","Epoch 19/50, Batch 5100/7628\n","Epoch 19/50, Batch 5200/7628\n","Epoch 19/50, Batch 5300/7628\n","Epoch 19/50, Batch 5400/7628\n","Epoch 19/50, Batch 5500/7628\n","Epoch 19/50, Batch 5600/7628\n","Epoch 19/50, Batch 5700/7628\n","Epoch 19/50, Batch 5800/7628\n","Epoch 19/50, Batch 5900/7628\n","Epoch 19/50, Batch 6000/7628\n","Epoch 19/50, Batch 6100/7628\n","Epoch 19/50, Batch 6200/7628\n","Epoch 19/50, Batch 6300/7628\n","Epoch 19/50, Batch 6400/7628\n","Epoch 19/50, Batch 6500/7628\n","Epoch 19/50, Batch 6600/7628\n","Epoch 19/50, Batch 6700/7628\n","Epoch 19/50, Batch 6800/7628\n","Epoch 19/50, Batch 6900/7628\n","Epoch 19/50, Batch 7000/7628\n","Epoch 19/50, Batch 7100/7628\n","Epoch 19/50, Batch 7200/7628\n","Epoch 19/50, Batch 7300/7628\n","Epoch 19/50, Batch 7400/7628\n","Epoch 19/50, Batch 7500/7628\n","Epoch 19/50, Batch 7600/7628\n","Epoch 19/50, Loss: 0.4453\n","Best model saved with loss: 0.4453\n","Epoch 20/50, Batch 0/7628\n","Epoch 20/50, Batch 100/7628\n","Epoch 20/50, Batch 200/7628\n","Epoch 20/50, Batch 300/7628\n","Epoch 20/50, Batch 400/7628\n","Epoch 20/50, Batch 500/7628\n","Epoch 20/50, Batch 600/7628\n","Epoch 20/50, Batch 700/7628\n","Epoch 20/50, Batch 800/7628\n","Epoch 20/50, Batch 900/7628\n","Epoch 20/50, Batch 1000/7628\n","Epoch 20/50, Batch 1100/7628\n","Epoch 20/50, Batch 1200/7628\n","Epoch 20/50, Batch 1300/7628\n","Epoch 20/50, Batch 1400/7628\n","Epoch 20/50, Batch 1500/7628\n","Epoch 20/50, Batch 1600/7628\n","Epoch 20/50, Batch 1700/7628\n","Epoch 20/50, Batch 1800/7628\n","Epoch 20/50, Batch 1900/7628\n","Epoch 20/50, Batch 2000/7628\n","Epoch 20/50, Batch 2100/7628\n","Epoch 20/50, Batch 2200/7628\n","Epoch 20/50, Batch 2300/7628\n","Epoch 20/50, Batch 2400/7628\n","Epoch 20/50, Batch 2500/7628\n","Epoch 20/50, Batch 2600/7628\n","Epoch 20/50, Batch 2700/7628\n","Epoch 20/50, Batch 2800/7628\n","Epoch 20/50, Batch 2900/7628\n","Epoch 20/50, Batch 3000/7628\n","Epoch 20/50, Batch 3100/7628\n","Epoch 20/50, Batch 3200/7628\n","Epoch 20/50, Batch 3300/7628\n","Epoch 20/50, Batch 3400/7628\n","Epoch 20/50, Batch 3500/7628\n","Epoch 20/50, Batch 3600/7628\n","Epoch 20/50, Batch 3700/7628\n","Epoch 20/50, Batch 3800/7628\n","Epoch 20/50, Batch 3900/7628\n","Epoch 20/50, Batch 4000/7628\n","Epoch 20/50, Batch 4100/7628\n","Epoch 20/50, Batch 4200/7628\n","Epoch 20/50, Batch 4300/7628\n","Epoch 20/50, Batch 4400/7628\n","Epoch 20/50, Batch 4500/7628\n","Epoch 20/50, Batch 4600/7628\n","Epoch 20/50, Batch 4700/7628\n","Epoch 20/50, Batch 4800/7628\n","Epoch 20/50, Batch 4900/7628\n","Epoch 20/50, Batch 5000/7628\n","Epoch 20/50, Batch 5100/7628\n","Epoch 20/50, Batch 5200/7628\n","Epoch 20/50, Batch 5300/7628\n","Epoch 20/50, Batch 5400/7628\n","Epoch 20/50, Batch 5500/7628\n","Epoch 20/50, Batch 5600/7628\n","Epoch 20/50, Batch 5700/7628\n","Epoch 20/50, Batch 5800/7628\n","Epoch 20/50, Batch 5900/7628\n","Epoch 20/50, Batch 6000/7628\n","Epoch 20/50, Batch 6100/7628\n","Epoch 20/50, Batch 6200/7628\n","Epoch 20/50, Batch 6300/7628\n","Epoch 20/50, Batch 6400/7628\n","Epoch 20/50, Batch 6500/7628\n","Epoch 20/50, Batch 6600/7628\n","Epoch 20/50, Batch 6700/7628\n","Epoch 20/50, Batch 6800/7628\n","Epoch 20/50, Batch 6900/7628\n","Epoch 20/50, Batch 7000/7628\n","Epoch 20/50, Batch 7100/7628\n","Epoch 20/50, Batch 7200/7628\n","Epoch 20/50, Batch 7300/7628\n","Epoch 20/50, Batch 7400/7628\n","Epoch 20/50, Batch 7500/7628\n","Epoch 20/50, Batch 7600/7628\n","Epoch 20/50, Loss: 0.4453\n","Best model saved with loss: 0.4453\n","Epoch 21/50, Batch 0/7628\n","Epoch 21/50, Batch 100/7628\n","Epoch 21/50, Batch 200/7628\n","Epoch 21/50, Batch 300/7628\n","Epoch 21/50, Batch 400/7628\n","Epoch 21/50, Batch 500/7628\n","Epoch 21/50, Batch 600/7628\n","Epoch 21/50, Batch 700/7628\n","Epoch 21/50, Batch 800/7628\n","Epoch 21/50, Batch 900/7628\n","Epoch 21/50, Batch 1000/7628\n","Epoch 21/50, Batch 1100/7628\n","Epoch 21/50, Batch 1200/7628\n","Epoch 21/50, Batch 1300/7628\n","Epoch 21/50, Batch 1400/7628\n","Epoch 21/50, Batch 1500/7628\n","Epoch 21/50, Batch 1600/7628\n","Epoch 21/50, Batch 1700/7628\n","Epoch 21/50, Batch 1800/7628\n","Epoch 21/50, Batch 1900/7628\n","Epoch 21/50, Batch 2000/7628\n","Epoch 21/50, Batch 2100/7628\n","Epoch 21/50, Batch 2200/7628\n","Epoch 21/50, Batch 2300/7628\n","Epoch 21/50, Batch 2400/7628\n","Epoch 21/50, Batch 2500/7628\n","Epoch 21/50, Batch 2600/7628\n","Epoch 21/50, Batch 2700/7628\n","Epoch 21/50, Batch 2800/7628\n","Epoch 21/50, Batch 2900/7628\n","Epoch 21/50, Batch 3000/7628\n","Epoch 21/50, Batch 3100/7628\n","Epoch 21/50, Batch 3200/7628\n","Epoch 21/50, Batch 3300/7628\n","Epoch 21/50, Batch 3400/7628\n","Epoch 21/50, Batch 3500/7628\n","Epoch 21/50, Batch 3600/7628\n","Epoch 21/50, Batch 3700/7628\n","Epoch 21/50, Batch 3800/7628\n","Epoch 21/50, Batch 3900/7628\n","Epoch 21/50, Batch 4000/7628\n","Epoch 21/50, Batch 4100/7628\n","Epoch 21/50, Batch 4200/7628\n","Epoch 21/50, Batch 4300/7628\n","Epoch 21/50, Batch 4400/7628\n","Epoch 21/50, Batch 4500/7628\n","Epoch 21/50, Batch 4600/7628\n","Epoch 21/50, Batch 4700/7628\n","Epoch 21/50, Batch 4800/7628\n","Epoch 21/50, Batch 4900/7628\n","Epoch 21/50, Batch 5000/7628\n","Epoch 21/50, Batch 5100/7628\n","Epoch 21/50, Batch 5200/7628\n","Epoch 21/50, Batch 5300/7628\n","Epoch 21/50, Batch 5400/7628\n","Epoch 21/50, Batch 5500/7628\n","Epoch 21/50, Batch 5600/7628\n","Epoch 21/50, Batch 5700/7628\n","Epoch 21/50, Batch 5800/7628\n","Epoch 21/50, Batch 5900/7628\n","Epoch 21/50, Batch 6000/7628\n","Epoch 21/50, Batch 6100/7628\n","Epoch 21/50, Batch 6200/7628\n","Epoch 21/50, Batch 6300/7628\n","Epoch 21/50, Batch 6400/7628\n","Epoch 21/50, Batch 6500/7628\n","Epoch 21/50, Batch 6600/7628\n","Epoch 21/50, Batch 6700/7628\n","Epoch 21/50, Batch 6800/7628\n","Epoch 21/50, Batch 6900/7628\n","Epoch 21/50, Batch 7000/7628\n","Epoch 21/50, Batch 7100/7628\n","Epoch 21/50, Batch 7200/7628\n","Epoch 21/50, Batch 7300/7628\n","Epoch 21/50, Batch 7400/7628\n","Epoch 21/50, Batch 7500/7628\n","Epoch 21/50, Batch 7600/7628\n","Epoch 21/50, Loss: 0.4453\n","Best model saved with loss: 0.4453\n","Epoch 22/50, Batch 0/7628\n","Epoch 22/50, Batch 100/7628\n","Epoch 22/50, Batch 200/7628\n","Epoch 22/50, Batch 300/7628\n","Epoch 22/50, Batch 400/7628\n","Epoch 22/50, Batch 500/7628\n","Epoch 22/50, Batch 600/7628\n","Epoch 22/50, Batch 700/7628\n","Epoch 22/50, Batch 800/7628\n","Epoch 22/50, Batch 900/7628\n","Epoch 22/50, Batch 1000/7628\n","Epoch 22/50, Batch 1100/7628\n","Epoch 22/50, Batch 1200/7628\n","Epoch 22/50, Batch 1300/7628\n","Epoch 22/50, Batch 1400/7628\n","Epoch 22/50, Batch 1500/7628\n","Epoch 22/50, Batch 1600/7628\n","Epoch 22/50, Batch 1700/7628\n","Epoch 22/50, Batch 1800/7628\n","Epoch 22/50, Batch 1900/7628\n","Epoch 22/50, Batch 2000/7628\n","Epoch 22/50, Batch 2100/7628\n","Epoch 22/50, Batch 2200/7628\n","Epoch 22/50, Batch 2300/7628\n","Epoch 22/50, Batch 2400/7628\n","Epoch 22/50, Batch 2500/7628\n","Epoch 22/50, Batch 2600/7628\n","Epoch 22/50, Batch 2700/7628\n","Epoch 22/50, Batch 2800/7628\n","Epoch 22/50, Batch 2900/7628\n","Epoch 22/50, Batch 3000/7628\n","Epoch 22/50, Batch 3100/7628\n","Epoch 22/50, Batch 3200/7628\n","Epoch 22/50, Batch 3300/7628\n","Epoch 22/50, Batch 3400/7628\n","Epoch 22/50, Batch 3500/7628\n","Epoch 22/50, Batch 3600/7628\n","Epoch 22/50, Batch 3700/7628\n","Epoch 22/50, Batch 3800/7628\n","Epoch 22/50, Batch 3900/7628\n","Epoch 22/50, Batch 4000/7628\n","Epoch 22/50, Batch 4100/7628\n","Epoch 22/50, Batch 4200/7628\n","Epoch 22/50, Batch 4300/7628\n","Epoch 22/50, Batch 4400/7628\n","Epoch 22/50, Batch 4500/7628\n","Epoch 22/50, Batch 4600/7628\n","Epoch 22/50, Batch 4700/7628\n","Epoch 22/50, Batch 4800/7628\n","Epoch 22/50, Batch 4900/7628\n","Epoch 22/50, Batch 5000/7628\n","Epoch 22/50, Batch 5100/7628\n","Epoch 22/50, Batch 5200/7628\n","Epoch 22/50, Batch 5300/7628\n","Epoch 22/50, Batch 5400/7628\n","Epoch 22/50, Batch 5500/7628\n","Epoch 22/50, Batch 5600/7628\n","Epoch 22/50, Batch 5700/7628\n","Epoch 22/50, Batch 5800/7628\n","Epoch 22/50, Batch 5900/7628\n","Epoch 22/50, Batch 6000/7628\n","Epoch 22/50, Batch 6100/7628\n","Epoch 22/50, Batch 6200/7628\n","Epoch 22/50, Batch 6300/7628\n","Epoch 22/50, Batch 6400/7628\n","Epoch 22/50, Batch 6500/7628\n","Epoch 22/50, Batch 6600/7628\n","Epoch 22/50, Batch 6700/7628\n","Epoch 22/50, Batch 6800/7628\n","Epoch 22/50, Batch 6900/7628\n","Epoch 22/50, Batch 7000/7628\n","Epoch 22/50, Batch 7100/7628\n","Epoch 22/50, Batch 7200/7628\n","Epoch 22/50, Batch 7300/7628\n","Epoch 22/50, Batch 7400/7628\n","Epoch 22/50, Batch 7500/7628\n","Epoch 22/50, Batch 7600/7628\n","Epoch 22/50, Loss: 0.4453\n","Best model saved with loss: 0.4453\n","Epoch 23/50, Batch 0/7628\n","Epoch 23/50, Batch 100/7628\n","Epoch 23/50, Batch 200/7628\n","Epoch 23/50, Batch 300/7628\n","Epoch 23/50, Batch 400/7628\n","Epoch 23/50, Batch 500/7628\n","Epoch 23/50, Batch 600/7628\n","Epoch 23/50, Batch 700/7628\n","Epoch 23/50, Batch 800/7628\n","Epoch 23/50, Batch 900/7628\n","Epoch 23/50, Batch 1000/7628\n","Epoch 23/50, Batch 1100/7628\n","Epoch 23/50, Batch 1200/7628\n","Epoch 23/50, Batch 1300/7628\n","Epoch 23/50, Batch 1400/7628\n","Epoch 23/50, Batch 1500/7628\n","Epoch 23/50, Batch 1600/7628\n","Epoch 23/50, Batch 1700/7628\n","Epoch 23/50, Batch 1800/7628\n","Epoch 23/50, Batch 1900/7628\n","Epoch 23/50, Batch 2000/7628\n","Epoch 23/50, Batch 2100/7628\n","Epoch 23/50, Batch 2200/7628\n","Epoch 23/50, Batch 2300/7628\n","Epoch 23/50, Batch 2400/7628\n","Epoch 23/50, Batch 2500/7628\n","Epoch 23/50, Batch 2600/7628\n","Epoch 23/50, Batch 2700/7628\n","Epoch 23/50, Batch 2800/7628\n","Epoch 23/50, Batch 2900/7628\n","Epoch 23/50, Batch 3000/7628\n","Epoch 23/50, Batch 3100/7628\n","Epoch 23/50, Batch 3200/7628\n","Epoch 23/50, Batch 3300/7628\n","Epoch 23/50, Batch 3400/7628\n","Epoch 23/50, Batch 3500/7628\n","Epoch 23/50, Batch 3600/7628\n","Epoch 23/50, Batch 3700/7628\n","Epoch 23/50, Batch 3800/7628\n","Epoch 23/50, Batch 3900/7628\n","Epoch 23/50, Batch 4000/7628\n","Epoch 23/50, Batch 4100/7628\n","Epoch 23/50, Batch 4200/7628\n","Epoch 23/50, Batch 4300/7628\n","Epoch 23/50, Batch 4400/7628\n","Epoch 23/50, Batch 4500/7628\n","Epoch 23/50, Batch 4600/7628\n","Epoch 23/50, Batch 4700/7628\n","Epoch 23/50, Batch 4800/7628\n","Epoch 23/50, Batch 4900/7628\n","Epoch 23/50, Batch 5000/7628\n","Epoch 23/50, Batch 5100/7628\n","Epoch 23/50, Batch 5200/7628\n","Epoch 23/50, Batch 5300/7628\n","Epoch 23/50, Batch 5400/7628\n","Epoch 23/50, Batch 5500/7628\n","Epoch 23/50, Batch 5600/7628\n","Epoch 23/50, Batch 5700/7628\n","Epoch 23/50, Batch 5800/7628\n","Epoch 23/50, Batch 5900/7628\n","Epoch 23/50, Batch 6000/7628\n","Epoch 23/50, Batch 6100/7628\n","Epoch 23/50, Batch 6200/7628\n","Epoch 23/50, Batch 6300/7628\n","Epoch 23/50, Batch 6400/7628\n","Epoch 23/50, Batch 6500/7628\n","Epoch 23/50, Batch 6600/7628\n","Epoch 23/50, Batch 6700/7628\n","Epoch 23/50, Batch 6800/7628\n","Epoch 23/50, Batch 6900/7628\n","Epoch 23/50, Batch 7000/7628\n","Epoch 23/50, Batch 7100/7628\n","Epoch 23/50, Batch 7200/7628\n","Epoch 23/50, Batch 7300/7628\n","Epoch 23/50, Batch 7400/7628\n","Epoch 23/50, Batch 7500/7628\n","Epoch 23/50, Batch 7600/7628\n","Epoch 23/50, Loss: 0.4453\n","Best model saved with loss: 0.4453\n","Epoch 24/50, Batch 0/7628\n","Epoch 24/50, Batch 100/7628\n","Epoch 24/50, Batch 200/7628\n","Epoch 24/50, Batch 300/7628\n","Epoch 24/50, Batch 400/7628\n","Epoch 24/50, Batch 500/7628\n","Epoch 24/50, Batch 600/7628\n","Epoch 24/50, Batch 700/7628\n","Epoch 24/50, Batch 800/7628\n","Epoch 24/50, Batch 900/7628\n","Epoch 24/50, Batch 1000/7628\n","Epoch 24/50, Batch 1100/7628\n","Epoch 24/50, Batch 1200/7628\n","Epoch 24/50, Batch 1300/7628\n","Epoch 24/50, Batch 1400/7628\n","Epoch 24/50, Batch 1500/7628\n","Epoch 24/50, Batch 1600/7628\n","Epoch 24/50, Batch 1700/7628\n","Epoch 24/50, Batch 1800/7628\n","Epoch 24/50, Batch 1900/7628\n","Epoch 24/50, Batch 2000/7628\n","Epoch 24/50, Batch 2100/7628\n","Epoch 24/50, Batch 2200/7628\n","Epoch 24/50, Batch 2300/7628\n","Epoch 24/50, Batch 2400/7628\n","Epoch 24/50, Batch 2500/7628\n","Epoch 24/50, Batch 2600/7628\n","Epoch 24/50, Batch 2700/7628\n","Epoch 24/50, Batch 2800/7628\n","Epoch 24/50, Batch 2900/7628\n","Epoch 24/50, Batch 3000/7628\n","Epoch 24/50, Batch 3100/7628\n","Epoch 24/50, Batch 3200/7628\n","Epoch 24/50, Batch 3300/7628\n","Epoch 24/50, Batch 3400/7628\n","Epoch 24/50, Batch 3500/7628\n","Epoch 24/50, Batch 3600/7628\n","Epoch 24/50, Batch 3700/7628\n","Epoch 24/50, Batch 3800/7628\n","Epoch 24/50, Batch 3900/7628\n","Epoch 24/50, Batch 4000/7628\n","Epoch 24/50, Batch 4100/7628\n","Epoch 24/50, Batch 4200/7628\n","Epoch 24/50, Batch 4300/7628\n","Epoch 24/50, Batch 4400/7628\n","Epoch 24/50, Batch 4500/7628\n","Epoch 24/50, Batch 4600/7628\n","Epoch 24/50, Batch 4700/7628\n","Epoch 24/50, Batch 4800/7628\n","Epoch 24/50, Batch 4900/7628\n","Epoch 24/50, Batch 5000/7628\n","Epoch 24/50, Batch 5100/7628\n","Epoch 24/50, Batch 5200/7628\n","Epoch 24/50, Batch 5300/7628\n","Epoch 24/50, Batch 5400/7628\n","Epoch 24/50, Batch 5500/7628\n","Epoch 24/50, Batch 5600/7628\n","Epoch 24/50, Batch 5700/7628\n","Epoch 24/50, Batch 5800/7628\n","Epoch 24/50, Batch 5900/7628\n","Epoch 24/50, Batch 6000/7628\n","Epoch 24/50, Batch 6100/7628\n","Epoch 24/50, Batch 6200/7628\n","Epoch 24/50, Batch 6300/7628\n","Epoch 24/50, Batch 6400/7628\n","Epoch 24/50, Batch 6500/7628\n","Epoch 24/50, Batch 6600/7628\n","Epoch 24/50, Batch 6700/7628\n","Epoch 24/50, Batch 6800/7628\n","Epoch 24/50, Batch 6900/7628\n","Epoch 24/50, Batch 7000/7628\n","Epoch 24/50, Batch 7100/7628\n","Epoch 24/50, Batch 7200/7628\n","Epoch 24/50, Batch 7300/7628\n","Epoch 24/50, Batch 7400/7628\n","Epoch 24/50, Batch 7500/7628\n","Epoch 24/50, Batch 7600/7628\n","Epoch 24/50, Loss: 0.4452\n","Best model saved with loss: 0.4452\n","Epoch 25/50, Batch 0/7628\n","Epoch 25/50, Batch 100/7628\n","Epoch 25/50, Batch 200/7628\n","Epoch 25/50, Batch 300/7628\n","Epoch 25/50, Batch 400/7628\n","Epoch 25/50, Batch 500/7628\n","Epoch 25/50, Batch 600/7628\n","Epoch 25/50, Batch 700/7628\n","Epoch 25/50, Batch 800/7628\n","Epoch 25/50, Batch 900/7628\n","Epoch 25/50, Batch 1000/7628\n","Epoch 25/50, Batch 1100/7628\n","Epoch 25/50, Batch 1200/7628\n","Epoch 25/50, Batch 1300/7628\n","Epoch 25/50, Batch 1400/7628\n","Epoch 25/50, Batch 1500/7628\n","Epoch 25/50, Batch 1600/7628\n","Epoch 25/50, Batch 1700/7628\n","Epoch 25/50, Batch 1800/7628\n","Epoch 25/50, Batch 1900/7628\n","Epoch 25/50, Batch 2000/7628\n","Epoch 25/50, Batch 2100/7628\n","Epoch 25/50, Batch 2200/7628\n","Epoch 25/50, Batch 2300/7628\n","Epoch 25/50, Batch 2400/7628\n","Epoch 25/50, Batch 2500/7628\n","Epoch 25/50, Batch 2600/7628\n","Epoch 25/50, Batch 2700/7628\n","Epoch 25/50, Batch 2800/7628\n","Epoch 25/50, Batch 2900/7628\n","Epoch 25/50, Batch 3000/7628\n","Epoch 25/50, Batch 3100/7628\n","Epoch 25/50, Batch 3200/7628\n","Epoch 25/50, Batch 3300/7628\n","Epoch 25/50, Batch 3400/7628\n","Epoch 25/50, Batch 3500/7628\n","Epoch 25/50, Batch 3600/7628\n","Epoch 25/50, Batch 3700/7628\n","Epoch 25/50, Batch 3800/7628\n","Epoch 25/50, Batch 3900/7628\n","Epoch 25/50, Batch 4000/7628\n","Epoch 25/50, Batch 4100/7628\n","Epoch 25/50, Batch 4200/7628\n","Epoch 25/50, Batch 4300/7628\n","Epoch 25/50, Batch 4400/7628\n","Epoch 25/50, Batch 4500/7628\n","Epoch 25/50, Batch 4600/7628\n","Epoch 25/50, Batch 4700/7628\n","Epoch 25/50, Batch 4800/7628\n","Epoch 25/50, Batch 4900/7628\n","Epoch 25/50, Batch 5000/7628\n","Epoch 25/50, Batch 5100/7628\n","Epoch 25/50, Batch 5200/7628\n","Epoch 25/50, Batch 5300/7628\n","Epoch 25/50, Batch 5400/7628\n","Epoch 25/50, Batch 5500/7628\n","Epoch 25/50, Batch 5600/7628\n","Epoch 25/50, Batch 5700/7628\n","Epoch 25/50, Batch 5800/7628\n","Epoch 25/50, Batch 5900/7628\n","Epoch 25/50, Batch 6000/7628\n","Epoch 25/50, Batch 6100/7628\n","Epoch 25/50, Batch 6200/7628\n","Epoch 25/50, Batch 6300/7628\n","Epoch 25/50, Batch 6400/7628\n","Epoch 25/50, Batch 6500/7628\n","Epoch 25/50, Batch 6600/7628\n","Epoch 25/50, Batch 6700/7628\n","Epoch 25/50, Batch 6800/7628\n","Epoch 25/50, Batch 6900/7628\n","Epoch 25/50, Batch 7000/7628\n","Epoch 25/50, Batch 7100/7628\n","Epoch 25/50, Batch 7200/7628\n","Epoch 25/50, Batch 7300/7628\n","Epoch 25/50, Batch 7400/7628\n","Epoch 25/50, Batch 7500/7628\n","Epoch 25/50, Batch 7600/7628\n","Epoch 25/50, Loss: 0.4452\n","Best model saved with loss: 0.4452\n","Epoch 26/50, Batch 0/7628\n","Epoch 26/50, Batch 100/7628\n","Epoch 26/50, Batch 200/7628\n","Epoch 26/50, Batch 300/7628\n","Epoch 26/50, Batch 400/7628\n","Epoch 26/50, Batch 500/7628\n","Epoch 26/50, Batch 600/7628\n","Epoch 26/50, Batch 700/7628\n","Epoch 26/50, Batch 800/7628\n","Epoch 26/50, Batch 900/7628\n","Epoch 26/50, Batch 1000/7628\n","Epoch 26/50, Batch 1100/7628\n","Epoch 26/50, Batch 1200/7628\n","Epoch 26/50, Batch 1300/7628\n","Epoch 26/50, Batch 1400/7628\n","Epoch 26/50, Batch 1500/7628\n","Epoch 26/50, Batch 1600/7628\n","Epoch 26/50, Batch 1700/7628\n","Epoch 26/50, Batch 1800/7628\n","Epoch 26/50, Batch 1900/7628\n","Epoch 26/50, Batch 2000/7628\n","Epoch 26/50, Batch 2100/7628\n","Epoch 26/50, Batch 2200/7628\n","Epoch 26/50, Batch 2300/7628\n","Epoch 26/50, Batch 2400/7628\n","Epoch 26/50, Batch 2500/7628\n","Epoch 26/50, Batch 2600/7628\n","Epoch 26/50, Batch 2700/7628\n","Epoch 26/50, Batch 2800/7628\n","Epoch 26/50, Batch 2900/7628\n","Epoch 26/50, Batch 3000/7628\n","Epoch 26/50, Batch 3100/7628\n","Epoch 26/50, Batch 3200/7628\n","Epoch 26/50, Batch 3300/7628\n","Epoch 26/50, Batch 3400/7628\n","Epoch 26/50, Batch 3500/7628\n","Epoch 26/50, Batch 3600/7628\n","Epoch 26/50, Batch 3700/7628\n","Epoch 26/50, Batch 3800/7628\n","Epoch 26/50, Batch 3900/7628\n","Epoch 26/50, Batch 4000/7628\n","Epoch 26/50, Batch 4100/7628\n","Epoch 26/50, Batch 4200/7628\n","Epoch 26/50, Batch 4300/7628\n","Epoch 26/50, Batch 4400/7628\n","Epoch 26/50, Batch 4500/7628\n","Epoch 26/50, Batch 4600/7628\n","Epoch 26/50, Batch 4700/7628\n","Epoch 26/50, Batch 4800/7628\n","Epoch 26/50, Batch 4900/7628\n","Epoch 26/50, Batch 5000/7628\n","Epoch 26/50, Batch 5100/7628\n","Epoch 26/50, Batch 5200/7628\n","Epoch 26/50, Batch 5300/7628\n","Epoch 26/50, Batch 5400/7628\n","Epoch 26/50, Batch 5500/7628\n","Epoch 26/50, Batch 5600/7628\n","Epoch 26/50, Batch 5700/7628\n","Epoch 26/50, Batch 5800/7628\n","Epoch 26/50, Batch 5900/7628\n","Epoch 26/50, Batch 6000/7628\n","Epoch 26/50, Batch 6100/7628\n","Epoch 26/50, Batch 6200/7628\n","Epoch 26/50, Batch 6300/7628\n","Epoch 26/50, Batch 6400/7628\n","Epoch 26/50, Batch 6500/7628\n","Epoch 26/50, Batch 6600/7628\n","Epoch 26/50, Batch 6700/7628\n","Epoch 26/50, Batch 6800/7628\n","Epoch 26/50, Batch 6900/7628\n","Epoch 26/50, Batch 7000/7628\n","Epoch 26/50, Batch 7100/7628\n","Epoch 26/50, Batch 7200/7628\n","Epoch 26/50, Batch 7300/7628\n","Epoch 26/50, Batch 7400/7628\n","Epoch 26/50, Batch 7500/7628\n","Epoch 26/50, Batch 7600/7628\n","Epoch 26/50, Loss: 0.4452\n","Best model saved with loss: 0.4452\n","Epoch 27/50, Batch 0/7628\n","Epoch 27/50, Batch 100/7628\n","Epoch 27/50, Batch 200/7628\n","Epoch 27/50, Batch 300/7628\n","Epoch 27/50, Batch 400/7628\n","Epoch 27/50, Batch 500/7628\n","Epoch 27/50, Batch 600/7628\n","Epoch 27/50, Batch 700/7628\n","Epoch 27/50, Batch 800/7628\n","Epoch 27/50, Batch 900/7628\n","Epoch 27/50, Batch 1000/7628\n","Epoch 27/50, Batch 1100/7628\n","Epoch 27/50, Batch 1200/7628\n","Epoch 27/50, Batch 1300/7628\n","Epoch 27/50, Batch 1400/7628\n","Epoch 27/50, Batch 1500/7628\n","Epoch 27/50, Batch 1600/7628\n","Epoch 27/50, Batch 1700/7628\n","Epoch 27/50, Batch 1800/7628\n","Epoch 27/50, Batch 1900/7628\n","Epoch 27/50, Batch 2000/7628\n","Epoch 27/50, Batch 2100/7628\n","Epoch 27/50, Batch 2200/7628\n","Epoch 27/50, Batch 2300/7628\n","Epoch 27/50, Batch 2400/7628\n","Epoch 27/50, Batch 2500/7628\n","Epoch 27/50, Batch 2600/7628\n","Epoch 27/50, Batch 2700/7628\n","Epoch 27/50, Batch 2800/7628\n","Epoch 27/50, Batch 2900/7628\n","Epoch 27/50, Batch 3000/7628\n","Epoch 27/50, Batch 3100/7628\n","Epoch 27/50, Batch 3200/7628\n","Epoch 27/50, Batch 3300/7628\n","Epoch 27/50, Batch 3400/7628\n","Epoch 27/50, Batch 3500/7628\n","Epoch 27/50, Batch 3600/7628\n","Epoch 27/50, Batch 3700/7628\n","Epoch 27/50, Batch 3800/7628\n","Epoch 27/50, Batch 3900/7628\n","Epoch 27/50, Batch 4000/7628\n","Epoch 27/50, Batch 4100/7628\n","Epoch 27/50, Batch 4200/7628\n","Epoch 27/50, Batch 4300/7628\n","Epoch 27/50, Batch 4400/7628\n","Epoch 27/50, Batch 4500/7628\n","Epoch 27/50, Batch 4600/7628\n","Epoch 27/50, Batch 4700/7628\n","Epoch 27/50, Batch 4800/7628\n","Epoch 27/50, Batch 4900/7628\n","Epoch 27/50, Batch 5000/7628\n","Epoch 27/50, Batch 5100/7628\n","Epoch 27/50, Batch 5200/7628\n","Epoch 27/50, Batch 5300/7628\n","Epoch 27/50, Batch 5400/7628\n","Epoch 27/50, Batch 5500/7628\n","Epoch 27/50, Batch 5600/7628\n","Epoch 27/50, Batch 5700/7628\n","Epoch 27/50, Batch 5800/7628\n","Epoch 27/50, Batch 5900/7628\n","Epoch 27/50, Batch 6000/7628\n","Epoch 27/50, Batch 6100/7628\n","Epoch 27/50, Batch 6200/7628\n","Epoch 27/50, Batch 6300/7628\n","Epoch 27/50, Batch 6400/7628\n","Epoch 27/50, Batch 6500/7628\n","Epoch 27/50, Batch 6600/7628\n","Epoch 27/50, Batch 6700/7628\n","Epoch 27/50, Batch 6800/7628\n","Epoch 27/50, Batch 6900/7628\n","Epoch 27/50, Batch 7000/7628\n","Epoch 27/50, Batch 7100/7628\n","Epoch 27/50, Batch 7200/7628\n","Epoch 27/50, Batch 7300/7628\n","Epoch 27/50, Batch 7400/7628\n","Epoch 27/50, Batch 7500/7628\n","Epoch 27/50, Batch 7600/7628\n","Epoch 27/50, Loss: 0.4452\n","Best model saved with loss: 0.4452\n","Epoch 28/50, Batch 0/7628\n","Epoch 28/50, Batch 100/7628\n","Epoch 28/50, Batch 200/7628\n","Epoch 28/50, Batch 300/7628\n","Epoch 28/50, Batch 400/7628\n","Epoch 28/50, Batch 500/7628\n","Epoch 28/50, Batch 600/7628\n","Epoch 28/50, Batch 700/7628\n","Epoch 28/50, Batch 800/7628\n","Epoch 28/50, Batch 900/7628\n","Epoch 28/50, Batch 1000/7628\n","Epoch 28/50, Batch 1100/7628\n","Epoch 28/50, Batch 1200/7628\n","Epoch 28/50, Batch 1300/7628\n","Epoch 28/50, Batch 1400/7628\n","Epoch 28/50, Batch 1500/7628\n","Epoch 28/50, Batch 1600/7628\n","Epoch 28/50, Batch 1700/7628\n","Epoch 28/50, Batch 1800/7628\n","Epoch 28/50, Batch 1900/7628\n","Epoch 28/50, Batch 2000/7628\n","Epoch 28/50, Batch 2100/7628\n","Epoch 28/50, Batch 2200/7628\n","Epoch 28/50, Batch 2300/7628\n","Epoch 28/50, Batch 2400/7628\n","Epoch 28/50, Batch 2500/7628\n","Epoch 28/50, Batch 2600/7628\n","Epoch 28/50, Batch 2700/7628\n","Epoch 28/50, Batch 2800/7628\n","Epoch 28/50, Batch 2900/7628\n","Epoch 28/50, Batch 3000/7628\n","Epoch 28/50, Batch 3100/7628\n","Epoch 28/50, Batch 3200/7628\n","Epoch 28/50, Batch 3300/7628\n","Epoch 28/50, Batch 3400/7628\n","Epoch 28/50, Batch 3500/7628\n","Epoch 28/50, Batch 3600/7628\n","Epoch 28/50, Batch 3700/7628\n","Epoch 28/50, Batch 3800/7628\n","Epoch 28/50, Batch 3900/7628\n","Epoch 28/50, Batch 4000/7628\n","Epoch 28/50, Batch 4100/7628\n","Epoch 28/50, Batch 4200/7628\n","Epoch 28/50, Batch 4300/7628\n","Epoch 28/50, Batch 4400/7628\n","Epoch 28/50, Batch 4500/7628\n","Epoch 28/50, Batch 4600/7628\n","Epoch 28/50, Batch 4700/7628\n","Epoch 28/50, Batch 4800/7628\n","Epoch 28/50, Batch 4900/7628\n","Epoch 28/50, Batch 5000/7628\n","Epoch 28/50, Batch 5100/7628\n","Epoch 28/50, Batch 5200/7628\n","Epoch 28/50, Batch 5300/7628\n","Epoch 28/50, Batch 5400/7628\n","Epoch 28/50, Batch 5500/7628\n","Epoch 28/50, Batch 5600/7628\n","Epoch 28/50, Batch 5700/7628\n","Epoch 28/50, Batch 5800/7628\n","Epoch 28/50, Batch 5900/7628\n","Epoch 28/50, Batch 6000/7628\n","Epoch 28/50, Batch 6100/7628\n","Epoch 28/50, Batch 6200/7628\n","Epoch 28/50, Batch 6300/7628\n","Epoch 28/50, Batch 6400/7628\n","Epoch 28/50, Batch 6500/7628\n","Epoch 28/50, Batch 6600/7628\n","Epoch 28/50, Batch 6700/7628\n","Epoch 28/50, Batch 6800/7628\n","Epoch 28/50, Batch 6900/7628\n","Epoch 28/50, Batch 7000/7628\n","Epoch 28/50, Batch 7100/7628\n","Epoch 28/50, Batch 7200/7628\n","Epoch 28/50, Batch 7300/7628\n","Epoch 28/50, Batch 7400/7628\n","Epoch 28/50, Batch 7500/7628\n","Epoch 28/50, Batch 7600/7628\n","Epoch 28/50, Loss: 0.4452\n","No improvement for 1 epoch(s)\n","Epoch 29/50, Batch 0/7628\n","Epoch 29/50, Batch 100/7628\n","Epoch 29/50, Batch 200/7628\n","Epoch 29/50, Batch 300/7628\n","Epoch 29/50, Batch 400/7628\n","Epoch 29/50, Batch 500/7628\n","Epoch 29/50, Batch 600/7628\n","Epoch 29/50, Batch 700/7628\n","Epoch 29/50, Batch 800/7628\n","Epoch 29/50, Batch 900/7628\n","Epoch 29/50, Batch 1000/7628\n","Epoch 29/50, Batch 1100/7628\n","Epoch 29/50, Batch 1200/7628\n","Epoch 29/50, Batch 1300/7628\n","Epoch 29/50, Batch 1400/7628\n","Epoch 29/50, Batch 1500/7628\n","Epoch 29/50, Batch 1600/7628\n","Epoch 29/50, Batch 1700/7628\n","Epoch 29/50, Batch 1800/7628\n","Epoch 29/50, Batch 1900/7628\n","Epoch 29/50, Batch 2000/7628\n","Epoch 29/50, Batch 2100/7628\n","Epoch 29/50, Batch 2200/7628\n","Epoch 29/50, Batch 2300/7628\n","Epoch 29/50, Batch 2400/7628\n","Epoch 29/50, Batch 2500/7628\n","Epoch 29/50, Batch 2600/7628\n","Epoch 29/50, Batch 2700/7628\n","Epoch 29/50, Batch 2800/7628\n","Epoch 29/50, Batch 2900/7628\n","Epoch 29/50, Batch 3000/7628\n","Epoch 29/50, Batch 3100/7628\n","Epoch 29/50, Batch 3200/7628\n","Epoch 29/50, Batch 3300/7628\n","Epoch 29/50, Batch 3400/7628\n","Epoch 29/50, Batch 3500/7628\n","Epoch 29/50, Batch 3600/7628\n","Epoch 29/50, Batch 3700/7628\n","Epoch 29/50, Batch 3800/7628\n","Epoch 29/50, Batch 3900/7628\n","Epoch 29/50, Batch 4000/7628\n","Epoch 29/50, Batch 4100/7628\n","Epoch 29/50, Batch 4200/7628\n","Epoch 29/50, Batch 4300/7628\n","Epoch 29/50, Batch 4400/7628\n","Epoch 29/50, Batch 4500/7628\n","Epoch 29/50, Batch 4600/7628\n","Epoch 29/50, Batch 4700/7628\n","Epoch 29/50, Batch 4800/7628\n","Epoch 29/50, Batch 4900/7628\n","Epoch 29/50, Batch 5000/7628\n","Epoch 29/50, Batch 5100/7628\n","Epoch 29/50, Batch 5200/7628\n","Epoch 29/50, Batch 5300/7628\n","Epoch 29/50, Batch 5400/7628\n","Epoch 29/50, Batch 5500/7628\n","Epoch 29/50, Batch 5600/7628\n","Epoch 29/50, Batch 5700/7628\n","Epoch 29/50, Batch 5800/7628\n","Epoch 29/50, Batch 5900/7628\n","Epoch 29/50, Batch 6000/7628\n","Epoch 29/50, Batch 6100/7628\n","Epoch 29/50, Batch 6200/7628\n","Epoch 29/50, Batch 6300/7628\n","Epoch 29/50, Batch 6400/7628\n","Epoch 29/50, Batch 6500/7628\n","Epoch 29/50, Batch 6600/7628\n","Epoch 29/50, Batch 6700/7628\n","Epoch 29/50, Batch 6800/7628\n","Epoch 29/50, Batch 6900/7628\n","Epoch 29/50, Batch 7000/7628\n","Epoch 29/50, Batch 7100/7628\n","Epoch 29/50, Batch 7200/7628\n","Epoch 29/50, Batch 7300/7628\n","Epoch 29/50, Batch 7400/7628\n","Epoch 29/50, Batch 7500/7628\n","Epoch 29/50, Batch 7600/7628\n","Epoch 29/50, Loss: 0.4452\n","No improvement for 2 epoch(s)\n","Epoch 30/50, Batch 0/7628\n","Epoch 30/50, Batch 100/7628\n","Epoch 30/50, Batch 200/7628\n","Epoch 30/50, Batch 300/7628\n","Epoch 30/50, Batch 400/7628\n","Epoch 30/50, Batch 500/7628\n","Epoch 30/50, Batch 600/7628\n","Epoch 30/50, Batch 700/7628\n","Epoch 30/50, Batch 800/7628\n","Epoch 30/50, Batch 900/7628\n","Epoch 30/50, Batch 1000/7628\n","Epoch 30/50, Batch 1100/7628\n","Epoch 30/50, Batch 1200/7628\n","Epoch 30/50, Batch 1300/7628\n","Epoch 30/50, Batch 1400/7628\n","Epoch 30/50, Batch 1500/7628\n","Epoch 30/50, Batch 1600/7628\n","Epoch 30/50, Batch 1700/7628\n","Epoch 30/50, Batch 1800/7628\n","Epoch 30/50, Batch 1900/7628\n","Epoch 30/50, Batch 2000/7628\n","Epoch 30/50, Batch 2100/7628\n","Epoch 30/50, Batch 2200/7628\n","Epoch 30/50, Batch 2300/7628\n","Epoch 30/50, Batch 2400/7628\n","Epoch 30/50, Batch 2500/7628\n","Epoch 30/50, Batch 2600/7628\n","Epoch 30/50, Batch 2700/7628\n","Epoch 30/50, Batch 2800/7628\n","Epoch 30/50, Batch 2900/7628\n","Epoch 30/50, Batch 3000/7628\n","Epoch 30/50, Batch 3100/7628\n","Epoch 30/50, Batch 3200/7628\n","Epoch 30/50, Batch 3300/7628\n","Epoch 30/50, Batch 3400/7628\n","Epoch 30/50, Batch 3500/7628\n","Epoch 30/50, Batch 3600/7628\n","Epoch 30/50, Batch 3700/7628\n","Epoch 30/50, Batch 3800/7628\n","Epoch 30/50, Batch 3900/7628\n","Epoch 30/50, Batch 4000/7628\n","Epoch 30/50, Batch 4100/7628\n","Epoch 30/50, Batch 4200/7628\n","Epoch 30/50, Batch 4300/7628\n","Epoch 30/50, Batch 4400/7628\n","Epoch 30/50, Batch 4500/7628\n","Epoch 30/50, Batch 4600/7628\n","Epoch 30/50, Batch 4700/7628\n","Epoch 30/50, Batch 4800/7628\n","Epoch 30/50, Batch 4900/7628\n","Epoch 30/50, Batch 5000/7628\n","Epoch 30/50, Batch 5100/7628\n","Epoch 30/50, Batch 5200/7628\n","Epoch 30/50, Batch 5300/7628\n","Epoch 30/50, Batch 5400/7628\n","Epoch 30/50, Batch 5500/7628\n","Epoch 30/50, Batch 5600/7628\n","Epoch 30/50, Batch 5700/7628\n","Epoch 30/50, Batch 5800/7628\n","Epoch 30/50, Batch 5900/7628\n","Epoch 30/50, Batch 6000/7628\n","Epoch 30/50, Batch 6100/7628\n","Epoch 30/50, Batch 6200/7628\n","Epoch 30/50, Batch 6300/7628\n","Epoch 30/50, Batch 6400/7628\n","Epoch 30/50, Batch 6500/7628\n","Epoch 30/50, Batch 6600/7628\n","Epoch 30/50, Batch 6700/7628\n","Epoch 30/50, Batch 6800/7628\n","Epoch 30/50, Batch 6900/7628\n","Epoch 30/50, Batch 7000/7628\n","Epoch 30/50, Batch 7100/7628\n","Epoch 30/50, Batch 7200/7628\n","Epoch 30/50, Batch 7300/7628\n","Epoch 30/50, Batch 7400/7628\n","Epoch 30/50, Batch 7500/7628\n","Epoch 30/50, Batch 7600/7628\n","Epoch 30/50, Loss: 0.4452\n","Best model saved with loss: 0.4452\n","Epoch 31/50, Batch 0/7628\n","Epoch 31/50, Batch 100/7628\n","Epoch 31/50, Batch 200/7628\n","Epoch 31/50, Batch 300/7628\n","Epoch 31/50, Batch 400/7628\n","Epoch 31/50, Batch 500/7628\n","Epoch 31/50, Batch 600/7628\n","Epoch 31/50, Batch 700/7628\n","Epoch 31/50, Batch 800/7628\n","Epoch 31/50, Batch 900/7628\n","Epoch 31/50, Batch 1000/7628\n","Epoch 31/50, Batch 1100/7628\n","Epoch 31/50, Batch 1200/7628\n","Epoch 31/50, Batch 1300/7628\n","Epoch 31/50, Batch 1400/7628\n","Epoch 31/50, Batch 1500/7628\n","Epoch 31/50, Batch 1600/7628\n","Epoch 31/50, Batch 1700/7628\n","Epoch 31/50, Batch 1800/7628\n","Epoch 31/50, Batch 1900/7628\n","Epoch 31/50, Batch 2000/7628\n","Epoch 31/50, Batch 2100/7628\n","Epoch 31/50, Batch 2200/7628\n","Epoch 31/50, Batch 2300/7628\n","Epoch 31/50, Batch 2400/7628\n","Epoch 31/50, Batch 2500/7628\n","Epoch 31/50, Batch 2600/7628\n","Epoch 31/50, Batch 2700/7628\n","Epoch 31/50, Batch 2800/7628\n","Epoch 31/50, Batch 2900/7628\n","Epoch 31/50, Batch 3000/7628\n","Epoch 31/50, Batch 3100/7628\n","Epoch 31/50, Batch 3200/7628\n","Epoch 31/50, Batch 3300/7628\n","Epoch 31/50, Batch 3400/7628\n","Epoch 31/50, Batch 3500/7628\n","Epoch 31/50, Batch 3600/7628\n","Epoch 31/50, Batch 3700/7628\n","Epoch 31/50, Batch 3800/7628\n","Epoch 31/50, Batch 3900/7628\n","Epoch 31/50, Batch 4000/7628\n","Epoch 31/50, Batch 4100/7628\n","Epoch 31/50, Batch 4200/7628\n","Epoch 31/50, Batch 4300/7628\n","Epoch 31/50, Batch 4400/7628\n","Epoch 31/50, Batch 4500/7628\n","Epoch 31/50, Batch 4600/7628\n","Epoch 31/50, Batch 4700/7628\n","Epoch 31/50, Batch 4800/7628\n","Epoch 31/50, Batch 4900/7628\n","Epoch 31/50, Batch 5000/7628\n","Epoch 31/50, Batch 5100/7628\n","Epoch 31/50, Batch 5200/7628\n","Epoch 31/50, Batch 5300/7628\n","Epoch 31/50, Batch 5400/7628\n","Epoch 31/50, Batch 5500/7628\n","Epoch 31/50, Batch 5600/7628\n","Epoch 31/50, Batch 5700/7628\n","Epoch 31/50, Batch 5800/7628\n","Epoch 31/50, Batch 5900/7628\n","Epoch 31/50, Batch 6000/7628\n","Epoch 31/50, Batch 6100/7628\n","Epoch 31/50, Batch 6200/7628\n","Epoch 31/50, Batch 6300/7628\n","Epoch 31/50, Batch 6400/7628\n","Epoch 31/50, Batch 6500/7628\n","Epoch 31/50, Batch 6600/7628\n","Epoch 31/50, Batch 6700/7628\n","Epoch 31/50, Batch 6800/7628\n","Epoch 31/50, Batch 6900/7628\n","Epoch 31/50, Batch 7000/7628\n","Epoch 31/50, Batch 7100/7628\n","Epoch 31/50, Batch 7200/7628\n","Epoch 31/50, Batch 7300/7628\n","Epoch 31/50, Batch 7400/7628\n","Epoch 31/50, Batch 7500/7628\n","Epoch 31/50, Batch 7600/7628\n","Epoch 31/50, Loss: 0.4452\n","No improvement for 1 epoch(s)\n","Epoch 32/50, Batch 0/7628\n","Epoch 32/50, Batch 100/7628\n","Epoch 32/50, Batch 200/7628\n","Epoch 32/50, Batch 300/7628\n","Epoch 32/50, Batch 400/7628\n","Epoch 32/50, Batch 500/7628\n","Epoch 32/50, Batch 600/7628\n","Epoch 32/50, Batch 700/7628\n","Epoch 32/50, Batch 800/7628\n","Epoch 32/50, Batch 900/7628\n","Epoch 32/50, Batch 1000/7628\n","Epoch 32/50, Batch 1100/7628\n","Epoch 32/50, Batch 1200/7628\n","Epoch 32/50, Batch 1300/7628\n","Epoch 32/50, Batch 1400/7628\n","Epoch 32/50, Batch 1500/7628\n","Epoch 32/50, Batch 1600/7628\n","Epoch 32/50, Batch 1700/7628\n","Epoch 32/50, Batch 1800/7628\n","Epoch 32/50, Batch 1900/7628\n","Epoch 32/50, Batch 2000/7628\n","Epoch 32/50, Batch 2100/7628\n","Epoch 32/50, Batch 2200/7628\n","Epoch 32/50, Batch 2300/7628\n","Epoch 32/50, Batch 2400/7628\n","Epoch 32/50, Batch 2500/7628\n","Epoch 32/50, Batch 2600/7628\n","Epoch 32/50, Batch 2700/7628\n","Epoch 32/50, Batch 2800/7628\n","Epoch 32/50, Batch 2900/7628\n","Epoch 32/50, Batch 3000/7628\n","Epoch 32/50, Batch 3100/7628\n","Epoch 32/50, Batch 3200/7628\n","Epoch 32/50, Batch 3300/7628\n","Epoch 32/50, Batch 3400/7628\n","Epoch 32/50, Batch 3500/7628\n","Epoch 32/50, Batch 3600/7628\n","Epoch 32/50, Batch 3700/7628\n","Epoch 32/50, Batch 3800/7628\n","Epoch 32/50, Batch 3900/7628\n","Epoch 32/50, Batch 4000/7628\n","Epoch 32/50, Batch 4100/7628\n","Epoch 32/50, Batch 4200/7628\n","Epoch 32/50, Batch 4300/7628\n","Epoch 32/50, Batch 4400/7628\n","Epoch 32/50, Batch 4500/7628\n","Epoch 32/50, Batch 4600/7628\n","Epoch 32/50, Batch 4700/7628\n","Epoch 32/50, Batch 4800/7628\n","Epoch 32/50, Batch 4900/7628\n","Epoch 32/50, Batch 5000/7628\n","Epoch 32/50, Batch 5100/7628\n","Epoch 32/50, Batch 5200/7628\n","Epoch 32/50, Batch 5300/7628\n","Epoch 32/50, Batch 5400/7628\n","Epoch 32/50, Batch 5500/7628\n","Epoch 32/50, Batch 5600/7628\n","Epoch 32/50, Batch 5700/7628\n","Epoch 32/50, Batch 5800/7628\n","Epoch 32/50, Batch 5900/7628\n","Epoch 32/50, Batch 6000/7628\n","Epoch 32/50, Batch 6100/7628\n","Epoch 32/50, Batch 6200/7628\n","Epoch 32/50, Batch 6300/7628\n","Epoch 32/50, Batch 6400/7628\n","Epoch 32/50, Batch 6500/7628\n","Epoch 32/50, Batch 6600/7628\n","Epoch 32/50, Batch 6700/7628\n","Epoch 32/50, Batch 6800/7628\n","Epoch 32/50, Batch 6900/7628\n","Epoch 32/50, Batch 7000/7628\n","Epoch 32/50, Batch 7100/7628\n","Epoch 32/50, Batch 7200/7628\n","Epoch 32/50, Batch 7300/7628\n","Epoch 32/50, Batch 7400/7628\n","Epoch 32/50, Batch 7500/7628\n","Epoch 32/50, Batch 7600/7628\n","Epoch 32/50, Loss: 0.4452\n","No improvement for 2 epoch(s)\n","Epoch 33/50, Batch 0/7628\n","Epoch 33/50, Batch 100/7628\n","Epoch 33/50, Batch 200/7628\n","Epoch 33/50, Batch 300/7628\n","Epoch 33/50, Batch 400/7628\n","Epoch 33/50, Batch 500/7628\n","Epoch 33/50, Batch 600/7628\n","Epoch 33/50, Batch 700/7628\n","Epoch 33/50, Batch 800/7628\n","Epoch 33/50, Batch 900/7628\n","Epoch 33/50, Batch 1000/7628\n","Epoch 33/50, Batch 1100/7628\n","Epoch 33/50, Batch 1200/7628\n","Epoch 33/50, Batch 1300/7628\n","Epoch 33/50, Batch 1400/7628\n","Epoch 33/50, Batch 1500/7628\n","Epoch 33/50, Batch 1600/7628\n","Epoch 33/50, Batch 1700/7628\n","Epoch 33/50, Batch 1800/7628\n","Epoch 33/50, Batch 1900/7628\n","Epoch 33/50, Batch 2000/7628\n","Epoch 33/50, Batch 2100/7628\n","Epoch 33/50, Batch 2200/7628\n","Epoch 33/50, Batch 2300/7628\n","Epoch 33/50, Batch 2400/7628\n","Epoch 33/50, Batch 2500/7628\n","Epoch 33/50, Batch 2600/7628\n","Epoch 33/50, Batch 2700/7628\n","Epoch 33/50, Batch 2800/7628\n","Epoch 33/50, Batch 2900/7628\n","Epoch 33/50, Batch 3000/7628\n","Epoch 33/50, Batch 3100/7628\n","Epoch 33/50, Batch 3200/7628\n","Epoch 33/50, Batch 3300/7628\n","Epoch 33/50, Batch 3400/7628\n","Epoch 33/50, Batch 3500/7628\n","Epoch 33/50, Batch 3600/7628\n","Epoch 33/50, Batch 3700/7628\n","Epoch 33/50, Batch 3800/7628\n","Epoch 33/50, Batch 3900/7628\n","Epoch 33/50, Batch 4000/7628\n","Epoch 33/50, Batch 4100/7628\n","Epoch 33/50, Batch 4200/7628\n","Epoch 33/50, Batch 4300/7628\n","Epoch 33/50, Batch 4400/7628\n","Epoch 33/50, Batch 4500/7628\n","Epoch 33/50, Batch 4600/7628\n","Epoch 33/50, Batch 4700/7628\n","Epoch 33/50, Batch 4800/7628\n","Epoch 33/50, Batch 4900/7628\n","Epoch 33/50, Batch 5000/7628\n","Epoch 33/50, Batch 5100/7628\n","Epoch 33/50, Batch 5200/7628\n","Epoch 33/50, Batch 5300/7628\n","Epoch 33/50, Batch 5400/7628\n","Epoch 33/50, Batch 5500/7628\n","Epoch 33/50, Batch 5600/7628\n","Epoch 33/50, Batch 5700/7628\n","Epoch 33/50, Batch 5800/7628\n","Epoch 33/50, Batch 5900/7628\n","Epoch 33/50, Batch 6000/7628\n","Epoch 33/50, Batch 6100/7628\n","Epoch 33/50, Batch 6200/7628\n","Epoch 33/50, Batch 6300/7628\n","Epoch 33/50, Batch 6400/7628\n","Epoch 33/50, Batch 6500/7628\n","Epoch 33/50, Batch 6600/7628\n","Epoch 33/50, Batch 6700/7628\n","Epoch 33/50, Batch 6800/7628\n","Epoch 33/50, Batch 6900/7628\n","Epoch 33/50, Batch 7000/7628\n","Epoch 33/50, Batch 7100/7628\n","Epoch 33/50, Batch 7200/7628\n","Epoch 33/50, Batch 7300/7628\n","Epoch 33/50, Batch 7400/7628\n","Epoch 33/50, Batch 7500/7628\n","Epoch 33/50, Batch 7600/7628\n","Epoch 33/50, Loss: 0.4452\n","Best model saved with loss: 0.4452\n","Epoch 34/50, Batch 0/7628\n","Epoch 34/50, Batch 100/7628\n","Epoch 34/50, Batch 200/7628\n","Epoch 34/50, Batch 300/7628\n","Epoch 34/50, Batch 400/7628\n","Epoch 34/50, Batch 500/7628\n","Epoch 34/50, Batch 600/7628\n","Epoch 34/50, Batch 700/7628\n","Epoch 34/50, Batch 800/7628\n","Epoch 34/50, Batch 900/7628\n","Epoch 34/50, Batch 1000/7628\n","Epoch 34/50, Batch 1100/7628\n","Epoch 34/50, Batch 1200/7628\n","Epoch 34/50, Batch 1300/7628\n","Epoch 34/50, Batch 1400/7628\n","Epoch 34/50, Batch 1500/7628\n","Epoch 34/50, Batch 1600/7628\n","Epoch 34/50, Batch 1700/7628\n","Epoch 34/50, Batch 1800/7628\n","Epoch 34/50, Batch 1900/7628\n","Epoch 34/50, Batch 2000/7628\n","Epoch 34/50, Batch 2100/7628\n","Epoch 34/50, Batch 2200/7628\n","Epoch 34/50, Batch 2300/7628\n","Epoch 34/50, Batch 2400/7628\n","Epoch 34/50, Batch 2500/7628\n","Epoch 34/50, Batch 2600/7628\n","Epoch 34/50, Batch 2700/7628\n","Epoch 34/50, Batch 2800/7628\n","Epoch 34/50, Batch 2900/7628\n","Epoch 34/50, Batch 3000/7628\n","Epoch 34/50, Batch 3100/7628\n","Epoch 34/50, Batch 3200/7628\n","Epoch 34/50, Batch 3300/7628\n","Epoch 34/50, Batch 3400/7628\n","Epoch 34/50, Batch 3500/7628\n","Epoch 34/50, Batch 3600/7628\n","Epoch 34/50, Batch 3700/7628\n","Epoch 34/50, Batch 3800/7628\n","Epoch 34/50, Batch 3900/7628\n","Epoch 34/50, Batch 4000/7628\n","Epoch 34/50, Batch 4100/7628\n","Epoch 34/50, Batch 4200/7628\n","Epoch 34/50, Batch 4300/7628\n","Epoch 34/50, Batch 4400/7628\n","Epoch 34/50, Batch 4500/7628\n","Epoch 34/50, Batch 4600/7628\n","Epoch 34/50, Batch 4700/7628\n","Epoch 34/50, Batch 4800/7628\n","Epoch 34/50, Batch 4900/7628\n","Epoch 34/50, Batch 5000/7628\n","Epoch 34/50, Batch 5100/7628\n","Epoch 34/50, Batch 5200/7628\n","Epoch 34/50, Batch 5300/7628\n","Epoch 34/50, Batch 5400/7628\n","Epoch 34/50, Batch 5500/7628\n","Epoch 34/50, Batch 5600/7628\n","Epoch 34/50, Batch 5700/7628\n","Epoch 34/50, Batch 5800/7628\n","Epoch 34/50, Batch 5900/7628\n","Epoch 34/50, Batch 6000/7628\n","Epoch 34/50, Batch 6100/7628\n","Epoch 34/50, Batch 6200/7628\n","Epoch 34/50, Batch 6300/7628\n","Epoch 34/50, Batch 6400/7628\n","Epoch 34/50, Batch 6500/7628\n","Epoch 34/50, Batch 6600/7628\n","Epoch 34/50, Batch 6700/7628\n","Epoch 34/50, Batch 6800/7628\n","Epoch 34/50, Batch 6900/7628\n","Epoch 34/50, Batch 7000/7628\n","Epoch 34/50, Batch 7100/7628\n","Epoch 34/50, Batch 7200/7628\n","Epoch 34/50, Batch 7300/7628\n","Epoch 34/50, Batch 7400/7628\n","Epoch 34/50, Batch 7500/7628\n","Epoch 34/50, Batch 7600/7628\n","Epoch 34/50, Loss: 0.4452\n","Best model saved with loss: 0.4452\n","Epoch 35/50, Batch 0/7628\n","Epoch 35/50, Batch 100/7628\n","Epoch 35/50, Batch 200/7628\n","Epoch 35/50, Batch 300/7628\n","Epoch 35/50, Batch 400/7628\n","Epoch 35/50, Batch 500/7628\n","Epoch 35/50, Batch 600/7628\n","Epoch 35/50, Batch 700/7628\n","Epoch 35/50, Batch 800/7628\n","Epoch 35/50, Batch 900/7628\n","Epoch 35/50, Batch 1000/7628\n","Epoch 35/50, Batch 1100/7628\n","Epoch 35/50, Batch 1200/7628\n","Epoch 35/50, Batch 1300/7628\n","Epoch 35/50, Batch 1400/7628\n","Epoch 35/50, Batch 1500/7628\n","Epoch 35/50, Batch 1600/7628\n","Epoch 35/50, Batch 1700/7628\n","Epoch 35/50, Batch 1800/7628\n","Epoch 35/50, Batch 1900/7628\n","Epoch 35/50, Batch 2000/7628\n","Epoch 35/50, Batch 2100/7628\n","Epoch 35/50, Batch 2200/7628\n","Epoch 35/50, Batch 2300/7628\n","Epoch 35/50, Batch 2400/7628\n","Epoch 35/50, Batch 2500/7628\n","Epoch 35/50, Batch 2600/7628\n","Epoch 35/50, Batch 2700/7628\n","Epoch 35/50, Batch 2800/7628\n","Epoch 35/50, Batch 2900/7628\n","Epoch 35/50, Batch 3000/7628\n","Epoch 35/50, Batch 3100/7628\n","Epoch 35/50, Batch 3200/7628\n","Epoch 35/50, Batch 3300/7628\n","Epoch 35/50, Batch 3400/7628\n","Epoch 35/50, Batch 3500/7628\n","Epoch 35/50, Batch 3600/7628\n","Epoch 35/50, Batch 3700/7628\n","Epoch 35/50, Batch 3800/7628\n","Epoch 35/50, Batch 3900/7628\n","Epoch 35/50, Batch 4000/7628\n","Epoch 35/50, Batch 4100/7628\n","Epoch 35/50, Batch 4200/7628\n","Epoch 35/50, Batch 4300/7628\n","Epoch 35/50, Batch 4400/7628\n","Epoch 35/50, Batch 4500/7628\n","Epoch 35/50, Batch 4600/7628\n","Epoch 35/50, Batch 4700/7628\n","Epoch 35/50, Batch 4800/7628\n","Epoch 35/50, Batch 4900/7628\n","Epoch 35/50, Batch 5000/7628\n","Epoch 35/50, Batch 5100/7628\n","Epoch 35/50, Batch 5200/7628\n","Epoch 35/50, Batch 5300/7628\n","Epoch 35/50, Batch 5400/7628\n","Epoch 35/50, Batch 5500/7628\n","Epoch 35/50, Batch 5600/7628\n","Epoch 35/50, Batch 5700/7628\n","Epoch 35/50, Batch 5800/7628\n","Epoch 35/50, Batch 5900/7628\n","Epoch 35/50, Batch 6000/7628\n","Epoch 35/50, Batch 6100/7628\n","Epoch 35/50, Batch 6200/7628\n","Epoch 35/50, Batch 6300/7628\n","Epoch 35/50, Batch 6400/7628\n","Epoch 35/50, Batch 6500/7628\n","Epoch 35/50, Batch 6600/7628\n","Epoch 35/50, Batch 6700/7628\n","Epoch 35/50, Batch 6800/7628\n","Epoch 35/50, Batch 6900/7628\n","Epoch 35/50, Batch 7000/7628\n","Epoch 35/50, Batch 7100/7628\n","Epoch 35/50, Batch 7200/7628\n","Epoch 35/50, Batch 7300/7628\n","Epoch 35/50, Batch 7400/7628\n","Epoch 35/50, Batch 7500/7628\n","Epoch 35/50, Batch 7600/7628\n","Epoch 35/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 36/50, Batch 0/7628\n","Epoch 36/50, Batch 100/7628\n","Epoch 36/50, Batch 200/7628\n","Epoch 36/50, Batch 300/7628\n","Epoch 36/50, Batch 400/7628\n","Epoch 36/50, Batch 500/7628\n","Epoch 36/50, Batch 600/7628\n","Epoch 36/50, Batch 700/7628\n","Epoch 36/50, Batch 800/7628\n","Epoch 36/50, Batch 900/7628\n","Epoch 36/50, Batch 1000/7628\n","Epoch 36/50, Batch 1100/7628\n","Epoch 36/50, Batch 1200/7628\n","Epoch 36/50, Batch 1300/7628\n","Epoch 36/50, Batch 1400/7628\n","Epoch 36/50, Batch 1500/7628\n","Epoch 36/50, Batch 1600/7628\n","Epoch 36/50, Batch 1700/7628\n","Epoch 36/50, Batch 1800/7628\n","Epoch 36/50, Batch 1900/7628\n","Epoch 36/50, Batch 2000/7628\n","Epoch 36/50, Batch 2100/7628\n","Epoch 36/50, Batch 2200/7628\n","Epoch 36/50, Batch 2300/7628\n","Epoch 36/50, Batch 2400/7628\n","Epoch 36/50, Batch 2500/7628\n","Epoch 36/50, Batch 2600/7628\n","Epoch 36/50, Batch 2700/7628\n","Epoch 36/50, Batch 2800/7628\n","Epoch 36/50, Batch 2900/7628\n","Epoch 36/50, Batch 3000/7628\n","Epoch 36/50, Batch 3100/7628\n","Epoch 36/50, Batch 3200/7628\n","Epoch 36/50, Batch 3300/7628\n","Epoch 36/50, Batch 3400/7628\n","Epoch 36/50, Batch 3500/7628\n","Epoch 36/50, Batch 3600/7628\n","Epoch 36/50, Batch 3700/7628\n","Epoch 36/50, Batch 3800/7628\n","Epoch 36/50, Batch 3900/7628\n","Epoch 36/50, Batch 4000/7628\n","Epoch 36/50, Batch 4100/7628\n","Epoch 36/50, Batch 4200/7628\n","Epoch 36/50, Batch 4300/7628\n","Epoch 36/50, Batch 4400/7628\n","Epoch 36/50, Batch 4500/7628\n","Epoch 36/50, Batch 4600/7628\n","Epoch 36/50, Batch 4700/7628\n","Epoch 36/50, Batch 4800/7628\n","Epoch 36/50, Batch 4900/7628\n","Epoch 36/50, Batch 5000/7628\n","Epoch 36/50, Batch 5100/7628\n","Epoch 36/50, Batch 5200/7628\n","Epoch 36/50, Batch 5300/7628\n","Epoch 36/50, Batch 5400/7628\n","Epoch 36/50, Batch 5500/7628\n","Epoch 36/50, Batch 5600/7628\n","Epoch 36/50, Batch 5700/7628\n","Epoch 36/50, Batch 5800/7628\n","Epoch 36/50, Batch 5900/7628\n","Epoch 36/50, Batch 6000/7628\n","Epoch 36/50, Batch 6100/7628\n","Epoch 36/50, Batch 6200/7628\n","Epoch 36/50, Batch 6300/7628\n","Epoch 36/50, Batch 6400/7628\n","Epoch 36/50, Batch 6500/7628\n","Epoch 36/50, Batch 6600/7628\n","Epoch 36/50, Batch 6700/7628\n","Epoch 36/50, Batch 6800/7628\n","Epoch 36/50, Batch 6900/7628\n","Epoch 36/50, Batch 7000/7628\n","Epoch 36/50, Batch 7100/7628\n","Epoch 36/50, Batch 7200/7628\n","Epoch 36/50, Batch 7300/7628\n","Epoch 36/50, Batch 7400/7628\n","Epoch 36/50, Batch 7500/7628\n","Epoch 36/50, Batch 7600/7628\n","Epoch 36/50, Loss: 0.4452\n","No improvement for 1 epoch(s)\n","Epoch 37/50, Batch 0/7628\n","Epoch 37/50, Batch 100/7628\n","Epoch 37/50, Batch 200/7628\n","Epoch 37/50, Batch 300/7628\n","Epoch 37/50, Batch 400/7628\n","Epoch 37/50, Batch 500/7628\n","Epoch 37/50, Batch 600/7628\n","Epoch 37/50, Batch 700/7628\n","Epoch 37/50, Batch 800/7628\n","Epoch 37/50, Batch 900/7628\n","Epoch 37/50, Batch 1000/7628\n","Epoch 37/50, Batch 1100/7628\n","Epoch 37/50, Batch 1200/7628\n","Epoch 37/50, Batch 1300/7628\n","Epoch 37/50, Batch 1400/7628\n","Epoch 37/50, Batch 1500/7628\n","Epoch 37/50, Batch 1600/7628\n","Epoch 37/50, Batch 1700/7628\n","Epoch 37/50, Batch 1800/7628\n","Epoch 37/50, Batch 1900/7628\n","Epoch 37/50, Batch 2000/7628\n","Epoch 37/50, Batch 2100/7628\n","Epoch 37/50, Batch 2200/7628\n","Epoch 37/50, Batch 2300/7628\n","Epoch 37/50, Batch 2400/7628\n","Epoch 37/50, Batch 2500/7628\n","Epoch 37/50, Batch 2600/7628\n","Epoch 37/50, Batch 2700/7628\n","Epoch 37/50, Batch 2800/7628\n","Epoch 37/50, Batch 2900/7628\n","Epoch 37/50, Batch 3000/7628\n","Epoch 37/50, Batch 3100/7628\n","Epoch 37/50, Batch 3200/7628\n","Epoch 37/50, Batch 3300/7628\n","Epoch 37/50, Batch 3400/7628\n","Epoch 37/50, Batch 3500/7628\n","Epoch 37/50, Batch 3600/7628\n","Epoch 37/50, Batch 3700/7628\n","Epoch 37/50, Batch 3800/7628\n","Epoch 37/50, Batch 3900/7628\n","Epoch 37/50, Batch 4000/7628\n","Epoch 37/50, Batch 4100/7628\n","Epoch 37/50, Batch 4200/7628\n","Epoch 37/50, Batch 4300/7628\n","Epoch 37/50, Batch 4400/7628\n","Epoch 37/50, Batch 4500/7628\n","Epoch 37/50, Batch 4600/7628\n","Epoch 37/50, Batch 4700/7628\n","Epoch 37/50, Batch 4800/7628\n","Epoch 37/50, Batch 4900/7628\n","Epoch 37/50, Batch 5000/7628\n","Epoch 37/50, Batch 5100/7628\n","Epoch 37/50, Batch 5200/7628\n","Epoch 37/50, Batch 5300/7628\n","Epoch 37/50, Batch 5400/7628\n","Epoch 37/50, Batch 5500/7628\n","Epoch 37/50, Batch 5600/7628\n","Epoch 37/50, Batch 5700/7628\n","Epoch 37/50, Batch 5800/7628\n","Epoch 37/50, Batch 5900/7628\n","Epoch 37/50, Batch 6000/7628\n","Epoch 37/50, Batch 6100/7628\n","Epoch 37/50, Batch 6200/7628\n","Epoch 37/50, Batch 6300/7628\n","Epoch 37/50, Batch 6400/7628\n","Epoch 37/50, Batch 6500/7628\n","Epoch 37/50, Batch 6600/7628\n","Epoch 37/50, Batch 6700/7628\n","Epoch 37/50, Batch 6800/7628\n","Epoch 37/50, Batch 6900/7628\n","Epoch 37/50, Batch 7000/7628\n","Epoch 37/50, Batch 7100/7628\n","Epoch 37/50, Batch 7200/7628\n","Epoch 37/50, Batch 7300/7628\n","Epoch 37/50, Batch 7400/7628\n","Epoch 37/50, Batch 7500/7628\n","Epoch 37/50, Batch 7600/7628\n","Epoch 37/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 38/50, Batch 0/7628\n","Epoch 38/50, Batch 100/7628\n","Epoch 38/50, Batch 200/7628\n","Epoch 38/50, Batch 300/7628\n","Epoch 38/50, Batch 400/7628\n","Epoch 38/50, Batch 500/7628\n","Epoch 38/50, Batch 600/7628\n","Epoch 38/50, Batch 700/7628\n","Epoch 38/50, Batch 800/7628\n","Epoch 38/50, Batch 900/7628\n","Epoch 38/50, Batch 1000/7628\n","Epoch 38/50, Batch 1100/7628\n","Epoch 38/50, Batch 1200/7628\n","Epoch 38/50, Batch 1300/7628\n","Epoch 38/50, Batch 1400/7628\n","Epoch 38/50, Batch 1500/7628\n","Epoch 38/50, Batch 1600/7628\n","Epoch 38/50, Batch 1700/7628\n","Epoch 38/50, Batch 1800/7628\n","Epoch 38/50, Batch 1900/7628\n","Epoch 38/50, Batch 2000/7628\n","Epoch 38/50, Batch 2100/7628\n","Epoch 38/50, Batch 2200/7628\n","Epoch 38/50, Batch 2300/7628\n","Epoch 38/50, Batch 2400/7628\n","Epoch 38/50, Batch 2500/7628\n","Epoch 38/50, Batch 2600/7628\n","Epoch 38/50, Batch 2700/7628\n","Epoch 38/50, Batch 2800/7628\n","Epoch 38/50, Batch 2900/7628\n","Epoch 38/50, Batch 3000/7628\n","Epoch 38/50, Batch 3100/7628\n","Epoch 38/50, Batch 3200/7628\n","Epoch 38/50, Batch 3300/7628\n","Epoch 38/50, Batch 3400/7628\n","Epoch 38/50, Batch 3500/7628\n","Epoch 38/50, Batch 3600/7628\n","Epoch 38/50, Batch 3700/7628\n","Epoch 38/50, Batch 3800/7628\n","Epoch 38/50, Batch 3900/7628\n","Epoch 38/50, Batch 4000/7628\n","Epoch 38/50, Batch 4100/7628\n","Epoch 38/50, Batch 4200/7628\n","Epoch 38/50, Batch 4300/7628\n","Epoch 38/50, Batch 4400/7628\n","Epoch 38/50, Batch 4500/7628\n","Epoch 38/50, Batch 4600/7628\n","Epoch 38/50, Batch 4700/7628\n","Epoch 38/50, Batch 4800/7628\n","Epoch 38/50, Batch 4900/7628\n","Epoch 38/50, Batch 5000/7628\n","Epoch 38/50, Batch 5100/7628\n","Epoch 38/50, Batch 5200/7628\n","Epoch 38/50, Batch 5300/7628\n","Epoch 38/50, Batch 5400/7628\n","Epoch 38/50, Batch 5500/7628\n","Epoch 38/50, Batch 5600/7628\n","Epoch 38/50, Batch 5700/7628\n","Epoch 38/50, Batch 5800/7628\n","Epoch 38/50, Batch 5900/7628\n","Epoch 38/50, Batch 6000/7628\n","Epoch 38/50, Batch 6100/7628\n","Epoch 38/50, Batch 6200/7628\n","Epoch 38/50, Batch 6300/7628\n","Epoch 38/50, Batch 6400/7628\n","Epoch 38/50, Batch 6500/7628\n","Epoch 38/50, Batch 6600/7628\n","Epoch 38/50, Batch 6700/7628\n","Epoch 38/50, Batch 6800/7628\n","Epoch 38/50, Batch 6900/7628\n","Epoch 38/50, Batch 7000/7628\n","Epoch 38/50, Batch 7100/7628\n","Epoch 38/50, Batch 7200/7628\n","Epoch 38/50, Batch 7300/7628\n","Epoch 38/50, Batch 7400/7628\n","Epoch 38/50, Batch 7500/7628\n","Epoch 38/50, Batch 7600/7628\n","Epoch 38/50, Loss: 0.4451\n","No improvement for 1 epoch(s)\n","Epoch 39/50, Batch 0/7628\n","Epoch 39/50, Batch 100/7628\n","Epoch 39/50, Batch 200/7628\n","Epoch 39/50, Batch 300/7628\n","Epoch 39/50, Batch 400/7628\n","Epoch 39/50, Batch 500/7628\n","Epoch 39/50, Batch 600/7628\n","Epoch 39/50, Batch 700/7628\n","Epoch 39/50, Batch 800/7628\n","Epoch 39/50, Batch 900/7628\n","Epoch 39/50, Batch 1000/7628\n","Epoch 39/50, Batch 1100/7628\n","Epoch 39/50, Batch 1200/7628\n","Epoch 39/50, Batch 1300/7628\n","Epoch 39/50, Batch 1400/7628\n","Epoch 39/50, Batch 1500/7628\n","Epoch 39/50, Batch 1600/7628\n","Epoch 39/50, Batch 1700/7628\n","Epoch 39/50, Batch 1800/7628\n","Epoch 39/50, Batch 1900/7628\n","Epoch 39/50, Batch 2000/7628\n","Epoch 39/50, Batch 2100/7628\n","Epoch 39/50, Batch 2200/7628\n","Epoch 39/50, Batch 2300/7628\n","Epoch 39/50, Batch 2400/7628\n","Epoch 39/50, Batch 2500/7628\n","Epoch 39/50, Batch 2600/7628\n","Epoch 39/50, Batch 2700/7628\n","Epoch 39/50, Batch 2800/7628\n","Epoch 39/50, Batch 2900/7628\n","Epoch 39/50, Batch 3000/7628\n","Epoch 39/50, Batch 3100/7628\n","Epoch 39/50, Batch 3200/7628\n","Epoch 39/50, Batch 3300/7628\n","Epoch 39/50, Batch 3400/7628\n","Epoch 39/50, Batch 3500/7628\n","Epoch 39/50, Batch 3600/7628\n","Epoch 39/50, Batch 3700/7628\n","Epoch 39/50, Batch 3800/7628\n","Epoch 39/50, Batch 3900/7628\n","Epoch 39/50, Batch 4000/7628\n","Epoch 39/50, Batch 4100/7628\n","Epoch 39/50, Batch 4200/7628\n","Epoch 39/50, Batch 4300/7628\n","Epoch 39/50, Batch 4400/7628\n","Epoch 39/50, Batch 4500/7628\n","Epoch 39/50, Batch 4600/7628\n","Epoch 39/50, Batch 4700/7628\n","Epoch 39/50, Batch 4800/7628\n","Epoch 39/50, Batch 4900/7628\n","Epoch 39/50, Batch 5000/7628\n","Epoch 39/50, Batch 5100/7628\n","Epoch 39/50, Batch 5200/7628\n","Epoch 39/50, Batch 5300/7628\n","Epoch 39/50, Batch 5400/7628\n","Epoch 39/50, Batch 5500/7628\n","Epoch 39/50, Batch 5600/7628\n","Epoch 39/50, Batch 5700/7628\n","Epoch 39/50, Batch 5800/7628\n","Epoch 39/50, Batch 5900/7628\n","Epoch 39/50, Batch 6000/7628\n","Epoch 39/50, Batch 6100/7628\n","Epoch 39/50, Batch 6200/7628\n","Epoch 39/50, Batch 6300/7628\n","Epoch 39/50, Batch 6400/7628\n","Epoch 39/50, Batch 6500/7628\n","Epoch 39/50, Batch 6600/7628\n","Epoch 39/50, Batch 6700/7628\n","Epoch 39/50, Batch 6800/7628\n","Epoch 39/50, Batch 6900/7628\n","Epoch 39/50, Batch 7000/7628\n","Epoch 39/50, Batch 7100/7628\n","Epoch 39/50, Batch 7200/7628\n","Epoch 39/50, Batch 7300/7628\n","Epoch 39/50, Batch 7400/7628\n","Epoch 39/50, Batch 7500/7628\n","Epoch 39/50, Batch 7600/7628\n","Epoch 39/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 40/50, Batch 0/7628\n","Epoch 40/50, Batch 100/7628\n","Epoch 40/50, Batch 200/7628\n","Epoch 40/50, Batch 300/7628\n","Epoch 40/50, Batch 400/7628\n","Epoch 40/50, Batch 500/7628\n","Epoch 40/50, Batch 600/7628\n","Epoch 40/50, Batch 700/7628\n","Epoch 40/50, Batch 800/7628\n","Epoch 40/50, Batch 900/7628\n","Epoch 40/50, Batch 1000/7628\n","Epoch 40/50, Batch 1100/7628\n","Epoch 40/50, Batch 1200/7628\n","Epoch 40/50, Batch 1300/7628\n","Epoch 40/50, Batch 1400/7628\n","Epoch 40/50, Batch 1500/7628\n","Epoch 40/50, Batch 1600/7628\n","Epoch 40/50, Batch 1700/7628\n","Epoch 40/50, Batch 1800/7628\n","Epoch 40/50, Batch 1900/7628\n","Epoch 40/50, Batch 2000/7628\n","Epoch 40/50, Batch 2100/7628\n","Epoch 40/50, Batch 2200/7628\n","Epoch 40/50, Batch 2300/7628\n","Epoch 40/50, Batch 2400/7628\n","Epoch 40/50, Batch 2500/7628\n","Epoch 40/50, Batch 2600/7628\n","Epoch 40/50, Batch 2700/7628\n","Epoch 40/50, Batch 2800/7628\n","Epoch 40/50, Batch 2900/7628\n","Epoch 40/50, Batch 3000/7628\n","Epoch 40/50, Batch 3100/7628\n","Epoch 40/50, Batch 3200/7628\n","Epoch 40/50, Batch 3300/7628\n","Epoch 40/50, Batch 3400/7628\n","Epoch 40/50, Batch 3500/7628\n","Epoch 40/50, Batch 3600/7628\n","Epoch 40/50, Batch 3700/7628\n","Epoch 40/50, Batch 3800/7628\n","Epoch 40/50, Batch 3900/7628\n","Epoch 40/50, Batch 4000/7628\n","Epoch 40/50, Batch 4100/7628\n","Epoch 40/50, Batch 4200/7628\n","Epoch 40/50, Batch 4300/7628\n","Epoch 40/50, Batch 4400/7628\n","Epoch 40/50, Batch 4500/7628\n","Epoch 40/50, Batch 4600/7628\n","Epoch 40/50, Batch 4700/7628\n","Epoch 40/50, Batch 4800/7628\n","Epoch 40/50, Batch 4900/7628\n","Epoch 40/50, Batch 5000/7628\n","Epoch 40/50, Batch 5100/7628\n","Epoch 40/50, Batch 5200/7628\n","Epoch 40/50, Batch 5300/7628\n","Epoch 40/50, Batch 5400/7628\n","Epoch 40/50, Batch 5500/7628\n","Epoch 40/50, Batch 5600/7628\n","Epoch 40/50, Batch 5700/7628\n","Epoch 40/50, Batch 5800/7628\n","Epoch 40/50, Batch 5900/7628\n","Epoch 40/50, Batch 6000/7628\n","Epoch 40/50, Batch 6100/7628\n","Epoch 40/50, Batch 6200/7628\n","Epoch 40/50, Batch 6300/7628\n","Epoch 40/50, Batch 6400/7628\n","Epoch 40/50, Batch 6500/7628\n","Epoch 40/50, Batch 6600/7628\n","Epoch 40/50, Batch 6700/7628\n","Epoch 40/50, Batch 6800/7628\n","Epoch 40/50, Batch 6900/7628\n","Epoch 40/50, Batch 7000/7628\n","Epoch 40/50, Batch 7100/7628\n","Epoch 40/50, Batch 7200/7628\n","Epoch 40/50, Batch 7300/7628\n","Epoch 40/50, Batch 7400/7628\n","Epoch 40/50, Batch 7500/7628\n","Epoch 40/50, Batch 7600/7628\n","Epoch 40/50, Loss: 0.4451\n","No improvement for 1 epoch(s)\n","Epoch 41/50, Batch 0/7628\n","Epoch 41/50, Batch 100/7628\n","Epoch 41/50, Batch 200/7628\n","Epoch 41/50, Batch 300/7628\n","Epoch 41/50, Batch 400/7628\n","Epoch 41/50, Batch 500/7628\n","Epoch 41/50, Batch 600/7628\n","Epoch 41/50, Batch 700/7628\n","Epoch 41/50, Batch 800/7628\n","Epoch 41/50, Batch 900/7628\n","Epoch 41/50, Batch 1000/7628\n","Epoch 41/50, Batch 1100/7628\n","Epoch 41/50, Batch 1200/7628\n","Epoch 41/50, Batch 1300/7628\n","Epoch 41/50, Batch 1400/7628\n","Epoch 41/50, Batch 1500/7628\n","Epoch 41/50, Batch 1600/7628\n","Epoch 41/50, Batch 1700/7628\n","Epoch 41/50, Batch 1800/7628\n","Epoch 41/50, Batch 1900/7628\n","Epoch 41/50, Batch 2000/7628\n","Epoch 41/50, Batch 2100/7628\n","Epoch 41/50, Batch 2200/7628\n","Epoch 41/50, Batch 2300/7628\n","Epoch 41/50, Batch 2400/7628\n","Epoch 41/50, Batch 2500/7628\n","Epoch 41/50, Batch 2600/7628\n","Epoch 41/50, Batch 2700/7628\n","Epoch 41/50, Batch 2800/7628\n","Epoch 41/50, Batch 2900/7628\n","Epoch 41/50, Batch 3000/7628\n","Epoch 41/50, Batch 3100/7628\n","Epoch 41/50, Batch 3200/7628\n","Epoch 41/50, Batch 3300/7628\n","Epoch 41/50, Batch 3400/7628\n","Epoch 41/50, Batch 3500/7628\n","Epoch 41/50, Batch 3600/7628\n","Epoch 41/50, Batch 3700/7628\n","Epoch 41/50, Batch 3800/7628\n","Epoch 41/50, Batch 3900/7628\n","Epoch 41/50, Batch 4000/7628\n","Epoch 41/50, Batch 4100/7628\n","Epoch 41/50, Batch 4200/7628\n","Epoch 41/50, Batch 4300/7628\n","Epoch 41/50, Batch 4400/7628\n","Epoch 41/50, Batch 4500/7628\n","Epoch 41/50, Batch 4600/7628\n","Epoch 41/50, Batch 4700/7628\n","Epoch 41/50, Batch 4800/7628\n","Epoch 41/50, Batch 4900/7628\n","Epoch 41/50, Batch 5000/7628\n","Epoch 41/50, Batch 5100/7628\n","Epoch 41/50, Batch 5200/7628\n","Epoch 41/50, Batch 5300/7628\n","Epoch 41/50, Batch 5400/7628\n","Epoch 41/50, Batch 5500/7628\n","Epoch 41/50, Batch 5600/7628\n","Epoch 41/50, Batch 5700/7628\n","Epoch 41/50, Batch 5800/7628\n","Epoch 41/50, Batch 5900/7628\n","Epoch 41/50, Batch 6000/7628\n","Epoch 41/50, Batch 6100/7628\n","Epoch 41/50, Batch 6200/7628\n","Epoch 41/50, Batch 6300/7628\n","Epoch 41/50, Batch 6400/7628\n","Epoch 41/50, Batch 6500/7628\n","Epoch 41/50, Batch 6600/7628\n","Epoch 41/50, Batch 6700/7628\n","Epoch 41/50, Batch 6800/7628\n","Epoch 41/50, Batch 6900/7628\n","Epoch 41/50, Batch 7000/7628\n","Epoch 41/50, Batch 7100/7628\n","Epoch 41/50, Batch 7200/7628\n","Epoch 41/50, Batch 7300/7628\n","Epoch 41/50, Batch 7400/7628\n","Epoch 41/50, Batch 7500/7628\n","Epoch 41/50, Batch 7600/7628\n","Epoch 41/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 42/50, Batch 0/7628\n","Epoch 42/50, Batch 100/7628\n","Epoch 42/50, Batch 200/7628\n","Epoch 42/50, Batch 300/7628\n","Epoch 42/50, Batch 400/7628\n","Epoch 42/50, Batch 500/7628\n","Epoch 42/50, Batch 600/7628\n","Epoch 42/50, Batch 700/7628\n","Epoch 42/50, Batch 800/7628\n","Epoch 42/50, Batch 900/7628\n","Epoch 42/50, Batch 1000/7628\n","Epoch 42/50, Batch 1100/7628\n","Epoch 42/50, Batch 1200/7628\n","Epoch 42/50, Batch 1300/7628\n","Epoch 42/50, Batch 1400/7628\n","Epoch 42/50, Batch 1500/7628\n","Epoch 42/50, Batch 1600/7628\n","Epoch 42/50, Batch 1700/7628\n","Epoch 42/50, Batch 1800/7628\n","Epoch 42/50, Batch 1900/7628\n","Epoch 42/50, Batch 2000/7628\n","Epoch 42/50, Batch 2100/7628\n","Epoch 42/50, Batch 2200/7628\n","Epoch 42/50, Batch 2300/7628\n","Epoch 42/50, Batch 2400/7628\n","Epoch 42/50, Batch 2500/7628\n","Epoch 42/50, Batch 2600/7628\n","Epoch 42/50, Batch 2700/7628\n","Epoch 42/50, Batch 2800/7628\n","Epoch 42/50, Batch 2900/7628\n","Epoch 42/50, Batch 3000/7628\n","Epoch 42/50, Batch 3100/7628\n","Epoch 42/50, Batch 3200/7628\n","Epoch 42/50, Batch 3300/7628\n","Epoch 42/50, Batch 3400/7628\n","Epoch 42/50, Batch 3500/7628\n","Epoch 42/50, Batch 3600/7628\n","Epoch 42/50, Batch 3700/7628\n","Epoch 42/50, Batch 3800/7628\n","Epoch 42/50, Batch 3900/7628\n","Epoch 42/50, Batch 4000/7628\n","Epoch 42/50, Batch 4100/7628\n","Epoch 42/50, Batch 4200/7628\n","Epoch 42/50, Batch 4300/7628\n","Epoch 42/50, Batch 4400/7628\n","Epoch 42/50, Batch 4500/7628\n","Epoch 42/50, Batch 4600/7628\n","Epoch 42/50, Batch 4700/7628\n","Epoch 42/50, Batch 4800/7628\n","Epoch 42/50, Batch 4900/7628\n","Epoch 42/50, Batch 5000/7628\n","Epoch 42/50, Batch 5100/7628\n","Epoch 42/50, Batch 5200/7628\n","Epoch 42/50, Batch 5300/7628\n","Epoch 42/50, Batch 5400/7628\n","Epoch 42/50, Batch 5500/7628\n","Epoch 42/50, Batch 5600/7628\n","Epoch 42/50, Batch 5700/7628\n","Epoch 42/50, Batch 5800/7628\n","Epoch 42/50, Batch 5900/7628\n","Epoch 42/50, Batch 6000/7628\n","Epoch 42/50, Batch 6100/7628\n","Epoch 42/50, Batch 6200/7628\n","Epoch 42/50, Batch 6300/7628\n","Epoch 42/50, Batch 6400/7628\n","Epoch 42/50, Batch 6500/7628\n","Epoch 42/50, Batch 6600/7628\n","Epoch 42/50, Batch 6700/7628\n","Epoch 42/50, Batch 6800/7628\n","Epoch 42/50, Batch 6900/7628\n","Epoch 42/50, Batch 7000/7628\n","Epoch 42/50, Batch 7100/7628\n","Epoch 42/50, Batch 7200/7628\n","Epoch 42/50, Batch 7300/7628\n","Epoch 42/50, Batch 7400/7628\n","Epoch 42/50, Batch 7500/7628\n","Epoch 42/50, Batch 7600/7628\n","Epoch 42/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 43/50, Batch 0/7628\n","Epoch 43/50, Batch 100/7628\n","Epoch 43/50, Batch 200/7628\n","Epoch 43/50, Batch 300/7628\n","Epoch 43/50, Batch 400/7628\n","Epoch 43/50, Batch 500/7628\n","Epoch 43/50, Batch 600/7628\n","Epoch 43/50, Batch 700/7628\n","Epoch 43/50, Batch 800/7628\n","Epoch 43/50, Batch 900/7628\n","Epoch 43/50, Batch 1000/7628\n","Epoch 43/50, Batch 1100/7628\n","Epoch 43/50, Batch 1200/7628\n","Epoch 43/50, Batch 1300/7628\n","Epoch 43/50, Batch 1400/7628\n","Epoch 43/50, Batch 1500/7628\n","Epoch 43/50, Batch 1600/7628\n","Epoch 43/50, Batch 1700/7628\n","Epoch 43/50, Batch 1800/7628\n","Epoch 43/50, Batch 1900/7628\n","Epoch 43/50, Batch 2000/7628\n","Epoch 43/50, Batch 2100/7628\n","Epoch 43/50, Batch 2200/7628\n","Epoch 43/50, Batch 2300/7628\n","Epoch 43/50, Batch 2400/7628\n","Epoch 43/50, Batch 2500/7628\n","Epoch 43/50, Batch 2600/7628\n","Epoch 43/50, Batch 2700/7628\n","Epoch 43/50, Batch 2800/7628\n","Epoch 43/50, Batch 2900/7628\n","Epoch 43/50, Batch 3000/7628\n","Epoch 43/50, Batch 3100/7628\n","Epoch 43/50, Batch 3200/7628\n","Epoch 43/50, Batch 3300/7628\n","Epoch 43/50, Batch 3400/7628\n","Epoch 43/50, Batch 3500/7628\n","Epoch 43/50, Batch 3600/7628\n","Epoch 43/50, Batch 3700/7628\n","Epoch 43/50, Batch 3800/7628\n","Epoch 43/50, Batch 3900/7628\n","Epoch 43/50, Batch 4000/7628\n","Epoch 43/50, Batch 4100/7628\n","Epoch 43/50, Batch 4200/7628\n","Epoch 43/50, Batch 4300/7628\n","Epoch 43/50, Batch 4400/7628\n","Epoch 43/50, Batch 4500/7628\n","Epoch 43/50, Batch 4600/7628\n","Epoch 43/50, Batch 4700/7628\n","Epoch 43/50, Batch 4800/7628\n","Epoch 43/50, Batch 4900/7628\n","Epoch 43/50, Batch 5000/7628\n","Epoch 43/50, Batch 5100/7628\n","Epoch 43/50, Batch 5200/7628\n","Epoch 43/50, Batch 5300/7628\n","Epoch 43/50, Batch 5400/7628\n","Epoch 43/50, Batch 5500/7628\n","Epoch 43/50, Batch 5600/7628\n","Epoch 43/50, Batch 5700/7628\n","Epoch 43/50, Batch 5800/7628\n","Epoch 43/50, Batch 5900/7628\n","Epoch 43/50, Batch 6000/7628\n","Epoch 43/50, Batch 6100/7628\n","Epoch 43/50, Batch 6200/7628\n","Epoch 43/50, Batch 6300/7628\n","Epoch 43/50, Batch 6400/7628\n","Epoch 43/50, Batch 6500/7628\n","Epoch 43/50, Batch 6600/7628\n","Epoch 43/50, Batch 6700/7628\n","Epoch 43/50, Batch 6800/7628\n","Epoch 43/50, Batch 6900/7628\n","Epoch 43/50, Batch 7000/7628\n","Epoch 43/50, Batch 7100/7628\n","Epoch 43/50, Batch 7200/7628\n","Epoch 43/50, Batch 7300/7628\n","Epoch 43/50, Batch 7400/7628\n","Epoch 43/50, Batch 7500/7628\n","Epoch 43/50, Batch 7600/7628\n","Epoch 43/50, Loss: 0.4451\n","No improvement for 1 epoch(s)\n","Epoch 44/50, Batch 0/7628\n","Epoch 44/50, Batch 100/7628\n","Epoch 44/50, Batch 200/7628\n","Epoch 44/50, Batch 300/7628\n","Epoch 44/50, Batch 400/7628\n","Epoch 44/50, Batch 500/7628\n","Epoch 44/50, Batch 600/7628\n","Epoch 44/50, Batch 700/7628\n","Epoch 44/50, Batch 800/7628\n","Epoch 44/50, Batch 900/7628\n","Epoch 44/50, Batch 1000/7628\n","Epoch 44/50, Batch 1100/7628\n","Epoch 44/50, Batch 1200/7628\n","Epoch 44/50, Batch 1300/7628\n","Epoch 44/50, Batch 1400/7628\n","Epoch 44/50, Batch 1500/7628\n","Epoch 44/50, Batch 1600/7628\n","Epoch 44/50, Batch 1700/7628\n","Epoch 44/50, Batch 1800/7628\n","Epoch 44/50, Batch 1900/7628\n","Epoch 44/50, Batch 2000/7628\n","Epoch 44/50, Batch 2100/7628\n","Epoch 44/50, Batch 2200/7628\n","Epoch 44/50, Batch 2300/7628\n","Epoch 44/50, Batch 2400/7628\n","Epoch 44/50, Batch 2500/7628\n","Epoch 44/50, Batch 2600/7628\n","Epoch 44/50, Batch 2700/7628\n","Epoch 44/50, Batch 2800/7628\n","Epoch 44/50, Batch 2900/7628\n","Epoch 44/50, Batch 3000/7628\n","Epoch 44/50, Batch 3100/7628\n","Epoch 44/50, Batch 3200/7628\n","Epoch 44/50, Batch 3300/7628\n","Epoch 44/50, Batch 3400/7628\n","Epoch 44/50, Batch 3500/7628\n","Epoch 44/50, Batch 3600/7628\n","Epoch 44/50, Batch 3700/7628\n","Epoch 44/50, Batch 3800/7628\n","Epoch 44/50, Batch 3900/7628\n","Epoch 44/50, Batch 4000/7628\n","Epoch 44/50, Batch 4100/7628\n","Epoch 44/50, Batch 4200/7628\n","Epoch 44/50, Batch 4300/7628\n","Epoch 44/50, Batch 4400/7628\n","Epoch 44/50, Batch 4500/7628\n","Epoch 44/50, Batch 4600/7628\n","Epoch 44/50, Batch 4700/7628\n","Epoch 44/50, Batch 4800/7628\n","Epoch 44/50, Batch 4900/7628\n","Epoch 44/50, Batch 5000/7628\n","Epoch 44/50, Batch 5100/7628\n","Epoch 44/50, Batch 5200/7628\n","Epoch 44/50, Batch 5300/7628\n","Epoch 44/50, Batch 5400/7628\n","Epoch 44/50, Batch 5500/7628\n","Epoch 44/50, Batch 5600/7628\n","Epoch 44/50, Batch 5700/7628\n","Epoch 44/50, Batch 5800/7628\n","Epoch 44/50, Batch 5900/7628\n","Epoch 44/50, Batch 6000/7628\n","Epoch 44/50, Batch 6100/7628\n","Epoch 44/50, Batch 6200/7628\n","Epoch 44/50, Batch 6300/7628\n","Epoch 44/50, Batch 6400/7628\n","Epoch 44/50, Batch 6500/7628\n","Epoch 44/50, Batch 6600/7628\n","Epoch 44/50, Batch 6700/7628\n","Epoch 44/50, Batch 6800/7628\n","Epoch 44/50, Batch 6900/7628\n","Epoch 44/50, Batch 7000/7628\n","Epoch 44/50, Batch 7100/7628\n","Epoch 44/50, Batch 7200/7628\n","Epoch 44/50, Batch 7300/7628\n","Epoch 44/50, Batch 7400/7628\n","Epoch 44/50, Batch 7500/7628\n","Epoch 44/50, Batch 7600/7628\n","Epoch 44/50, Loss: 0.4451\n","No improvement for 2 epoch(s)\n","Epoch 45/50, Batch 0/7628\n","Epoch 45/50, Batch 100/7628\n","Epoch 45/50, Batch 200/7628\n","Epoch 45/50, Batch 300/7628\n","Epoch 45/50, Batch 400/7628\n","Epoch 45/50, Batch 500/7628\n","Epoch 45/50, Batch 600/7628\n","Epoch 45/50, Batch 700/7628\n","Epoch 45/50, Batch 800/7628\n","Epoch 45/50, Batch 900/7628\n","Epoch 45/50, Batch 1000/7628\n","Epoch 45/50, Batch 1100/7628\n","Epoch 45/50, Batch 1200/7628\n","Epoch 45/50, Batch 1300/7628\n","Epoch 45/50, Batch 1400/7628\n","Epoch 45/50, Batch 1500/7628\n","Epoch 45/50, Batch 1600/7628\n","Epoch 45/50, Batch 1700/7628\n","Epoch 45/50, Batch 1800/7628\n","Epoch 45/50, Batch 1900/7628\n","Epoch 45/50, Batch 2000/7628\n","Epoch 45/50, Batch 2100/7628\n","Epoch 45/50, Batch 2200/7628\n","Epoch 45/50, Batch 2300/7628\n","Epoch 45/50, Batch 2400/7628\n","Epoch 45/50, Batch 2500/7628\n","Epoch 45/50, Batch 2600/7628\n","Epoch 45/50, Batch 2700/7628\n","Epoch 45/50, Batch 2800/7628\n","Epoch 45/50, Batch 2900/7628\n","Epoch 45/50, Batch 3000/7628\n","Epoch 45/50, Batch 3100/7628\n","Epoch 45/50, Batch 3200/7628\n","Epoch 45/50, Batch 3300/7628\n","Epoch 45/50, Batch 3400/7628\n","Epoch 45/50, Batch 3500/7628\n","Epoch 45/50, Batch 3600/7628\n","Epoch 45/50, Batch 3700/7628\n","Epoch 45/50, Batch 3800/7628\n","Epoch 45/50, Batch 3900/7628\n","Epoch 45/50, Batch 4000/7628\n","Epoch 45/50, Batch 4100/7628\n","Epoch 45/50, Batch 4200/7628\n","Epoch 45/50, Batch 4300/7628\n","Epoch 45/50, Batch 4400/7628\n","Epoch 45/50, Batch 4500/7628\n","Epoch 45/50, Batch 4600/7628\n","Epoch 45/50, Batch 4700/7628\n","Epoch 45/50, Batch 4800/7628\n","Epoch 45/50, Batch 4900/7628\n","Epoch 45/50, Batch 5000/7628\n","Epoch 45/50, Batch 5100/7628\n","Epoch 45/50, Batch 5200/7628\n","Epoch 45/50, Batch 5300/7628\n","Epoch 45/50, Batch 5400/7628\n","Epoch 45/50, Batch 5500/7628\n","Epoch 45/50, Batch 5600/7628\n","Epoch 45/50, Batch 5700/7628\n","Epoch 45/50, Batch 5800/7628\n","Epoch 45/50, Batch 5900/7628\n","Epoch 45/50, Batch 6000/7628\n","Epoch 45/50, Batch 6100/7628\n","Epoch 45/50, Batch 6200/7628\n","Epoch 45/50, Batch 6300/7628\n","Epoch 45/50, Batch 6400/7628\n","Epoch 45/50, Batch 6500/7628\n","Epoch 45/50, Batch 6600/7628\n","Epoch 45/50, Batch 6700/7628\n","Epoch 45/50, Batch 6800/7628\n","Epoch 45/50, Batch 6900/7628\n","Epoch 45/50, Batch 7000/7628\n","Epoch 45/50, Batch 7100/7628\n","Epoch 45/50, Batch 7200/7628\n","Epoch 45/50, Batch 7300/7628\n","Epoch 45/50, Batch 7400/7628\n","Epoch 45/50, Batch 7500/7628\n","Epoch 45/50, Batch 7600/7628\n","Epoch 45/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 46/50, Batch 0/7628\n","Epoch 46/50, Batch 100/7628\n","Epoch 46/50, Batch 200/7628\n","Epoch 46/50, Batch 300/7628\n","Epoch 46/50, Batch 400/7628\n","Epoch 46/50, Batch 500/7628\n","Epoch 46/50, Batch 600/7628\n","Epoch 46/50, Batch 700/7628\n","Epoch 46/50, Batch 800/7628\n","Epoch 46/50, Batch 900/7628\n","Epoch 46/50, Batch 1000/7628\n","Epoch 46/50, Batch 1100/7628\n","Epoch 46/50, Batch 1200/7628\n","Epoch 46/50, Batch 1300/7628\n","Epoch 46/50, Batch 1400/7628\n","Epoch 46/50, Batch 1500/7628\n","Epoch 46/50, Batch 1600/7628\n","Epoch 46/50, Batch 1700/7628\n","Epoch 46/50, Batch 1800/7628\n","Epoch 46/50, Batch 1900/7628\n","Epoch 46/50, Batch 2000/7628\n","Epoch 46/50, Batch 2100/7628\n","Epoch 46/50, Batch 2200/7628\n","Epoch 46/50, Batch 2300/7628\n","Epoch 46/50, Batch 2400/7628\n","Epoch 46/50, Batch 2500/7628\n","Epoch 46/50, Batch 2600/7628\n","Epoch 46/50, Batch 2700/7628\n","Epoch 46/50, Batch 2800/7628\n","Epoch 46/50, Batch 2900/7628\n","Epoch 46/50, Batch 3000/7628\n","Epoch 46/50, Batch 3100/7628\n","Epoch 46/50, Batch 3200/7628\n","Epoch 46/50, Batch 3300/7628\n","Epoch 46/50, Batch 3400/7628\n","Epoch 46/50, Batch 3500/7628\n","Epoch 46/50, Batch 3600/7628\n","Epoch 46/50, Batch 3700/7628\n","Epoch 46/50, Batch 3800/7628\n","Epoch 46/50, Batch 3900/7628\n","Epoch 46/50, Batch 4000/7628\n","Epoch 46/50, Batch 4100/7628\n","Epoch 46/50, Batch 4200/7628\n","Epoch 46/50, Batch 4300/7628\n","Epoch 46/50, Batch 4400/7628\n","Epoch 46/50, Batch 4500/7628\n","Epoch 46/50, Batch 4600/7628\n","Epoch 46/50, Batch 4700/7628\n","Epoch 46/50, Batch 4800/7628\n","Epoch 46/50, Batch 4900/7628\n","Epoch 46/50, Batch 5000/7628\n","Epoch 46/50, Batch 5100/7628\n","Epoch 46/50, Batch 5200/7628\n","Epoch 46/50, Batch 5300/7628\n","Epoch 46/50, Batch 5400/7628\n","Epoch 46/50, Batch 5500/7628\n","Epoch 46/50, Batch 5600/7628\n","Epoch 46/50, Batch 5700/7628\n","Epoch 46/50, Batch 5800/7628\n","Epoch 46/50, Batch 5900/7628\n","Epoch 46/50, Batch 6000/7628\n","Epoch 46/50, Batch 6100/7628\n","Epoch 46/50, Batch 6200/7628\n","Epoch 46/50, Batch 6300/7628\n","Epoch 46/50, Batch 6400/7628\n","Epoch 46/50, Batch 6500/7628\n","Epoch 46/50, Batch 6600/7628\n","Epoch 46/50, Batch 6700/7628\n","Epoch 46/50, Batch 6800/7628\n","Epoch 46/50, Batch 6900/7628\n","Epoch 46/50, Batch 7000/7628\n","Epoch 46/50, Batch 7100/7628\n","Epoch 46/50, Batch 7200/7628\n","Epoch 46/50, Batch 7300/7628\n","Epoch 46/50, Batch 7400/7628\n","Epoch 46/50, Batch 7500/7628\n","Epoch 46/50, Batch 7600/7628\n","Epoch 46/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 47/50, Batch 0/7628\n","Epoch 47/50, Batch 100/7628\n","Epoch 47/50, Batch 200/7628\n","Epoch 47/50, Batch 300/7628\n","Epoch 47/50, Batch 400/7628\n","Epoch 47/50, Batch 500/7628\n","Epoch 47/50, Batch 600/7628\n","Epoch 47/50, Batch 700/7628\n","Epoch 47/50, Batch 800/7628\n","Epoch 47/50, Batch 900/7628\n","Epoch 47/50, Batch 1000/7628\n","Epoch 47/50, Batch 1100/7628\n","Epoch 47/50, Batch 1200/7628\n","Epoch 47/50, Batch 1300/7628\n","Epoch 47/50, Batch 1400/7628\n","Epoch 47/50, Batch 1500/7628\n","Epoch 47/50, Batch 1600/7628\n","Epoch 47/50, Batch 1700/7628\n","Epoch 47/50, Batch 1800/7628\n","Epoch 47/50, Batch 1900/7628\n","Epoch 47/50, Batch 2000/7628\n","Epoch 47/50, Batch 2100/7628\n","Epoch 47/50, Batch 2200/7628\n","Epoch 47/50, Batch 2300/7628\n","Epoch 47/50, Batch 2400/7628\n","Epoch 47/50, Batch 2500/7628\n","Epoch 47/50, Batch 2600/7628\n","Epoch 47/50, Batch 2700/7628\n","Epoch 47/50, Batch 2800/7628\n","Epoch 47/50, Batch 2900/7628\n","Epoch 47/50, Batch 3000/7628\n","Epoch 47/50, Batch 3100/7628\n","Epoch 47/50, Batch 3200/7628\n","Epoch 47/50, Batch 3300/7628\n","Epoch 47/50, Batch 3400/7628\n","Epoch 47/50, Batch 3500/7628\n","Epoch 47/50, Batch 3600/7628\n","Epoch 47/50, Batch 3700/7628\n","Epoch 47/50, Batch 3800/7628\n","Epoch 47/50, Batch 3900/7628\n","Epoch 47/50, Batch 4000/7628\n","Epoch 47/50, Batch 4100/7628\n","Epoch 47/50, Batch 4200/7628\n","Epoch 47/50, Batch 4300/7628\n","Epoch 47/50, Batch 4400/7628\n","Epoch 47/50, Batch 4500/7628\n","Epoch 47/50, Batch 4600/7628\n","Epoch 47/50, Batch 4700/7628\n","Epoch 47/50, Batch 4800/7628\n","Epoch 47/50, Batch 4900/7628\n","Epoch 47/50, Batch 5000/7628\n","Epoch 47/50, Batch 5100/7628\n","Epoch 47/50, Batch 5200/7628\n","Epoch 47/50, Batch 5300/7628\n","Epoch 47/50, Batch 5400/7628\n","Epoch 47/50, Batch 5500/7628\n","Epoch 47/50, Batch 5600/7628\n","Epoch 47/50, Batch 5700/7628\n","Epoch 47/50, Batch 5800/7628\n","Epoch 47/50, Batch 5900/7628\n","Epoch 47/50, Batch 6000/7628\n","Epoch 47/50, Batch 6100/7628\n","Epoch 47/50, Batch 6200/7628\n","Epoch 47/50, Batch 6300/7628\n","Epoch 47/50, Batch 6400/7628\n","Epoch 47/50, Batch 6500/7628\n","Epoch 47/50, Batch 6600/7628\n","Epoch 47/50, Batch 6700/7628\n","Epoch 47/50, Batch 6800/7628\n","Epoch 47/50, Batch 6900/7628\n","Epoch 47/50, Batch 7000/7628\n","Epoch 47/50, Batch 7100/7628\n","Epoch 47/50, Batch 7200/7628\n","Epoch 47/50, Batch 7300/7628\n","Epoch 47/50, Batch 7400/7628\n","Epoch 47/50, Batch 7500/7628\n","Epoch 47/50, Batch 7600/7628\n","Epoch 47/50, Loss: 0.4451\n","Best model saved with loss: 0.4451\n","Epoch 48/50, Batch 0/7628\n","Epoch 48/50, Batch 100/7628\n","Epoch 48/50, Batch 200/7628\n","Epoch 48/50, Batch 300/7628\n","Epoch 48/50, Batch 400/7628\n","Epoch 48/50, Batch 500/7628\n","Epoch 48/50, Batch 600/7628\n","Epoch 48/50, Batch 700/7628\n","Epoch 48/50, Batch 800/7628\n","Epoch 48/50, Batch 900/7628\n","Epoch 48/50, Batch 1000/7628\n","Epoch 48/50, Batch 1100/7628\n","Epoch 48/50, Batch 1200/7628\n","Epoch 48/50, Batch 1300/7628\n","Epoch 48/50, Batch 1400/7628\n","Epoch 48/50, Batch 1500/7628\n","Epoch 48/50, Batch 1600/7628\n","Epoch 48/50, Batch 1700/7628\n","Epoch 48/50, Batch 1800/7628\n","Epoch 48/50, Batch 1900/7628\n","Epoch 48/50, Batch 2000/7628\n","Epoch 48/50, Batch 2100/7628\n","Epoch 48/50, Batch 2200/7628\n","Epoch 48/50, Batch 2300/7628\n","Epoch 48/50, Batch 2400/7628\n","Epoch 48/50, Batch 2500/7628\n","Epoch 48/50, Batch 2600/7628\n","Epoch 48/50, Batch 2700/7628\n","Epoch 48/50, Batch 2800/7628\n","Epoch 48/50, Batch 2900/7628\n","Epoch 48/50, Batch 3000/7628\n","Epoch 48/50, Batch 3100/7628\n","Epoch 48/50, Batch 3200/7628\n","Epoch 48/50, Batch 3300/7628\n","Epoch 48/50, Batch 3400/7628\n","Epoch 48/50, Batch 3500/7628\n","Epoch 48/50, Batch 3600/7628\n","Epoch 48/50, Batch 3700/7628\n","Epoch 48/50, Batch 3800/7628\n","Epoch 48/50, Batch 3900/7628\n","Epoch 48/50, Batch 4000/7628\n","Epoch 48/50, Batch 4100/7628\n","Epoch 48/50, Batch 4200/7628\n","Epoch 48/50, Batch 4300/7628\n","Epoch 48/50, Batch 4400/7628\n","Epoch 48/50, Batch 4500/7628\n","Epoch 48/50, Batch 4600/7628\n","Epoch 48/50, Batch 4700/7628\n","Epoch 48/50, Batch 4800/7628\n","Epoch 48/50, Batch 4900/7628\n","Epoch 48/50, Batch 5000/7628\n","Epoch 48/50, Batch 5100/7628\n","Epoch 48/50, Batch 5200/7628\n","Epoch 48/50, Batch 5300/7628\n","Epoch 48/50, Batch 5400/7628\n","Epoch 48/50, Batch 5500/7628\n","Epoch 48/50, Batch 5600/7628\n","Epoch 48/50, Batch 5700/7628\n","Epoch 48/50, Batch 5800/7628\n","Epoch 48/50, Batch 5900/7628\n","Epoch 48/50, Batch 6000/7628\n","Epoch 48/50, Batch 6100/7628\n","Epoch 48/50, Batch 6200/7628\n","Epoch 48/50, Batch 6300/7628\n","Epoch 48/50, Batch 6400/7628\n","Epoch 48/50, Batch 6500/7628\n","Epoch 48/50, Batch 6600/7628\n","Epoch 48/50, Batch 6700/7628\n","Epoch 48/50, Batch 6800/7628\n","Epoch 48/50, Batch 6900/7628\n","Epoch 48/50, Batch 7000/7628\n","Epoch 48/50, Batch 7100/7628\n","Epoch 48/50, Batch 7200/7628\n","Epoch 48/50, Batch 7300/7628\n","Epoch 48/50, Batch 7400/7628\n","Epoch 48/50, Batch 7500/7628\n","Epoch 48/50, Batch 7600/7628\n","Epoch 48/50, Loss: 0.4450\n","Best model saved with loss: 0.4450\n","Epoch 49/50, Batch 0/7628\n","Epoch 49/50, Batch 100/7628\n","Epoch 49/50, Batch 200/7628\n","Epoch 49/50, Batch 300/7628\n","Epoch 49/50, Batch 400/7628\n","Epoch 49/50, Batch 500/7628\n","Epoch 49/50, Batch 600/7628\n","Epoch 49/50, Batch 700/7628\n","Epoch 49/50, Batch 800/7628\n","Epoch 49/50, Batch 900/7628\n","Epoch 49/50, Batch 1000/7628\n","Epoch 49/50, Batch 1100/7628\n","Epoch 49/50, Batch 1200/7628\n","Epoch 49/50, Batch 1300/7628\n","Epoch 49/50, Batch 1400/7628\n","Epoch 49/50, Batch 1500/7628\n","Epoch 49/50, Batch 1600/7628\n","Epoch 49/50, Batch 1700/7628\n","Epoch 49/50, Batch 1800/7628\n","Epoch 49/50, Batch 1900/7628\n","Epoch 49/50, Batch 2000/7628\n","Epoch 49/50, Batch 2100/7628\n","Epoch 49/50, Batch 2200/7628\n","Epoch 49/50, Batch 2300/7628\n","Epoch 49/50, Batch 2400/7628\n","Epoch 49/50, Batch 2500/7628\n","Epoch 49/50, Batch 2600/7628\n","Epoch 49/50, Batch 2700/7628\n","Epoch 49/50, Batch 2800/7628\n","Epoch 49/50, Batch 2900/7628\n","Epoch 49/50, Batch 3000/7628\n","Epoch 49/50, Batch 3100/7628\n","Epoch 49/50, Batch 3200/7628\n","Epoch 49/50, Batch 3300/7628\n","Epoch 49/50, Batch 3400/7628\n","Epoch 49/50, Batch 3500/7628\n","Epoch 49/50, Batch 3600/7628\n","Epoch 49/50, Batch 3700/7628\n","Epoch 49/50, Batch 3800/7628\n","Epoch 49/50, Batch 3900/7628\n","Epoch 49/50, Batch 4000/7628\n","Epoch 49/50, Batch 4100/7628\n","Epoch 49/50, Batch 4200/7628\n","Epoch 49/50, Batch 4300/7628\n","Epoch 49/50, Batch 4400/7628\n","Epoch 49/50, Batch 4500/7628\n","Epoch 49/50, Batch 4600/7628\n","Epoch 49/50, Batch 4700/7628\n","Epoch 49/50, Batch 4800/7628\n","Epoch 49/50, Batch 4900/7628\n","Epoch 49/50, Batch 5000/7628\n","Epoch 49/50, Batch 5100/7628\n","Epoch 49/50, Batch 5200/7628\n","Epoch 49/50, Batch 5300/7628\n","Epoch 49/50, Batch 5400/7628\n","Epoch 49/50, Batch 5500/7628\n","Epoch 49/50, Batch 5600/7628\n","Epoch 49/50, Batch 5700/7628\n","Epoch 49/50, Batch 5800/7628\n","Epoch 49/50, Batch 5900/7628\n","Epoch 49/50, Batch 6000/7628\n","Epoch 49/50, Batch 6100/7628\n","Epoch 49/50, Batch 6200/7628\n","Epoch 49/50, Batch 6300/7628\n","Epoch 49/50, Batch 6400/7628\n","Epoch 49/50, Batch 6500/7628\n","Epoch 49/50, Batch 6600/7628\n","Epoch 49/50, Batch 6700/7628\n","Epoch 49/50, Batch 6800/7628\n","Epoch 49/50, Batch 6900/7628\n","Epoch 49/50, Batch 7000/7628\n","Epoch 49/50, Batch 7100/7628\n","Epoch 49/50, Batch 7200/7628\n","Epoch 49/50, Batch 7300/7628\n","Epoch 49/50, Batch 7400/7628\n","Epoch 49/50, Batch 7500/7628\n","Epoch 49/50, Batch 7600/7628\n","Epoch 49/50, Loss: 0.4450\n","No improvement for 1 epoch(s)\n","Epoch 50/50, Batch 0/7628\n","Epoch 50/50, Batch 100/7628\n","Epoch 50/50, Batch 200/7628\n","Epoch 50/50, Batch 300/7628\n","Epoch 50/50, Batch 400/7628\n","Epoch 50/50, Batch 500/7628\n","Epoch 50/50, Batch 600/7628\n","Epoch 50/50, Batch 700/7628\n","Epoch 50/50, Batch 800/7628\n","Epoch 50/50, Batch 900/7628\n","Epoch 50/50, Batch 1000/7628\n","Epoch 50/50, Batch 1100/7628\n","Epoch 50/50, Batch 1200/7628\n","Epoch 50/50, Batch 1300/7628\n","Epoch 50/50, Batch 1400/7628\n","Epoch 50/50, Batch 1500/7628\n","Epoch 50/50, Batch 1600/7628\n","Epoch 50/50, Batch 1700/7628\n","Epoch 50/50, Batch 1800/7628\n","Epoch 50/50, Batch 1900/7628\n","Epoch 50/50, Batch 2000/7628\n","Epoch 50/50, Batch 2100/7628\n","Epoch 50/50, Batch 2200/7628\n","Epoch 50/50, Batch 2300/7628\n","Epoch 50/50, Batch 2400/7628\n","Epoch 50/50, Batch 2500/7628\n","Epoch 50/50, Batch 2600/7628\n","Epoch 50/50, Batch 2700/7628\n","Epoch 50/50, Batch 2800/7628\n","Epoch 50/50, Batch 2900/7628\n","Epoch 50/50, Batch 3000/7628\n","Epoch 50/50, Batch 3100/7628\n","Epoch 50/50, Batch 3200/7628\n","Epoch 50/50, Batch 3300/7628\n","Epoch 50/50, Batch 3400/7628\n","Epoch 50/50, Batch 3500/7628\n","Epoch 50/50, Batch 3600/7628\n","Epoch 50/50, Batch 3700/7628\n","Epoch 50/50, Batch 3800/7628\n","Epoch 50/50, Batch 3900/7628\n","Epoch 50/50, Batch 4000/7628\n","Epoch 50/50, Batch 4100/7628\n","Epoch 50/50, Batch 4200/7628\n","Epoch 50/50, Batch 4300/7628\n","Epoch 50/50, Batch 4400/7628\n","Epoch 50/50, Batch 4500/7628\n","Epoch 50/50, Batch 4600/7628\n","Epoch 50/50, Batch 4700/7628\n","Epoch 50/50, Batch 4800/7628\n","Epoch 50/50, Batch 4900/7628\n","Epoch 50/50, Batch 5000/7628\n","Epoch 50/50, Batch 5100/7628\n","Epoch 50/50, Batch 5200/7628\n","Epoch 50/50, Batch 5300/7628\n","Epoch 50/50, Batch 5400/7628\n","Epoch 50/50, Batch 5500/7628\n","Epoch 50/50, Batch 5600/7628\n","Epoch 50/50, Batch 5700/7628\n","Epoch 50/50, Batch 5800/7628\n","Epoch 50/50, Batch 5900/7628\n","Epoch 50/50, Batch 6000/7628\n","Epoch 50/50, Batch 6100/7628\n","Epoch 50/50, Batch 6200/7628\n","Epoch 50/50, Batch 6300/7628\n","Epoch 50/50, Batch 6400/7628\n","Epoch 50/50, Batch 6500/7628\n","Epoch 50/50, Batch 6600/7628\n","Epoch 50/50, Batch 6700/7628\n","Epoch 50/50, Batch 6800/7628\n","Epoch 50/50, Batch 6900/7628\n","Epoch 50/50, Batch 7000/7628\n","Epoch 50/50, Batch 7100/7628\n","Epoch 50/50, Batch 7200/7628\n","Epoch 50/50, Batch 7300/7628\n","Epoch 50/50, Batch 7400/7628\n","Epoch 50/50, Batch 7500/7628\n","Epoch 50/50, Batch 7600/7628\n","Epoch 50/50, Loss: 0.4451\n","No improvement for 2 epoch(s)\n","Best model loaded!\n","Model saved successfully!\n","Embeddings saved successfully!\n"]}]},{"cell_type":"code","source":["# TwoTowerRecall Example\n","# 1. Load model\n","model = TwoTowerRecall(user_feature_dim=8, item_feature_dim=10, embedding_dim=64)\n","checkpoint = torch.load('two_tower_model.pth')\n","model.user_tower.load_state_dict(checkpoint['user_tower'])\n","model.item_tower.load_state_dict(checkpoint['item_tower'])\n","model.user_tower = model.user_tower.to(device)\n","model.item_tower = model.item_tower.to(device)\n","\n","# 2. Load embeddings from database\n","try:\n","    with psycopg2.connect(\n","        host=os.getenv('DB_HOST'),\n","        port=os.getenv('DB_PORT'),\n","        dbname=os.getenv('DB_NAME'),\n","        user=os.getenv('DB_USER'),\n","        password=os.getenv('DB_PASSWORD'),\n","        sslmode='require'\n","    ) as conn:\n","        with conn.cursor() as cursor:\n","            cursor.execute(\"\"\"\n","            SELECT \"ProductID\", \"Embedding\"\n","            FROM item_embeddings\n","            \"\"\")\n","            item_data = cursor.fetchall()\n","\n","            item_id_to_idx = {pid: i for i, (pid, _) in enumerate(item_data)}\n","            item_embeddings = np.array([emb for _, emb in item_data])\n","            print(\"Item embeddings loaded successfully!\")\n","except Exception as e:\n","    print(f\"Error loading embeddings: {e}\")\n","\n","# 3. Recall (example)\n","# Example: user_id=12345 with features\n","user_id = 2820558652430377474\n","user_features = user_feature_dict[user_id]\n","user_tensor = torch.FloatTensor([user_features]).to(device)\n","user_embedding = model.encode_users(user_tensor)[0]\n","\n","recommendations_idx = model.recall(user_embedding, item_embeddings, top_k=500)\n","recommendations = [list(item_id_to_idx.keys())[idx] for idx in recommendations_idx]\n","print(f\"Recalled {len(recommendations)} candidates from TwoTower\")\n","print(f\"Top 10 candidates: {recommendations[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQxfHFYHDnlV","executionInfo":{"status":"ok","timestamp":1770774792121,"user_tz":-60,"elapsed":7020,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"9febe09d-d4b9-483f-ee1e-e4a73effd535"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Item embeddings loaded successfully!\n","Recalled 500 candidates from TwoTower\n","Top 10 candidates: [1870088131203853896, 3393324386710413510, 4104639588574737041, 7255470514920952691, 5832182010792345789, 1677626284184941631, 4720422053137086922, 1025562429060199077, 56020583862746888, 280451233911325994]\n"]}]},{"cell_type":"markdown","source":["### 2) Collaborative Filtering"],"metadata":{"id":"B2h2Rg-gFgR1"}},{"cell_type":"markdown","source":["- #### Item-based Collaborative Filtering (ItemCF)\n","  ItemCF computes item-to-item similarity by analyzing co-occurrence patterns in user purchase histories—items frequently bought by the same users are deemed similar—then recommends items similar to those the user has previously interacted with, capturing product relationships without requiring user profiling.\n","\n","  user purchase history → identify purchased items → compute item-item similarity (via co-purchase patterns) → retrieve items similar to user's history → recommend related products."],"metadata":{"id":"jwUulnVxFqWS"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.sparse import csr_matrix\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","class ItemCF:\n","\n","    def __init__(self, db_config: Dict[str, str]):\n","        \"\"\"\n","        Initialize ItemCF recommender.\n","        Args:\n","            db_config: Database connection configuration dictionary\n","        \"\"\"\n","\n","        self.db_config = db_config\n","        self.item_similarity = {}\n","\n","    def compute_similarity(self, min_common_users: int = 5, top_k: int = 100):\n","        \"\"\"\n","        Compute item-item similarity matrix using vectorized operations.\n","        Args:\n","            min_common_users: Minimum number of common users required for similarity calculation\n","            top_k: Keep top-K most similar items for each item\n","        \"\"\"\n","\n","        # Load user-item interactions from database\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ClientID\", \"ProductID\", \"PurchaseCount\"\n","                    FROM user_item_interactions\n","                    \"\"\")\n","                    interactions = cursor.fetchall()\n","        except Exception as e:\n","            print(f\"Error loading user-item interactions: {e}\")\n","            return\n","\n","        # Build sparse user-item matrix\n","        user_id_map = {}\n","        item_id_map = {}\n","        rows, cols, data = [], [], []\n","\n","        for client_id, product_id, count in interactions:\n","            if client_id not in user_id_map:\n","                user_id_map[client_id] = len(user_id_map)\n","            if product_id not in item_id_map:\n","                item_id_map[product_id] = len(item_id_map)\n","\n","            rows.append(user_id_map[client_id])\n","            cols.append(item_id_map[product_id])\n","            data.append(count)\n","\n","        user_item_matrix = csr_matrix(\n","            (data, (rows, cols)),\n","            shape=(len(user_id_map), len(item_id_map))\n","        )\n","\n","        print(f\"Matrix shape: {user_item_matrix.shape}\")\n","        print(f\"Sparsity: {1 - user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1]):.4f}\")\n","\n","        # Compute item-item cosine similarity\n","        item_item_sim = cosine_similarity(user_item_matrix.T, dense_output=False)\n","\n","        # Compute co-occurrence count\n","        user_item_binary = (user_item_matrix > 0).astype(np.float32)\n","        co_occurrence = user_item_binary.T.dot(user_item_binary)\n","\n","        # Convert to CSR format\n","        item_item_sim = item_item_sim.tocsr()\n","        co_occurrence = co_occurrence.tocsr()\n","\n","        # Build reverse mapping\n","        item_id_reverse = {idx: pid for pid, idx in item_id_map.items()}\n","\n","        # Extract top-K similar items for each item\n","        for item_idx in range(len(item_id_map)):\n","            item_a = item_id_reverse[item_idx]\n","\n","            # Get non-zero similarity indices\n","            sim_row = item_item_sim[item_idx]\n","            nonzero_cols = sim_row.nonzero()[1]\n","\n","            # Filter out self-similarity\n","            nonzero_cols = nonzero_cols[nonzero_cols != item_idx]\n","\n","            if len(nonzero_cols) == 0:\n","                self.item_similarity[item_a] = []\n","                continue\n","\n","            # Get similarity values\n","            sim_values = sim_row[0, nonzero_cols].toarray().ravel()\n","\n","            # Get co-occurrence counts\n","            cooc_values = co_occurrence[item_idx, nonzero_cols].toarray().ravel()\n","\n","            # Filter by minimum common users\n","            valid_mask = cooc_values >= min_common_users\n","            valid_indices = nonzero_cols[valid_mask]\n","            valid_similarities = sim_values[valid_mask]\n","\n","            if len(valid_indices) == 0:\n","                self.item_similarity[item_a] = []\n","                continue\n","\n","            # Select top-K candidates\n","            if len(valid_indices) <= top_k:\n","                top_local_indices = np.arange(len(valid_indices))\n","            else:\n","                top_local_indices = np.argpartition(valid_similarities, -top_k)[-top_k:]\n","\n","            # Sort top-K by similarity score\n","            top_local_indices = top_local_indices[np.argsort(valid_similarities[top_local_indices])[::-1]]\n","\n","            # Build similarity list\n","            similarities = [\n","                (item_id_reverse[valid_indices[i]], float(valid_similarities[i]))\n","                for i in top_local_indices\n","            ]\n","\n","            self.item_similarity[item_a] = similarities\n","\n","            if (item_idx + 1) % 1000 == 0:\n","                print(f\"Processed {item_idx + 1}/{len(item_id_map)} items\")\n","\n","    def save_to_db(self):\n","        \"\"\"\n","        Save computed item similarity to database.\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    # Clear existing similarity data\n","                    cursor.execute(\"TRUNCATE TABLE item_similarity\")\n","\n","                    # Prepare batch insert data\n","                    data = []\n","                    for item_a, similar_items in self.item_similarity.items():\n","                        for item_b, similarity in similar_items:\n","                            data.append((item_a, item_b, similarity, len(similar_items)))\n","\n","                    # Batch insert similarity records\n","                    execute_values(cursor, \"\"\"\n","                    INSERT INTO item_similarity (\"ProductID_A\", \"ProductID_B\", \"Similarity\", \"CoOccurrenceCount\")\n","                    VALUES %s\n","                    \"\"\", data)\n","\n","                    conn.commit()\n","                    print(\"Item similarity saved successfully!\")\n","        except Exception as e:\n","            print(f\"Error saving item similarity: {e}\")\n","\n","    def load_from_db(self):\n","        \"\"\"\n","        Load item similarity from database.\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ProductID_A\", \"ProductID_B\", \"Similarity\"\n","                    FROM item_similarity\n","                    ORDER BY \"ProductID_A\", \"Similarity\" DESC\n","                    \"\"\")\n","\n","                    rows = cursor.fetchall()\n","\n","                    for item_a, item_b, similarity in rows:\n","                        if item_a not in self.item_similarity:\n","                            self.item_similarity[item_a] = []\n","                        self.item_similarity[item_a].append((item_b, float(similarity)))\n","\n","                    print(f\"Item similarity loaded successfully! Total items: {len(self.item_similarity)}\")\n","        except Exception as e:\n","            print(f\"Error loading item similarity: {e}\")\n","\n","    def recall(self, user_purchased_items: List[int], top_k: int = 300) -> List[int]:\n","        \"\"\"\n","        Recall candidate items based on user purchase history.\n","        Args:\n","            user_purchased_items: List of product IDs user has purchased\n","            top_k: Number of candidate items to recall\n","        Returns:\n","            List of recommended product IDs ranked by aggregated similarity score\n","        \"\"\"\n","\n","        # Aggregate similarity scores from purchased items\n","        candidate_scores = {}\n","\n","        for item_id in user_purchased_items:\n","            if item_id not in self.item_similarity:\n","                continue\n","\n","            for similar_item, similarity in self.item_similarity[item_id]:\n","                # Skip items user already purchased\n","                if similar_item in user_purchased_items:\n","                    continue\n","\n","                # Accumulate similarity scores\n","                if similar_item not in candidate_scores:\n","                    candidate_scores[similar_item] = 0\n","                candidate_scores[similar_item] += similarity\n","\n","        # Sort candidates by score\n","        candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n","        return [item_id for item_id, score in candidates[:top_k]]"],"metadata":{"id":"Trr-WWkSJcAS","executionInfo":{"status":"ok","timestamp":1770777886144,"user_tz":-60,"elapsed":45,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# ItemCF Computation Process\n","# 1. Initialize\n","db_config = {\n","    'host': os.getenv('DB_HOST'),\n","    'port': os.getenv('DB_PORT'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'sslmode': 'require'\n","}\n","\n","item_cf = ItemCF(db_config)\n","\n","# 2. Compute similarity\n","item_cf.compute_similarity(min_common_users=5, top_k=100)\n","\n","# 3. Save to database\n","item_cf.save_to_db()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XGZYtNyAPPv","executionInfo":{"status":"ok","timestamp":1770770828435,"user_tz":-60,"elapsed":181302,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"bb0be226-cd0a-4be7-f591-da55fa122472"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix shape: (304929, 29730)\n","Sparsity: 0.9999\n","Processed 1000/29730 items\n","Processed 2000/29730 items\n","Processed 3000/29730 items\n","Processed 4000/29730 items\n","Processed 7000/29730 items\n","Processed 9000/29730 items\n","Processed 17000/29730 items\n","Item similarity saved successfully!\n"]}]},{"cell_type":"code","source":["# 4. Recall (example)\n","# Example: user purchased items [1194042316259024227, 910783516894460725, 733557522770138156]\n","user_purchased = [1194042316259024227, 910783516894460725, 733557522770138156]\n","recommendations = item_cf.recall(user_purchased, top_k=300)\n","print(f\"Recalled {len(recommendations)} candidates from ItemCF\")\n","print(f\"Top 10 candidates: {recommendations[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfsjwNn0D6bI","executionInfo":{"status":"ok","timestamp":1770774801453,"user_tz":-60,"elapsed":48,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"01db6114-4d49-4ac6-b8d8-fef19c0598cf"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Recalled 85 candidates from ItemCF\n","Top 10 candidates: [6306151556054509590, 1233079729704511877, 1002319356880102587, 4735736287986553092, 1518309762619768287, 4872929861894050765, 2475752611632244287, 3625979375121276682, 6093539354933854031, 6448349970795474714]\n"]}]},{"cell_type":"markdown","source":["- #### User-based Collaborative Filtering (UserCF)\n","  UserCF identifies users with similar behavioral patterns by comparing their purchase histories, then recommends items purchased by these similar users but not yet acquired by the target user, effectively leveraging collective intelligence from user communities with closely aligned preferences.\n","\n","  target user's purchase history → find similar users (via overlapping purchases) → aggregate items bought by similar users → filter out already-purchased items → recommend from similar users' preferences."],"metadata":{"id":"u5pMqS_KGNku"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.sparse import csr_matrix\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","class UserCF:\n","\n","    def __init__(self, db_config: Dict[str, str]):\n","        \"\"\"\n","        Initialize UserCF recommender.\n","        Args:\n","            db_config: Database connection configuration dictionary\n","        \"\"\"\n","\n","        self.db_config = db_config\n","        self.user_similarity = {}\n","\n","    def compute_similarity(self, min_common_items: int = 3, top_k: int = 50):\n","        \"\"\"\n","        Compute user-user similarity matrix using vectorized operations.\n","        Args:\n","            min_common_items: Minimum number of common items required for similarity calculation\n","            top_k: Keep top-K most similar users for each user\n","        \"\"\"\n","\n","        # Load user-item interactions from database\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ClientID\", \"ProductID\", \"PurchaseCount\"\n","                    FROM user_item_interactions\n","                    \"\"\")\n","                    interactions = cursor.fetchall()\n","        except Exception as e:\n","            print(f\"Error loading user-item interactions: {e}\")\n","            return\n","\n","        # Build sparse user-item matrix\n","        user_id_map = {}\n","        item_id_map = {}\n","        rows, cols, data = [], [], []\n","\n","        for client_id, product_id, count in interactions:\n","            if client_id not in user_id_map:\n","                user_id_map[client_id] = len(user_id_map)\n","            if product_id not in item_id_map:\n","                item_id_map[product_id] = len(item_id_map)\n","\n","            rows.append(user_id_map[client_id])\n","            cols.append(item_id_map[product_id])\n","            data.append(count)\n","\n","        user_item_matrix = csr_matrix(\n","            (data, (rows, cols)),\n","            shape=(len(user_id_map), len(item_id_map))\n","        )\n","\n","        print(f\"Matrix shape: {user_item_matrix.shape}\")\n","        print(f\"Sparsity: {1 - user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1]):.4f}\")\n","\n","        # Compute user-user cosine similarity\n","        user_user_sim = cosine_similarity(user_item_matrix, dense_output=False)\n","\n","        # Compute co-occurrence count\n","        user_item_binary = (user_item_matrix > 0).astype(np.float32)\n","        co_occurrence = user_item_binary.dot(user_item_binary.T)\n","\n","        # Convert to CSR format\n","        user_user_sim = user_user_sim.tocsr()\n","        co_occurrence = co_occurrence.tocsr()\n","\n","        # Build reverse mapping\n","        user_id_reverse = {idx: uid for uid, idx in user_id_map.items()}\n","\n","        # Extract top-K similar users for each user\n","        for user_idx in range(len(user_id_map)):\n","            user_a = user_id_reverse[user_idx]\n","\n","            # Get non-zero similarity indices\n","            sim_row = user_user_sim[user_idx]\n","            nonzero_cols = sim_row.nonzero()[1]\n","\n","            # Filter out self-similarity\n","            nonzero_cols = nonzero_cols[nonzero_cols != user_idx]\n","\n","            if len(nonzero_cols) == 0:\n","                self.user_similarity[user_a] = []\n","                continue\n","\n","            # Get similarity values\n","            sim_values = sim_row[0, nonzero_cols].toarray().ravel()\n","\n","            # Get co-occurrence counts\n","            cooc_values = co_occurrence[user_idx, nonzero_cols].toarray().ravel()\n","\n","            # Filter by minimum common items\n","            valid_mask = cooc_values >= min_common_items\n","            valid_indices = nonzero_cols[valid_mask]\n","            valid_similarities = sim_values[valid_mask]\n","            valid_common_items = cooc_values[valid_mask]\n","\n","            if len(valid_indices) == 0:\n","                self.user_similarity[user_a] = []\n","                continue\n","\n","            # Select top-K candidates\n","            if len(valid_indices) <= top_k:\n","                top_local_indices = np.arange(len(valid_indices))\n","            else:\n","                top_local_indices = np.argpartition(valid_similarities, -top_k)[-top_k:]\n","\n","            # Sort top-K by similarity score\n","            top_local_indices = top_local_indices[np.argsort(valid_similarities[top_local_indices])[::-1]]\n","\n","            # Build similarity list\n","            similarities = [\n","                (user_id_reverse[valid_indices[i]], float(valid_similarities[i]), int(valid_common_items[i]))\n","                for i in top_local_indices\n","            ]\n","\n","            self.user_similarity[user_a] = similarities\n","\n","            if (user_idx + 1) % 1000 == 0:\n","                print(f\"Processed {user_idx + 1}/{len(user_id_map)} users\")\n","\n","    def save_to_db(self):\n","        \"\"\"\n","        Save computed user similarity to database.\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    # Clear existing similarity data\n","                    cursor.execute(\"TRUNCATE TABLE user_similarity\")\n","\n","                    # Prepare batch insert data\n","                    data = []\n","                    for user_a, similar_users in self.user_similarity.items():\n","                        for user_b, similarity, common_items in similar_users:\n","                            data.append((user_a, user_b, similarity, common_items))\n","\n","                    # Batch insert similarity records\n","                    execute_values(cursor, \"\"\"\n","                    INSERT INTO user_similarity (\"ClientID_A\", \"ClientID_B\", \"Similarity\", \"CommonProducts\")\n","                    VALUES %s\n","                    \"\"\", data)\n","\n","                    conn.commit()\n","                    print(\"User similarity saved successfully!\")\n","        except Exception as e:\n","            print(f\"Error saving user similarity: {e}\")\n","\n","    def load_from_db(self):\n","        \"\"\"\n","        Load user similarity from database.\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ClientID_A\", \"ClientID_B\", \"Similarity\", \"CommonProducts\"\n","                    FROM user_similarity\n","                    ORDER BY \"ClientID_A\", \"Similarity\" DESC\n","                    \"\"\")\n","\n","                    rows = cursor.fetchall()\n","\n","                    for user_a, user_b, similarity, common_products in rows:\n","                        if user_a not in self.user_similarity:\n","                            self.user_similarity[user_a] = []\n","                        self.user_similarity[user_a].append((user_b, float(similarity), int(common_products)))\n","\n","                    print(f\"User similarity loaded successfully! Total users: {len(self.user_similarity)}\")\n","        except Exception as e:\n","            print(f\"Error loading user similarity: {e}\")\n","\n","    def recall(self, user_id: int, user_purchased_items: List[int], top_k: int = 200) -> List[int]:\n","        \"\"\"\n","        Recall candidate items based on similar users' purchases.\n","        Args:\n","            user_id: Target user ID\n","            user_purchased_items: List of item IDs target user has purchased\n","            top_k: Number of candidate items to recall\n","        Returns:\n","            List of recommended item IDs ranked by aggregated similarity score\n","        \"\"\"\n","\n","        if user_id not in self.user_similarity:\n","            return []\n","\n","        # Load similar users' purchases from database\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    similar_user_ids = [user_b for user_b, _, _ in self.user_similarity[user_id]]\n","\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ClientID\", \"ProductID\", \"PurchaseCount\"\n","                    FROM user_item_interactions\n","                    WHERE \"ClientID\" = ANY(%s)\n","                    \"\"\", (similar_user_ids,))\n","\n","                    interactions = cursor.fetchall()\n","        except Exception as e:\n","            print(f\"Error loading similar users' purchases: {e}\")\n","            return []\n","\n","        # Aggregate similarity scores from similar users\n","        candidate_scores = {}\n","        user_sim_dict = {user_b: sim for user_b, sim, _ in self.user_similarity[user_id]}\n","\n","        for similar_user, item_id, count in interactions:\n","            # Skip items user already purchased\n","            if item_id in user_purchased_items:\n","                continue\n","\n","            # Accumulate similarity scores\n","            if item_id not in candidate_scores:\n","                candidate_scores[item_id] = 0\n","            candidate_scores[item_id] += user_sim_dict[similar_user]\n","\n","        # Sort candidates by score\n","        candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n","        return [item_id for item_id, score in candidates[:top_k]]"],"metadata":{"id":"ADJ0CbQQGNE4","executionInfo":{"status":"ok","timestamp":1770777934625,"user_tz":-60,"elapsed":16,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# UserCF Computation Process\n","# 1. Initialize\n","db_config = {\n","    'host': os.getenv('DB_HOST'),\n","    'port': os.getenv('DB_PORT'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'sslmode': 'require'\n","}\n","\n","user_cf = UserCF(db_config)\n","\n","# 2. Compute similarity\n","user_cf.compute_similarity(min_common_items=3, top_k=50)\n","\n","# 3. Save to database\n","user_cf.save_to_db()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quqU5zeGAaUj","executionInfo":{"status":"ok","timestamp":1770772153481,"user_tz":-60,"elapsed":1034275,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"8bad2d09-de09-44a3-b686-6bac11a3736f"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix shape: (304929, 29730)\n","Sparsity: 0.9999\n","Processed 1000/304929 users\n","Processed 2000/304929 users\n","Processed 5000/304929 users\n","Processed 6000/304929 users\n","Processed 7000/304929 users\n","Processed 10000/304929 users\n","Processed 11000/304929 users\n","Processed 12000/304929 users\n","Processed 14000/304929 users\n","Processed 19000/304929 users\n","Processed 21000/304929 users\n","Processed 23000/304929 users\n","Processed 24000/304929 users\n","Processed 25000/304929 users\n","Processed 26000/304929 users\n","Processed 27000/304929 users\n","Processed 31000/304929 users\n","Processed 33000/304929 users\n","Processed 35000/304929 users\n","Processed 36000/304929 users\n","Processed 38000/304929 users\n","Processed 40000/304929 users\n","Processed 48000/304929 users\n","Processed 49000/304929 users\n","Processed 54000/304929 users\n","Processed 55000/304929 users\n","Processed 58000/304929 users\n","Processed 59000/304929 users\n","Processed 60000/304929 users\n","Processed 76000/304929 users\n","Processed 81000/304929 users\n","Processed 85000/304929 users\n","Processed 92000/304929 users\n","Processed 96000/304929 users\n","Processed 102000/304929 users\n","Processed 109000/304929 users\n","Processed 110000/304929 users\n","Processed 111000/304929 users\n","Processed 118000/304929 users\n","Processed 125000/304929 users\n","Processed 136000/304929 users\n","Processed 139000/304929 users\n","Processed 153000/304929 users\n","Processed 182000/304929 users\n","Processed 189000/304929 users\n","Processed 195000/304929 users\n","User similarity saved successfully!\n"]}]},{"cell_type":"code","source":["# 4. Recall (example)\n","# Example: user_id=354804904215332, purchased items [7309427166711388171, 5102145079475080421, 8277585465807360411]\n","user_id = 354804904215332\n","user_purchased = [7309427166711388171, 5102145079475080421, 8277585465807360411]\n","recommendations = user_cf.recall(user_id, user_purchased, top_k=200)\n","print(f\"Recalled {len(recommendations)} candidates from UserCF\")\n","print(f\"Top 10 candidates: {recommendations[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97eqeA4dAgUp","executionInfo":{"status":"ok","timestamp":1770774822480,"user_tz":-60,"elapsed":1040,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"29c9bc05-e6c3-4f1c-cfc2-43f699bceced"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Recalled 87 candidates from UserCF\n","Top 10 candidates: [1297447046109733993, 7860705371944056504, 1756872849309157178, 5607669002671406521, 5271488841287897246, 1635127223391354351, 901948250157397415, 882983173168811119, 8105826504302370754, 1604895678997680431]\n"]}]},{"cell_type":"markdown","source":["### 3) Popularity-based Recall"],"metadata":{"id":"E2B_WijeGgai"}},{"cell_type":"markdown","source":["- #### Global Popularity\n","  Global popularity surfaces trending items by ranking products based on recent engagement metrics (purchases, GMV, or conversion rates) across the entire platform, providing baseline recommendations that effectively capture mainstream demand and serve as highly effective cold-start solutions.\n","\n","  aggregate platform-wide engagement (purchases/clicks/GMV) → rank items by popularity metrics → apply time decay (recent trends weighted higher) → retrieve top-N trending items → baseline recommendations."],"metadata":{"id":"v5n9MpGlG1t7"}},{"cell_type":"code","source":["from typing import List, Dict\n","import psycopg2\n","\n","class GlobalPopular:\n","\n","    def __init__(self, db_config: Dict[str, str]):\n","        \"\"\"\n","        Initialize global popularity recall.\n","        Args:\n","            db_config: Database connection configuration\n","        \"\"\"\n","\n","        self.db_config = db_config\n","        self.popular_items = []\n","\n","    def load_from_db(self, top_k: int = 1000):\n","        \"\"\"\n","        Load popular items from database.\n","        Args:\n","            top_k: Number of popular items to load\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ProductID\", \"PopularityScore\"\n","                    FROM global_popular\n","                    ORDER BY \"Rank\" ASC\n","                    LIMIT %s\n","                    \"\"\", (top_k,))\n","\n","                    self.popular_items = [(product_id, score) for product_id, score in cursor.fetchall()]\n","                    print(f\"Loaded {len(self.popular_items)} popular items!\")\n","        except Exception as e:\n","            print(f\"Error loading popular items: {e}\")\n","\n","    def recall(self, top_k: int = 200) -> List[int]:\n","        \"\"\"\n","        Recall top-K popular items.\n","        Args:\n","            top_k: Number of items to recall\n","        Returns:\n","            List of popular item IDs\n","        \"\"\"\n","\n","        return [item_id for item_id, score in self.popular_items[:top_k]]"],"metadata":{"id":"5odUnr1tJcpZ","executionInfo":{"status":"ok","timestamp":1770762050225,"user_tz":-60,"elapsed":37,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# GlobalPopular Example\n","# 1. Initialize\n","db_config = {\n","    'host': os.getenv('DB_HOST'),\n","    'port': os.getenv('DB_PORT'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'sslmode': 'require'\n","}\n","\n","global_popular = GlobalPopular(db_config)\n","\n","# 2. Load from database\n","global_popular.load_from_db(top_k=1000)\n","\n","# 3. Recall (example)\n","# Example: recall top 200 global popular items\n","recommendations = global_popular.recall(top_k=200)\n","print(f\"Recalled {len(recommendations)} candidates from GlobalPopular\")\n","print(f\"Top 10 candidates: {recommendations[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8HRvUgnDRUd","executionInfo":{"status":"ok","timestamp":1770774839349,"user_tz":-60,"elapsed":1177,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"f12d3fee-b77f-4478-e6f2-3ebdd3816248"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 1000 popular items!\n","Recalled 200 candidates from GlobalPopular\n","Top 10 candidates: [7409975748690869382, 7762597382304525219, 1421634154572742367, 1053601088228117848, 5626151450577313519, 1108527350021568362, 8761826855035940162, 8360937214612273557, 8193836411099087368, 6804252697652906339]\n"]}]},{"cell_type":"markdown","source":["- #### Interest-based Category Recall\n","  Interest-based recall extracts user preference profiles from historical purchases—aggregating category distributions, brand affinities, and price sensitivities—then retrieves top-performing items within each identified interest category, personalizing recommendations while maintaining diversity across user preference dimensions.\n","\n","  user purchase history → extract category/brand/price preferences → weight by recency & frequency → identify top interest dimensions → retrieve high-quality items per category → diversified personalized candidates."],"metadata":{"id":"-w3hM4V_G8OK"}},{"cell_type":"code","source":["from typing import List, Dict\n","import psycopg2\n","\n","class CategoryPopular:\n","\n","    def __init__(self, db_config: Dict[str, str]):\n","        \"\"\"\n","        Initialize category popularity recall.\n","        Args:\n","            db_config: Database connection configuration\n","        \"\"\"\n","\n","        self.db_config = db_config\n","        self.category_items = {}\n","\n","    def load_from_db(self, top_k_per_category: int = 100):\n","        \"\"\"\n","        Load popular items per category from database.\n","        Args:\n","            top_k_per_category: Number of popular items to load per category\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"Category\", \"ProductID\", \"PopularityScore\"\n","                    FROM category_popular\n","                    WHERE \"Rank\" <= %s\n","                    ORDER BY \"Category\", \"Rank\" ASC\n","                    \"\"\", (top_k_per_category,))\n","\n","                    for category, product_id, score in cursor.fetchall():\n","                        if category not in self.category_items:\n","                            self.category_items[category] = []\n","                        self.category_items[category].append((product_id, score))\n","\n","                    print(f\"Loaded popular items for {len(self.category_items)} categories!\")\n","        except Exception as e:\n","            print(f\"Error loading category popular items: {e}\")\n","\n","    def recall(self, user_categories: List[str], top_k: int = 200) -> List[int]:\n","        \"\"\"\n","        Recall popular items from user's preferred categories.\n","        Args:\n","            user_categories: List of user's preferred categories (e.g., top 3)\n","            top_k: Number of items to recall\n","        Returns:\n","            List of popular item IDs across user's categories\n","        \"\"\"\n","\n","        candidates = []\n","        items_per_category = top_k // len(user_categories) if user_categories else 0\n","\n","        for category in user_categories:\n","            if category not in self.category_items:\n","                continue\n","\n","            category_items = self.category_items[category][:items_per_category]\n","            candidates.extend([item_id for item_id, score in category_items])\n","\n","        return candidates[:top_k]"],"metadata":{"id":"DsT-pDdtzJtS","executionInfo":{"status":"ok","timestamp":1770762067397,"user_tz":-60,"elapsed":6,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# CategoryPopular Example\n","# 1. Initialize\n","db_config = {\n","    'host': os.getenv('DB_HOST'),\n","    'port': os.getenv('DB_PORT'),\n","    'dbname': os.getenv('DB_NAME'),\n","    'user': os.getenv('DB_USER'),\n","    'password': os.getenv('DB_PASSWORD'),\n","    'sslmode': 'require'\n","}\n","\n","category_popular = CategoryPopular(db_config)\n","\n","# 2. Load from database\n","category_popular.load_from_db(top_k_per_category=100)\n","\n","# 3. Recall (example)\n","# Example: user's top categories are ['Softball', 'Basketball', 'Football']\n","user_categories = ['Softball', 'Basketball', 'Football']\n","recommendations = category_popular.recall(user_categories, top_k=200)\n","print(f\"Recalled {len(recommendations)} candidates from CategoryPopular\")\n","print(f\"Top 10 candidates: {recommendations[:10]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giHunk4fDJAe","executionInfo":{"status":"ok","timestamp":1770774854583,"user_tz":-60,"elapsed":1295,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"6018a5a6-5368-4397-bd88-488d9bef7c24"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded popular items for 16 categories!\n","Recalled 198 candidates from CategoryPopular\n","Top 10 candidates: [7144655757625465514, 7547771225294777844, 1892025359185548203, 4421504418588540577, 3499264361068470157, 6860993275576814774, 4014156734236123534, 5699178692738563273, 663096478818697557, 8486138029609193439]\n"]}]},{"cell_type":"markdown","source":["## Stage 2: Coarse Ranking"],"metadata":{"id":"uvN4VzfQzTV5"}},{"cell_type":"markdown","source":["A lightweight model quickly scores all recalled candidates using core features and compressed representations, filtering the pool from thousands to hundreds while maintaining low latency, acting as an efficient buffer between broad retrieval and expensive precise ranking."],"metadata":{"id":"3jD69-QdzXJM"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from typing import List, Dict, Tuple\n","import numpy as np\n","\n","class CoarseRanking:\n","\n","    def __init__(self, feature_dim: int, hidden_dim: int = 64):\n","        \"\"\"\n","        Initialize coarse ranking model.\n","        Args:\n","            feature_dim: Input feature dimension\n","            hidden_dim: Hidden layer dimension\n","        \"\"\"\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(feature_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1),\n","            nn.Sigmoid()\n","        )\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n","\n","    def train_step(self, features: torch.Tensor, labels: torch.Tensor) -> float:\n","        \"\"\"\n","        Single training step.\n","        Args:\n","            features: Input features [batch_size, feature_dim]\n","            labels: Binary labels [batch_size]\n","        Returns:\n","            Loss value\n","        \"\"\"\n","\n","        self.model.train()\n","        self.optimizer.zero_grad()\n","\n","        predictions = self.model(features).squeeze()\n","        loss = nn.BCELoss()(predictions, labels.float())\n","\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","    def predict(self, features: torch.Tensor) -> np.ndarray:\n","        \"\"\"\n","        Predict scores for candidates.\n","        Args:\n","            features: Input features [num_candidates, feature_dim]\n","        Returns:\n","            Predicted scores [num_candidates]\n","        \"\"\"\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            scores = self.model(features).squeeze()\n","        self.model.train()\n","        return scores.cpu().numpy()\n","\n","    def rank(self, candidate_ids: List[int], features: torch.Tensor, top_k: int = 500) -> List[int]:\n","        \"\"\"\n","        Rank candidates and return top-K.\n","        Args:\n","            candidate_ids: List of candidate item IDs\n","            features: Features for all candidates [num_candidates, feature_dim]\n","            top_k: Number of items to keep\n","        Returns:\n","            Top-K candidate IDs after coarse ranking\n","        \"\"\"\n","        scores = self.predict(features)\n","\n","        # Sort by score descending\n","        ranked_indices = np.argsort(scores)[::-1][:top_k]\n","\n","        return [candidate_ids[i] for i in ranked_indices]"],"metadata":{"id":"m95qzTWrzVIR","executionInfo":{"status":"ok","timestamp":1770762224555,"user_tz":-60,"elapsed":2,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","# 0. Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# 1. Load training data from database\n","try:\n","    with psycopg2.connect(\n","        host=os.getenv('DB_HOST'),\n","        port=os.getenv('DB_PORT'),\n","        dbname=os.getenv('DB_NAME'),\n","        user=os.getenv('DB_USER'),\n","        password=os.getenv('DB_PASSWORD'),\n","        sslmode='require'\n","    ) as conn:\n","        with conn.cursor() as cursor:\n","            cursor.execute(\"\"\"\n","            SELECT \"ClientID\", \"ProductID\"\n","            FROM user_item_interactions\n","            WHERE \"PurchaseCount\" > 0\n","            \"\"\")\n","            positive_samples = cursor.fetchall()\n","\n","            cursor.execute(\"\"\"\n","            SELECT \"ClientID\", \"TotalPurchases\", \"TotalSpendEuro\", \"AvgOrderValue\",\n","                   \"DaysSinceLastPurchase\", \"PurchaseFrequency\", \"UniqueProductsBought\",\n","                   \"TotalQuantity\", \"Age\"\n","            FROM clients\n","            \"\"\")\n","            user_features_data = cursor.fetchall()\n","\n","            cursor.execute(\"\"\"\n","            SELECT \"ProductID\", \"TotalSales\", \"TotalQuantitySold\", \"Sales7d\",\n","                   \"Sales30d\", \"AvgPrice\", \"TotalRevenue\", \"UniqueBuyers\",\n","                   \"AvgQuantityPerOrder\", \"TotalStockQuantity\", \"StockCountries\"\n","            FROM products\n","            \"\"\")\n","            item_features_data = cursor.fetchall()\n","            print(\"Training data loaded successfully!\")\n","except Exception as e:\n","    print(f\"Error loading training data: {e}\")\n","    raise\n","\n","# 2. Prepare features\n","user_feature_dict = {uid: [0 if x is None else float(x) for x in features]\n","                     for uid, *features in user_features_data}\n","item_feature_dict = {iid: [0 if x is None else float(x) for x in features]\n","                     for iid, *features in item_features_data}\n","\n","user_feature_dim = len(user_features_data[0]) - 1\n","item_feature_dim = len(item_features_data[0]) - 1\n","\n","print(f\"Total positive samples: {len(positive_samples)}\")\n","\n","# 3. Generate negative samples\n","print(\"Generating negative samples...\")\n","all_item_ids = np.array(list(item_feature_dict.keys()))\n","positive_set = set(positive_samples)\n","training_samples = []\n","\n","# Pre-generate 10 candidate negative samples for each positive sample\n","neg_candidates = np.random.choice(all_item_ids, size=(len(positive_samples), 10), replace=True)\n","\n","for idx, (user_id, item_id) in enumerate(positive_samples):\n","    if user_id not in user_feature_dict or item_id not in item_feature_dict:\n","        continue\n","\n","    user_feat = user_feature_dict[user_id]\n","    item_feat = item_feature_dict[item_id]\n","    combined_feat = user_feat + item_feat\n","    training_samples.append((combined_feat, 1))\n","\n","    # Find first candidate not in positive_set\n","    for neg_item in neg_candidates[idx]:\n","        if (user_id, neg_item) not in positive_set and neg_item in item_feature_dict:\n","            neg_item_feat = item_feature_dict[neg_item]\n","            combined_feat = user_feat + neg_item_feat\n","            training_samples.append((combined_feat, 0))\n","            break\n","\n","print(f\"Total training samples: {len(training_samples)}\")\n","\n","# 4. Prepare tensors\n","features_tensor_all = torch.FloatTensor([feat for feat, _ in training_samples])\n","labels_tensor_all = torch.FloatTensor([label for _, label in training_samples])\n","\n","dataset = TensorDataset(features_tensor_all, labels_tensor_all)\n","loader = DataLoader(\n","    dataset,\n","    batch_size=256,\n","    shuffle=True\n",")\n","\n","# 5. Initialize model\n","feature_dim = user_feature_dim + item_feature_dim\n","model = CoarseRanking(feature_dim, hidden_dim=64)\n","model.model = model.model.to(device)\n","\n","# 6. Training loop\n","epochs = 50\n","patience = 5\n","best_loss = float('inf')\n","patience_counter = 0\n","\n","for epoch in range(epochs):\n","    epoch_loss = 0\n","    num_batches = 0\n","\n","    for batch_idx, (features_tensor, labels_tensor) in enumerate(loader):\n","        features_tensor = features_tensor.to(device)\n","        labels_tensor = labels_tensor.to(device)\n","\n","        loss = model.train_step(features_tensor, labels_tensor)\n","        epoch_loss += loss\n","        num_batches += 1\n","\n","        if batch_idx % 100 == 0:\n","            print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx}/{len(loader)}\")\n","\n","    avg_loss = epoch_loss / num_batches\n","    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","    # Early stopping\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        patience_counter = 0\n","        # Save best model\n","        torch.save({\n","            'model': model.model.state_dict(),\n","            'feature_dim': feature_dim\n","        }, 'coarse_ranking_model_best.pth')\n","        print(f\"Best model saved with loss: {best_loss:.4f}\")\n","    else:\n","        patience_counter += 1\n","        print(f\"No improvement for {patience_counter} epoch(s)\")\n","\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n","            break\n","\n","# 7. Load best model\n","checkpoint = torch.load('coarse_ranking_model_best.pth')\n","model.model.load_state_dict(checkpoint['model'])\n","print(\"Best model loaded!\")\n","\n","# 8. Save final model\n","torch.save({\n","    'model': model.model.state_dict(),\n","    'feature_dim': feature_dim\n","}, 'coarse_ranking_model.pth')\n","print(\"Model saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEuxsK-6Fxtp","executionInfo":{"status":"ok","timestamp":1770762922867,"user_tz":-60,"elapsed":638009,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"25171919-c2d4-4d7d-9364-4a1139be7380"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training data loaded successfully!\n","Total positive samples: 976274\n","Generating negative samples...\n","Total training samples: 1952548\n","Epoch 1/50, Batch 0/7628\n","Epoch 1/50, Batch 100/7628\n","Epoch 1/50, Batch 200/7628\n","Epoch 1/50, Batch 300/7628\n","Epoch 1/50, Batch 400/7628\n","Epoch 1/50, Batch 500/7628\n","Epoch 1/50, Batch 600/7628\n","Epoch 1/50, Batch 700/7628\n","Epoch 1/50, Batch 800/7628\n","Epoch 1/50, Batch 900/7628\n","Epoch 1/50, Batch 1000/7628\n","Epoch 1/50, Batch 1100/7628\n","Epoch 1/50, Batch 1200/7628\n","Epoch 1/50, Batch 1300/7628\n","Epoch 1/50, Batch 1400/7628\n","Epoch 1/50, Batch 1500/7628\n","Epoch 1/50, Batch 1600/7628\n","Epoch 1/50, Batch 1700/7628\n","Epoch 1/50, Batch 1800/7628\n","Epoch 1/50, Batch 1900/7628\n","Epoch 1/50, Batch 2000/7628\n","Epoch 1/50, Batch 2100/7628\n","Epoch 1/50, Batch 2200/7628\n","Epoch 1/50, Batch 2300/7628\n","Epoch 1/50, Batch 2400/7628\n","Epoch 1/50, Batch 2500/7628\n","Epoch 1/50, Batch 2600/7628\n","Epoch 1/50, Batch 2700/7628\n","Epoch 1/50, Batch 2800/7628\n","Epoch 1/50, Batch 2900/7628\n","Epoch 1/50, Batch 3000/7628\n","Epoch 1/50, Batch 3100/7628\n","Epoch 1/50, Batch 3200/7628\n","Epoch 1/50, Batch 3300/7628\n","Epoch 1/50, Batch 3400/7628\n","Epoch 1/50, Batch 3500/7628\n","Epoch 1/50, Batch 3600/7628\n","Epoch 1/50, Batch 3700/7628\n","Epoch 1/50, Batch 3800/7628\n","Epoch 1/50, Batch 3900/7628\n","Epoch 1/50, Batch 4000/7628\n","Epoch 1/50, Batch 4100/7628\n","Epoch 1/50, Batch 4200/7628\n","Epoch 1/50, Batch 4300/7628\n","Epoch 1/50, Batch 4400/7628\n","Epoch 1/50, Batch 4500/7628\n","Epoch 1/50, Batch 4600/7628\n","Epoch 1/50, Batch 4700/7628\n","Epoch 1/50, Batch 4800/7628\n","Epoch 1/50, Batch 4900/7628\n","Epoch 1/50, Batch 5000/7628\n","Epoch 1/50, Batch 5100/7628\n","Epoch 1/50, Batch 5200/7628\n","Epoch 1/50, Batch 5300/7628\n","Epoch 1/50, Batch 5400/7628\n","Epoch 1/50, Batch 5500/7628\n","Epoch 1/50, Batch 5600/7628\n","Epoch 1/50, Batch 5700/7628\n","Epoch 1/50, Batch 5800/7628\n","Epoch 1/50, Batch 5900/7628\n","Epoch 1/50, Batch 6000/7628\n","Epoch 1/50, Batch 6100/7628\n","Epoch 1/50, Batch 6200/7628\n","Epoch 1/50, Batch 6300/7628\n","Epoch 1/50, Batch 6400/7628\n","Epoch 1/50, Batch 6500/7628\n","Epoch 1/50, Batch 6600/7628\n","Epoch 1/50, Batch 6700/7628\n","Epoch 1/50, Batch 6800/7628\n","Epoch 1/50, Batch 6900/7628\n","Epoch 1/50, Batch 7000/7628\n","Epoch 1/50, Batch 7100/7628\n","Epoch 1/50, Batch 7200/7628\n","Epoch 1/50, Batch 7300/7628\n","Epoch 1/50, Batch 7400/7628\n","Epoch 1/50, Batch 7500/7628\n","Epoch 1/50, Batch 7600/7628\n","Epoch 1/50, Loss: 2.4632\n","Best model saved with loss: 2.4632\n","Epoch 2/50, Batch 0/7628\n","Epoch 2/50, Batch 100/7628\n","Epoch 2/50, Batch 200/7628\n","Epoch 2/50, Batch 300/7628\n","Epoch 2/50, Batch 400/7628\n","Epoch 2/50, Batch 500/7628\n","Epoch 2/50, Batch 600/7628\n","Epoch 2/50, Batch 700/7628\n","Epoch 2/50, Batch 800/7628\n","Epoch 2/50, Batch 900/7628\n","Epoch 2/50, Batch 1000/7628\n","Epoch 2/50, Batch 1100/7628\n","Epoch 2/50, Batch 1200/7628\n","Epoch 2/50, Batch 1300/7628\n","Epoch 2/50, Batch 1400/7628\n","Epoch 2/50, Batch 1500/7628\n","Epoch 2/50, Batch 1600/7628\n","Epoch 2/50, Batch 1700/7628\n","Epoch 2/50, Batch 1800/7628\n","Epoch 2/50, Batch 1900/7628\n","Epoch 2/50, Batch 2000/7628\n","Epoch 2/50, Batch 2100/7628\n","Epoch 2/50, Batch 2200/7628\n","Epoch 2/50, Batch 2300/7628\n","Epoch 2/50, Batch 2400/7628\n","Epoch 2/50, Batch 2500/7628\n","Epoch 2/50, Batch 2600/7628\n","Epoch 2/50, Batch 2700/7628\n","Epoch 2/50, Batch 2800/7628\n","Epoch 2/50, Batch 2900/7628\n","Epoch 2/50, Batch 3000/7628\n","Epoch 2/50, Batch 3100/7628\n","Epoch 2/50, Batch 3200/7628\n","Epoch 2/50, Batch 3300/7628\n","Epoch 2/50, Batch 3400/7628\n","Epoch 2/50, Batch 3500/7628\n","Epoch 2/50, Batch 3600/7628\n","Epoch 2/50, Batch 3700/7628\n","Epoch 2/50, Batch 3800/7628\n","Epoch 2/50, Batch 3900/7628\n","Epoch 2/50, Batch 4000/7628\n","Epoch 2/50, Batch 4100/7628\n","Epoch 2/50, Batch 4200/7628\n","Epoch 2/50, Batch 4300/7628\n","Epoch 2/50, Batch 4400/7628\n","Epoch 2/50, Batch 4500/7628\n","Epoch 2/50, Batch 4600/7628\n","Epoch 2/50, Batch 4700/7628\n","Epoch 2/50, Batch 4800/7628\n","Epoch 2/50, Batch 4900/7628\n","Epoch 2/50, Batch 5000/7628\n","Epoch 2/50, Batch 5100/7628\n","Epoch 2/50, Batch 5200/7628\n","Epoch 2/50, Batch 5300/7628\n","Epoch 2/50, Batch 5400/7628\n","Epoch 2/50, Batch 5500/7628\n","Epoch 2/50, Batch 5600/7628\n","Epoch 2/50, Batch 5700/7628\n","Epoch 2/50, Batch 5800/7628\n","Epoch 2/50, Batch 5900/7628\n","Epoch 2/50, Batch 6000/7628\n","Epoch 2/50, Batch 6100/7628\n","Epoch 2/50, Batch 6200/7628\n","Epoch 2/50, Batch 6300/7628\n","Epoch 2/50, Batch 6400/7628\n","Epoch 2/50, Batch 6500/7628\n","Epoch 2/50, Batch 6600/7628\n","Epoch 2/50, Batch 6700/7628\n","Epoch 2/50, Batch 6800/7628\n","Epoch 2/50, Batch 6900/7628\n","Epoch 2/50, Batch 7000/7628\n","Epoch 2/50, Batch 7100/7628\n","Epoch 2/50, Batch 7200/7628\n","Epoch 2/50, Batch 7300/7628\n","Epoch 2/50, Batch 7400/7628\n","Epoch 2/50, Batch 7500/7628\n","Epoch 2/50, Batch 7600/7628\n","Epoch 2/50, Loss: 0.4656\n","Best model saved with loss: 0.4656\n","Epoch 3/50, Batch 0/7628\n","Epoch 3/50, Batch 100/7628\n","Epoch 3/50, Batch 200/7628\n","Epoch 3/50, Batch 300/7628\n","Epoch 3/50, Batch 400/7628\n","Epoch 3/50, Batch 500/7628\n","Epoch 3/50, Batch 600/7628\n","Epoch 3/50, Batch 700/7628\n","Epoch 3/50, Batch 800/7628\n","Epoch 3/50, Batch 900/7628\n","Epoch 3/50, Batch 1000/7628\n","Epoch 3/50, Batch 1100/7628\n","Epoch 3/50, Batch 1200/7628\n","Epoch 3/50, Batch 1300/7628\n","Epoch 3/50, Batch 1400/7628\n","Epoch 3/50, Batch 1500/7628\n","Epoch 3/50, Batch 1600/7628\n","Epoch 3/50, Batch 1700/7628\n","Epoch 3/50, Batch 1800/7628\n","Epoch 3/50, Batch 1900/7628\n","Epoch 3/50, Batch 2000/7628\n","Epoch 3/50, Batch 2100/7628\n","Epoch 3/50, Batch 2200/7628\n","Epoch 3/50, Batch 2300/7628\n","Epoch 3/50, Batch 2400/7628\n","Epoch 3/50, Batch 2500/7628\n","Epoch 3/50, Batch 2600/7628\n","Epoch 3/50, Batch 2700/7628\n","Epoch 3/50, Batch 2800/7628\n","Epoch 3/50, Batch 2900/7628\n","Epoch 3/50, Batch 3000/7628\n","Epoch 3/50, Batch 3100/7628\n","Epoch 3/50, Batch 3200/7628\n","Epoch 3/50, Batch 3300/7628\n","Epoch 3/50, Batch 3400/7628\n","Epoch 3/50, Batch 3500/7628\n","Epoch 3/50, Batch 3600/7628\n","Epoch 3/50, Batch 3700/7628\n","Epoch 3/50, Batch 3800/7628\n","Epoch 3/50, Batch 3900/7628\n","Epoch 3/50, Batch 4000/7628\n","Epoch 3/50, Batch 4100/7628\n","Epoch 3/50, Batch 4200/7628\n","Epoch 3/50, Batch 4300/7628\n","Epoch 3/50, Batch 4400/7628\n","Epoch 3/50, Batch 4500/7628\n","Epoch 3/50, Batch 4600/7628\n","Epoch 3/50, Batch 4700/7628\n","Epoch 3/50, Batch 4800/7628\n","Epoch 3/50, Batch 4900/7628\n","Epoch 3/50, Batch 5000/7628\n","Epoch 3/50, Batch 5100/7628\n","Epoch 3/50, Batch 5200/7628\n","Epoch 3/50, Batch 5300/7628\n","Epoch 3/50, Batch 5400/7628\n","Epoch 3/50, Batch 5500/7628\n","Epoch 3/50, Batch 5600/7628\n","Epoch 3/50, Batch 5700/7628\n","Epoch 3/50, Batch 5800/7628\n","Epoch 3/50, Batch 5900/7628\n","Epoch 3/50, Batch 6000/7628\n","Epoch 3/50, Batch 6100/7628\n","Epoch 3/50, Batch 6200/7628\n","Epoch 3/50, Batch 6300/7628\n","Epoch 3/50, Batch 6400/7628\n","Epoch 3/50, Batch 6500/7628\n","Epoch 3/50, Batch 6600/7628\n","Epoch 3/50, Batch 6700/7628\n","Epoch 3/50, Batch 6800/7628\n","Epoch 3/50, Batch 6900/7628\n","Epoch 3/50, Batch 7000/7628\n","Epoch 3/50, Batch 7100/7628\n","Epoch 3/50, Batch 7200/7628\n","Epoch 3/50, Batch 7300/7628\n","Epoch 3/50, Batch 7400/7628\n","Epoch 3/50, Batch 7500/7628\n","Epoch 3/50, Batch 7600/7628\n","Epoch 3/50, Loss: 0.4430\n","Best model saved with loss: 0.4430\n","Epoch 4/50, Batch 0/7628\n","Epoch 4/50, Batch 100/7628\n","Epoch 4/50, Batch 200/7628\n","Epoch 4/50, Batch 300/7628\n","Epoch 4/50, Batch 400/7628\n","Epoch 4/50, Batch 500/7628\n","Epoch 4/50, Batch 600/7628\n","Epoch 4/50, Batch 700/7628\n","Epoch 4/50, Batch 800/7628\n","Epoch 4/50, Batch 900/7628\n","Epoch 4/50, Batch 1000/7628\n","Epoch 4/50, Batch 1100/7628\n","Epoch 4/50, Batch 1200/7628\n","Epoch 4/50, Batch 1300/7628\n","Epoch 4/50, Batch 1400/7628\n","Epoch 4/50, Batch 1500/7628\n","Epoch 4/50, Batch 1600/7628\n","Epoch 4/50, Batch 1700/7628\n","Epoch 4/50, Batch 1800/7628\n","Epoch 4/50, Batch 1900/7628\n","Epoch 4/50, Batch 2000/7628\n","Epoch 4/50, Batch 2100/7628\n","Epoch 4/50, Batch 2200/7628\n","Epoch 4/50, Batch 2300/7628\n","Epoch 4/50, Batch 2400/7628\n","Epoch 4/50, Batch 2500/7628\n","Epoch 4/50, Batch 2600/7628\n","Epoch 4/50, Batch 2700/7628\n","Epoch 4/50, Batch 2800/7628\n","Epoch 4/50, Batch 2900/7628\n","Epoch 4/50, Batch 3000/7628\n","Epoch 4/50, Batch 3100/7628\n","Epoch 4/50, Batch 3200/7628\n","Epoch 4/50, Batch 3300/7628\n","Epoch 4/50, Batch 3400/7628\n","Epoch 4/50, Batch 3500/7628\n","Epoch 4/50, Batch 3600/7628\n","Epoch 4/50, Batch 3700/7628\n","Epoch 4/50, Batch 3800/7628\n","Epoch 4/50, Batch 3900/7628\n","Epoch 4/50, Batch 4000/7628\n","Epoch 4/50, Batch 4100/7628\n","Epoch 4/50, Batch 4200/7628\n","Epoch 4/50, Batch 4300/7628\n","Epoch 4/50, Batch 4400/7628\n","Epoch 4/50, Batch 4500/7628\n","Epoch 4/50, Batch 4600/7628\n","Epoch 4/50, Batch 4700/7628\n","Epoch 4/50, Batch 4800/7628\n","Epoch 4/50, Batch 4900/7628\n","Epoch 4/50, Batch 5000/7628\n","Epoch 4/50, Batch 5100/7628\n","Epoch 4/50, Batch 5200/7628\n","Epoch 4/50, Batch 5300/7628\n","Epoch 4/50, Batch 5400/7628\n","Epoch 4/50, Batch 5500/7628\n","Epoch 4/50, Batch 5600/7628\n","Epoch 4/50, Batch 5700/7628\n","Epoch 4/50, Batch 5800/7628\n","Epoch 4/50, Batch 5900/7628\n","Epoch 4/50, Batch 6000/7628\n","Epoch 4/50, Batch 6100/7628\n","Epoch 4/50, Batch 6200/7628\n","Epoch 4/50, Batch 6300/7628\n","Epoch 4/50, Batch 6400/7628\n","Epoch 4/50, Batch 6500/7628\n","Epoch 4/50, Batch 6600/7628\n","Epoch 4/50, Batch 6700/7628\n","Epoch 4/50, Batch 6800/7628\n","Epoch 4/50, Batch 6900/7628\n","Epoch 4/50, Batch 7000/7628\n","Epoch 4/50, Batch 7100/7628\n","Epoch 4/50, Batch 7200/7628\n","Epoch 4/50, Batch 7300/7628\n","Epoch 4/50, Batch 7400/7628\n","Epoch 4/50, Batch 7500/7628\n","Epoch 4/50, Batch 7600/7628\n","Epoch 4/50, Loss: 0.4189\n","Best model saved with loss: 0.4189\n","Epoch 5/50, Batch 0/7628\n","Epoch 5/50, Batch 100/7628\n","Epoch 5/50, Batch 200/7628\n","Epoch 5/50, Batch 300/7628\n","Epoch 5/50, Batch 400/7628\n","Epoch 5/50, Batch 500/7628\n","Epoch 5/50, Batch 600/7628\n","Epoch 5/50, Batch 700/7628\n","Epoch 5/50, Batch 800/7628\n","Epoch 5/50, Batch 900/7628\n","Epoch 5/50, Batch 1000/7628\n","Epoch 5/50, Batch 1100/7628\n","Epoch 5/50, Batch 1200/7628\n","Epoch 5/50, Batch 1300/7628\n","Epoch 5/50, Batch 1400/7628\n","Epoch 5/50, Batch 1500/7628\n","Epoch 5/50, Batch 1600/7628\n","Epoch 5/50, Batch 1700/7628\n","Epoch 5/50, Batch 1800/7628\n","Epoch 5/50, Batch 1900/7628\n","Epoch 5/50, Batch 2000/7628\n","Epoch 5/50, Batch 2100/7628\n","Epoch 5/50, Batch 2200/7628\n","Epoch 5/50, Batch 2300/7628\n","Epoch 5/50, Batch 2400/7628\n","Epoch 5/50, Batch 2500/7628\n","Epoch 5/50, Batch 2600/7628\n","Epoch 5/50, Batch 2700/7628\n","Epoch 5/50, Batch 2800/7628\n","Epoch 5/50, Batch 2900/7628\n","Epoch 5/50, Batch 3000/7628\n","Epoch 5/50, Batch 3100/7628\n","Epoch 5/50, Batch 3200/7628\n","Epoch 5/50, Batch 3300/7628\n","Epoch 5/50, Batch 3400/7628\n","Epoch 5/50, Batch 3500/7628\n","Epoch 5/50, Batch 3600/7628\n","Epoch 5/50, Batch 3700/7628\n","Epoch 5/50, Batch 3800/7628\n","Epoch 5/50, Batch 3900/7628\n","Epoch 5/50, Batch 4000/7628\n","Epoch 5/50, Batch 4100/7628\n","Epoch 5/50, Batch 4200/7628\n","Epoch 5/50, Batch 4300/7628\n","Epoch 5/50, Batch 4400/7628\n","Epoch 5/50, Batch 4500/7628\n","Epoch 5/50, Batch 4600/7628\n","Epoch 5/50, Batch 4700/7628\n","Epoch 5/50, Batch 4800/7628\n","Epoch 5/50, Batch 4900/7628\n","Epoch 5/50, Batch 5000/7628\n","Epoch 5/50, Batch 5100/7628\n","Epoch 5/50, Batch 5200/7628\n","Epoch 5/50, Batch 5300/7628\n","Epoch 5/50, Batch 5400/7628\n","Epoch 5/50, Batch 5500/7628\n","Epoch 5/50, Batch 5600/7628\n","Epoch 5/50, Batch 5700/7628\n","Epoch 5/50, Batch 5800/7628\n","Epoch 5/50, Batch 5900/7628\n","Epoch 5/50, Batch 6000/7628\n","Epoch 5/50, Batch 6100/7628\n","Epoch 5/50, Batch 6200/7628\n","Epoch 5/50, Batch 6300/7628\n","Epoch 5/50, Batch 6400/7628\n","Epoch 5/50, Batch 6500/7628\n","Epoch 5/50, Batch 6600/7628\n","Epoch 5/50, Batch 6700/7628\n","Epoch 5/50, Batch 6800/7628\n","Epoch 5/50, Batch 6900/7628\n","Epoch 5/50, Batch 7000/7628\n","Epoch 5/50, Batch 7100/7628\n","Epoch 5/50, Batch 7200/7628\n","Epoch 5/50, Batch 7300/7628\n","Epoch 5/50, Batch 7400/7628\n","Epoch 5/50, Batch 7500/7628\n","Epoch 5/50, Batch 7600/7628\n","Epoch 5/50, Loss: 0.2821\n","Best model saved with loss: 0.2821\n","Epoch 6/50, Batch 0/7628\n","Epoch 6/50, Batch 100/7628\n","Epoch 6/50, Batch 200/7628\n","Epoch 6/50, Batch 300/7628\n","Epoch 6/50, Batch 400/7628\n","Epoch 6/50, Batch 500/7628\n","Epoch 6/50, Batch 600/7628\n","Epoch 6/50, Batch 700/7628\n","Epoch 6/50, Batch 800/7628\n","Epoch 6/50, Batch 900/7628\n","Epoch 6/50, Batch 1000/7628\n","Epoch 6/50, Batch 1100/7628\n","Epoch 6/50, Batch 1200/7628\n","Epoch 6/50, Batch 1300/7628\n","Epoch 6/50, Batch 1400/7628\n","Epoch 6/50, Batch 1500/7628\n","Epoch 6/50, Batch 1600/7628\n","Epoch 6/50, Batch 1700/7628\n","Epoch 6/50, Batch 1800/7628\n","Epoch 6/50, Batch 1900/7628\n","Epoch 6/50, Batch 2000/7628\n","Epoch 6/50, Batch 2100/7628\n","Epoch 6/50, Batch 2200/7628\n","Epoch 6/50, Batch 2300/7628\n","Epoch 6/50, Batch 2400/7628\n","Epoch 6/50, Batch 2500/7628\n","Epoch 6/50, Batch 2600/7628\n","Epoch 6/50, Batch 2700/7628\n","Epoch 6/50, Batch 2800/7628\n","Epoch 6/50, Batch 2900/7628\n","Epoch 6/50, Batch 3000/7628\n","Epoch 6/50, Batch 3100/7628\n","Epoch 6/50, Batch 3200/7628\n","Epoch 6/50, Batch 3300/7628\n","Epoch 6/50, Batch 3400/7628\n","Epoch 6/50, Batch 3500/7628\n","Epoch 6/50, Batch 3600/7628\n","Epoch 6/50, Batch 3700/7628\n","Epoch 6/50, Batch 3800/7628\n","Epoch 6/50, Batch 3900/7628\n","Epoch 6/50, Batch 4000/7628\n","Epoch 6/50, Batch 4100/7628\n","Epoch 6/50, Batch 4200/7628\n","Epoch 6/50, Batch 4300/7628\n","Epoch 6/50, Batch 4400/7628\n","Epoch 6/50, Batch 4500/7628\n","Epoch 6/50, Batch 4600/7628\n","Epoch 6/50, Batch 4700/7628\n","Epoch 6/50, Batch 4800/7628\n","Epoch 6/50, Batch 4900/7628\n","Epoch 6/50, Batch 5000/7628\n","Epoch 6/50, Batch 5100/7628\n","Epoch 6/50, Batch 5200/7628\n","Epoch 6/50, Batch 5300/7628\n","Epoch 6/50, Batch 5400/7628\n","Epoch 6/50, Batch 5500/7628\n","Epoch 6/50, Batch 5600/7628\n","Epoch 6/50, Batch 5700/7628\n","Epoch 6/50, Batch 5800/7628\n","Epoch 6/50, Batch 5900/7628\n","Epoch 6/50, Batch 6000/7628\n","Epoch 6/50, Batch 6100/7628\n","Epoch 6/50, Batch 6200/7628\n","Epoch 6/50, Batch 6300/7628\n","Epoch 6/50, Batch 6400/7628\n","Epoch 6/50, Batch 6500/7628\n","Epoch 6/50, Batch 6600/7628\n","Epoch 6/50, Batch 6700/7628\n","Epoch 6/50, Batch 6800/7628\n","Epoch 6/50, Batch 6900/7628\n","Epoch 6/50, Batch 7000/7628\n","Epoch 6/50, Batch 7100/7628\n","Epoch 6/50, Batch 7200/7628\n","Epoch 6/50, Batch 7300/7628\n","Epoch 6/50, Batch 7400/7628\n","Epoch 6/50, Batch 7500/7628\n","Epoch 6/50, Batch 7600/7628\n","Epoch 6/50, Loss: 0.3028\n","No improvement for 1 epoch(s)\n","Epoch 7/50, Batch 0/7628\n","Epoch 7/50, Batch 100/7628\n","Epoch 7/50, Batch 200/7628\n","Epoch 7/50, Batch 300/7628\n","Epoch 7/50, Batch 400/7628\n","Epoch 7/50, Batch 500/7628\n","Epoch 7/50, Batch 600/7628\n","Epoch 7/50, Batch 700/7628\n","Epoch 7/50, Batch 800/7628\n","Epoch 7/50, Batch 900/7628\n","Epoch 7/50, Batch 1000/7628\n","Epoch 7/50, Batch 1100/7628\n","Epoch 7/50, Batch 1200/7628\n","Epoch 7/50, Batch 1300/7628\n","Epoch 7/50, Batch 1400/7628\n","Epoch 7/50, Batch 1500/7628\n","Epoch 7/50, Batch 1600/7628\n","Epoch 7/50, Batch 1700/7628\n","Epoch 7/50, Batch 1800/7628\n","Epoch 7/50, Batch 1900/7628\n","Epoch 7/50, Batch 2000/7628\n","Epoch 7/50, Batch 2100/7628\n","Epoch 7/50, Batch 2200/7628\n","Epoch 7/50, Batch 2300/7628\n","Epoch 7/50, Batch 2400/7628\n","Epoch 7/50, Batch 2500/7628\n","Epoch 7/50, Batch 2600/7628\n","Epoch 7/50, Batch 2700/7628\n","Epoch 7/50, Batch 2800/7628\n","Epoch 7/50, Batch 2900/7628\n","Epoch 7/50, Batch 3000/7628\n","Epoch 7/50, Batch 3100/7628\n","Epoch 7/50, Batch 3200/7628\n","Epoch 7/50, Batch 3300/7628\n","Epoch 7/50, Batch 3400/7628\n","Epoch 7/50, Batch 3500/7628\n","Epoch 7/50, Batch 3600/7628\n","Epoch 7/50, Batch 3700/7628\n","Epoch 7/50, Batch 3800/7628\n","Epoch 7/50, Batch 3900/7628\n","Epoch 7/50, Batch 4000/7628\n","Epoch 7/50, Batch 4100/7628\n","Epoch 7/50, Batch 4200/7628\n","Epoch 7/50, Batch 4300/7628\n","Epoch 7/50, Batch 4400/7628\n","Epoch 7/50, Batch 4500/7628\n","Epoch 7/50, Batch 4600/7628\n","Epoch 7/50, Batch 4700/7628\n","Epoch 7/50, Batch 4800/7628\n","Epoch 7/50, Batch 4900/7628\n","Epoch 7/50, Batch 5000/7628\n","Epoch 7/50, Batch 5100/7628\n","Epoch 7/50, Batch 5200/7628\n","Epoch 7/50, Batch 5300/7628\n","Epoch 7/50, Batch 5400/7628\n","Epoch 7/50, Batch 5500/7628\n","Epoch 7/50, Batch 5600/7628\n","Epoch 7/50, Batch 5700/7628\n","Epoch 7/50, Batch 5800/7628\n","Epoch 7/50, Batch 5900/7628\n","Epoch 7/50, Batch 6000/7628\n","Epoch 7/50, Batch 6100/7628\n","Epoch 7/50, Batch 6200/7628\n","Epoch 7/50, Batch 6300/7628\n","Epoch 7/50, Batch 6400/7628\n","Epoch 7/50, Batch 6500/7628\n","Epoch 7/50, Batch 6600/7628\n","Epoch 7/50, Batch 6700/7628\n","Epoch 7/50, Batch 6800/7628\n","Epoch 7/50, Batch 6900/7628\n","Epoch 7/50, Batch 7000/7628\n","Epoch 7/50, Batch 7100/7628\n","Epoch 7/50, Batch 7200/7628\n","Epoch 7/50, Batch 7300/7628\n","Epoch 7/50, Batch 7400/7628\n","Epoch 7/50, Batch 7500/7628\n","Epoch 7/50, Batch 7600/7628\n","Epoch 7/50, Loss: 0.3355\n","No improvement for 2 epoch(s)\n","Epoch 8/50, Batch 0/7628\n","Epoch 8/50, Batch 100/7628\n","Epoch 8/50, Batch 200/7628\n","Epoch 8/50, Batch 300/7628\n","Epoch 8/50, Batch 400/7628\n","Epoch 8/50, Batch 500/7628\n","Epoch 8/50, Batch 600/7628\n","Epoch 8/50, Batch 700/7628\n","Epoch 8/50, Batch 800/7628\n","Epoch 8/50, Batch 900/7628\n","Epoch 8/50, Batch 1000/7628\n","Epoch 8/50, Batch 1100/7628\n","Epoch 8/50, Batch 1200/7628\n","Epoch 8/50, Batch 1300/7628\n","Epoch 8/50, Batch 1400/7628\n","Epoch 8/50, Batch 1500/7628\n","Epoch 8/50, Batch 1600/7628\n","Epoch 8/50, Batch 1700/7628\n","Epoch 8/50, Batch 1800/7628\n","Epoch 8/50, Batch 1900/7628\n","Epoch 8/50, Batch 2000/7628\n","Epoch 8/50, Batch 2100/7628\n","Epoch 8/50, Batch 2200/7628\n","Epoch 8/50, Batch 2300/7628\n","Epoch 8/50, Batch 2400/7628\n","Epoch 8/50, Batch 2500/7628\n","Epoch 8/50, Batch 2600/7628\n","Epoch 8/50, Batch 2700/7628\n","Epoch 8/50, Batch 2800/7628\n","Epoch 8/50, Batch 2900/7628\n","Epoch 8/50, Batch 3000/7628\n","Epoch 8/50, Batch 3100/7628\n","Epoch 8/50, Batch 3200/7628\n","Epoch 8/50, Batch 3300/7628\n","Epoch 8/50, Batch 3400/7628\n","Epoch 8/50, Batch 3500/7628\n","Epoch 8/50, Batch 3600/7628\n","Epoch 8/50, Batch 3700/7628\n","Epoch 8/50, Batch 3800/7628\n","Epoch 8/50, Batch 3900/7628\n","Epoch 8/50, Batch 4000/7628\n","Epoch 8/50, Batch 4100/7628\n","Epoch 8/50, Batch 4200/7628\n","Epoch 8/50, Batch 4300/7628\n","Epoch 8/50, Batch 4400/7628\n","Epoch 8/50, Batch 4500/7628\n","Epoch 8/50, Batch 4600/7628\n","Epoch 8/50, Batch 4700/7628\n","Epoch 8/50, Batch 4800/7628\n","Epoch 8/50, Batch 4900/7628\n","Epoch 8/50, Batch 5000/7628\n","Epoch 8/50, Batch 5100/7628\n","Epoch 8/50, Batch 5200/7628\n","Epoch 8/50, Batch 5300/7628\n","Epoch 8/50, Batch 5400/7628\n","Epoch 8/50, Batch 5500/7628\n","Epoch 8/50, Batch 5600/7628\n","Epoch 8/50, Batch 5700/7628\n","Epoch 8/50, Batch 5800/7628\n","Epoch 8/50, Batch 5900/7628\n","Epoch 8/50, Batch 6000/7628\n","Epoch 8/50, Batch 6100/7628\n","Epoch 8/50, Batch 6200/7628\n","Epoch 8/50, Batch 6300/7628\n","Epoch 8/50, Batch 6400/7628\n","Epoch 8/50, Batch 6500/7628\n","Epoch 8/50, Batch 6600/7628\n","Epoch 8/50, Batch 6700/7628\n","Epoch 8/50, Batch 6800/7628\n","Epoch 8/50, Batch 6900/7628\n","Epoch 8/50, Batch 7000/7628\n","Epoch 8/50, Batch 7100/7628\n","Epoch 8/50, Batch 7200/7628\n","Epoch 8/50, Batch 7300/7628\n","Epoch 8/50, Batch 7400/7628\n","Epoch 8/50, Batch 7500/7628\n","Epoch 8/50, Batch 7600/7628\n","Epoch 8/50, Loss: 0.2839\n","No improvement for 3 epoch(s)\n","Epoch 9/50, Batch 0/7628\n","Epoch 9/50, Batch 100/7628\n","Epoch 9/50, Batch 200/7628\n","Epoch 9/50, Batch 300/7628\n","Epoch 9/50, Batch 400/7628\n","Epoch 9/50, Batch 500/7628\n","Epoch 9/50, Batch 600/7628\n","Epoch 9/50, Batch 700/7628\n","Epoch 9/50, Batch 800/7628\n","Epoch 9/50, Batch 900/7628\n","Epoch 9/50, Batch 1000/7628\n","Epoch 9/50, Batch 1100/7628\n","Epoch 9/50, Batch 1200/7628\n","Epoch 9/50, Batch 1300/7628\n","Epoch 9/50, Batch 1400/7628\n","Epoch 9/50, Batch 1500/7628\n","Epoch 9/50, Batch 1600/7628\n","Epoch 9/50, Batch 1700/7628\n","Epoch 9/50, Batch 1800/7628\n","Epoch 9/50, Batch 1900/7628\n","Epoch 9/50, Batch 2000/7628\n","Epoch 9/50, Batch 2100/7628\n","Epoch 9/50, Batch 2200/7628\n","Epoch 9/50, Batch 2300/7628\n","Epoch 9/50, Batch 2400/7628\n","Epoch 9/50, Batch 2500/7628\n","Epoch 9/50, Batch 2600/7628\n","Epoch 9/50, Batch 2700/7628\n","Epoch 9/50, Batch 2800/7628\n","Epoch 9/50, Batch 2900/7628\n","Epoch 9/50, Batch 3000/7628\n","Epoch 9/50, Batch 3100/7628\n","Epoch 9/50, Batch 3200/7628\n","Epoch 9/50, Batch 3300/7628\n","Epoch 9/50, Batch 3400/7628\n","Epoch 9/50, Batch 3500/7628\n","Epoch 9/50, Batch 3600/7628\n","Epoch 9/50, Batch 3700/7628\n","Epoch 9/50, Batch 3800/7628\n","Epoch 9/50, Batch 3900/7628\n","Epoch 9/50, Batch 4000/7628\n","Epoch 9/50, Batch 4100/7628\n","Epoch 9/50, Batch 4200/7628\n","Epoch 9/50, Batch 4300/7628\n","Epoch 9/50, Batch 4400/7628\n","Epoch 9/50, Batch 4500/7628\n","Epoch 9/50, Batch 4600/7628\n","Epoch 9/50, Batch 4700/7628\n","Epoch 9/50, Batch 4800/7628\n","Epoch 9/50, Batch 4900/7628\n","Epoch 9/50, Batch 5000/7628\n","Epoch 9/50, Batch 5100/7628\n","Epoch 9/50, Batch 5200/7628\n","Epoch 9/50, Batch 5300/7628\n","Epoch 9/50, Batch 5400/7628\n","Epoch 9/50, Batch 5500/7628\n","Epoch 9/50, Batch 5600/7628\n","Epoch 9/50, Batch 5700/7628\n","Epoch 9/50, Batch 5800/7628\n","Epoch 9/50, Batch 5900/7628\n","Epoch 9/50, Batch 6000/7628\n","Epoch 9/50, Batch 6100/7628\n","Epoch 9/50, Batch 6200/7628\n","Epoch 9/50, Batch 6300/7628\n","Epoch 9/50, Batch 6400/7628\n","Epoch 9/50, Batch 6500/7628\n","Epoch 9/50, Batch 6600/7628\n","Epoch 9/50, Batch 6700/7628\n","Epoch 9/50, Batch 6800/7628\n","Epoch 9/50, Batch 6900/7628\n","Epoch 9/50, Batch 7000/7628\n","Epoch 9/50, Batch 7100/7628\n","Epoch 9/50, Batch 7200/7628\n","Epoch 9/50, Batch 7300/7628\n","Epoch 9/50, Batch 7400/7628\n","Epoch 9/50, Batch 7500/7628\n","Epoch 9/50, Batch 7600/7628\n","Epoch 9/50, Loss: 0.2763\n","Best model saved with loss: 0.2763\n","Epoch 10/50, Batch 0/7628\n","Epoch 10/50, Batch 100/7628\n","Epoch 10/50, Batch 200/7628\n","Epoch 10/50, Batch 300/7628\n","Epoch 10/50, Batch 400/7628\n","Epoch 10/50, Batch 500/7628\n","Epoch 10/50, Batch 600/7628\n","Epoch 10/50, Batch 700/7628\n","Epoch 10/50, Batch 800/7628\n","Epoch 10/50, Batch 900/7628\n","Epoch 10/50, Batch 1000/7628\n","Epoch 10/50, Batch 1100/7628\n","Epoch 10/50, Batch 1200/7628\n","Epoch 10/50, Batch 1300/7628\n","Epoch 10/50, Batch 1400/7628\n","Epoch 10/50, Batch 1500/7628\n","Epoch 10/50, Batch 1600/7628\n","Epoch 10/50, Batch 1700/7628\n","Epoch 10/50, Batch 1800/7628\n","Epoch 10/50, Batch 1900/7628\n","Epoch 10/50, Batch 2000/7628\n","Epoch 10/50, Batch 2100/7628\n","Epoch 10/50, Batch 2200/7628\n","Epoch 10/50, Batch 2300/7628\n","Epoch 10/50, Batch 2400/7628\n","Epoch 10/50, Batch 2500/7628\n","Epoch 10/50, Batch 2600/7628\n","Epoch 10/50, Batch 2700/7628\n","Epoch 10/50, Batch 2800/7628\n","Epoch 10/50, Batch 2900/7628\n","Epoch 10/50, Batch 3000/7628\n","Epoch 10/50, Batch 3100/7628\n","Epoch 10/50, Batch 3200/7628\n","Epoch 10/50, Batch 3300/7628\n","Epoch 10/50, Batch 3400/7628\n","Epoch 10/50, Batch 3500/7628\n","Epoch 10/50, Batch 3600/7628\n","Epoch 10/50, Batch 3700/7628\n","Epoch 10/50, Batch 3800/7628\n","Epoch 10/50, Batch 3900/7628\n","Epoch 10/50, Batch 4000/7628\n","Epoch 10/50, Batch 4100/7628\n","Epoch 10/50, Batch 4200/7628\n","Epoch 10/50, Batch 4300/7628\n","Epoch 10/50, Batch 4400/7628\n","Epoch 10/50, Batch 4500/7628\n","Epoch 10/50, Batch 4600/7628\n","Epoch 10/50, Batch 4700/7628\n","Epoch 10/50, Batch 4800/7628\n","Epoch 10/50, Batch 4900/7628\n","Epoch 10/50, Batch 5000/7628\n","Epoch 10/50, Batch 5100/7628\n","Epoch 10/50, Batch 5200/7628\n","Epoch 10/50, Batch 5300/7628\n","Epoch 10/50, Batch 5400/7628\n","Epoch 10/50, Batch 5500/7628\n","Epoch 10/50, Batch 5600/7628\n","Epoch 10/50, Batch 5700/7628\n","Epoch 10/50, Batch 5800/7628\n","Epoch 10/50, Batch 5900/7628\n","Epoch 10/50, Batch 6000/7628\n","Epoch 10/50, Batch 6100/7628\n","Epoch 10/50, Batch 6200/7628\n","Epoch 10/50, Batch 6300/7628\n","Epoch 10/50, Batch 6400/7628\n","Epoch 10/50, Batch 6500/7628\n","Epoch 10/50, Batch 6600/7628\n","Epoch 10/50, Batch 6700/7628\n","Epoch 10/50, Batch 6800/7628\n","Epoch 10/50, Batch 6900/7628\n","Epoch 10/50, Batch 7000/7628\n","Epoch 10/50, Batch 7100/7628\n","Epoch 10/50, Batch 7200/7628\n","Epoch 10/50, Batch 7300/7628\n","Epoch 10/50, Batch 7400/7628\n","Epoch 10/50, Batch 7500/7628\n","Epoch 10/50, Batch 7600/7628\n","Epoch 10/50, Loss: 0.2799\n","No improvement for 1 epoch(s)\n","Epoch 11/50, Batch 0/7628\n","Epoch 11/50, Batch 100/7628\n","Epoch 11/50, Batch 200/7628\n","Epoch 11/50, Batch 300/7628\n","Epoch 11/50, Batch 400/7628\n","Epoch 11/50, Batch 500/7628\n","Epoch 11/50, Batch 600/7628\n","Epoch 11/50, Batch 700/7628\n","Epoch 11/50, Batch 800/7628\n","Epoch 11/50, Batch 900/7628\n","Epoch 11/50, Batch 1000/7628\n","Epoch 11/50, Batch 1100/7628\n","Epoch 11/50, Batch 1200/7628\n","Epoch 11/50, Batch 1300/7628\n","Epoch 11/50, Batch 1400/7628\n","Epoch 11/50, Batch 1500/7628\n","Epoch 11/50, Batch 1600/7628\n","Epoch 11/50, Batch 1700/7628\n","Epoch 11/50, Batch 1800/7628\n","Epoch 11/50, Batch 1900/7628\n","Epoch 11/50, Batch 2000/7628\n","Epoch 11/50, Batch 2100/7628\n","Epoch 11/50, Batch 2200/7628\n","Epoch 11/50, Batch 2300/7628\n","Epoch 11/50, Batch 2400/7628\n","Epoch 11/50, Batch 2500/7628\n","Epoch 11/50, Batch 2600/7628\n","Epoch 11/50, Batch 2700/7628\n","Epoch 11/50, Batch 2800/7628\n","Epoch 11/50, Batch 2900/7628\n","Epoch 11/50, Batch 3000/7628\n","Epoch 11/50, Batch 3100/7628\n","Epoch 11/50, Batch 3200/7628\n","Epoch 11/50, Batch 3300/7628\n","Epoch 11/50, Batch 3400/7628\n","Epoch 11/50, Batch 3500/7628\n","Epoch 11/50, Batch 3600/7628\n","Epoch 11/50, Batch 3700/7628\n","Epoch 11/50, Batch 3800/7628\n","Epoch 11/50, Batch 3900/7628\n","Epoch 11/50, Batch 4000/7628\n","Epoch 11/50, Batch 4100/7628\n","Epoch 11/50, Batch 4200/7628\n","Epoch 11/50, Batch 4300/7628\n","Epoch 11/50, Batch 4400/7628\n","Epoch 11/50, Batch 4500/7628\n","Epoch 11/50, Batch 4600/7628\n","Epoch 11/50, Batch 4700/7628\n","Epoch 11/50, Batch 4800/7628\n","Epoch 11/50, Batch 4900/7628\n","Epoch 11/50, Batch 5000/7628\n","Epoch 11/50, Batch 5100/7628\n","Epoch 11/50, Batch 5200/7628\n","Epoch 11/50, Batch 5300/7628\n","Epoch 11/50, Batch 5400/7628\n","Epoch 11/50, Batch 5500/7628\n","Epoch 11/50, Batch 5600/7628\n","Epoch 11/50, Batch 5700/7628\n","Epoch 11/50, Batch 5800/7628\n","Epoch 11/50, Batch 5900/7628\n","Epoch 11/50, Batch 6000/7628\n","Epoch 11/50, Batch 6100/7628\n","Epoch 11/50, Batch 6200/7628\n","Epoch 11/50, Batch 6300/7628\n","Epoch 11/50, Batch 6400/7628\n","Epoch 11/50, Batch 6500/7628\n","Epoch 11/50, Batch 6600/7628\n","Epoch 11/50, Batch 6700/7628\n","Epoch 11/50, Batch 6800/7628\n","Epoch 11/50, Batch 6900/7628\n","Epoch 11/50, Batch 7000/7628\n","Epoch 11/50, Batch 7100/7628\n","Epoch 11/50, Batch 7200/7628\n","Epoch 11/50, Batch 7300/7628\n","Epoch 11/50, Batch 7400/7628\n","Epoch 11/50, Batch 7500/7628\n","Epoch 11/50, Batch 7600/7628\n","Epoch 11/50, Loss: 0.2751\n","Best model saved with loss: 0.2751\n","Epoch 12/50, Batch 0/7628\n","Epoch 12/50, Batch 100/7628\n","Epoch 12/50, Batch 200/7628\n","Epoch 12/50, Batch 300/7628\n","Epoch 12/50, Batch 400/7628\n","Epoch 12/50, Batch 500/7628\n","Epoch 12/50, Batch 600/7628\n","Epoch 12/50, Batch 700/7628\n","Epoch 12/50, Batch 800/7628\n","Epoch 12/50, Batch 900/7628\n","Epoch 12/50, Batch 1000/7628\n","Epoch 12/50, Batch 1100/7628\n","Epoch 12/50, Batch 1200/7628\n","Epoch 12/50, Batch 1300/7628\n","Epoch 12/50, Batch 1400/7628\n","Epoch 12/50, Batch 1500/7628\n","Epoch 12/50, Batch 1600/7628\n","Epoch 12/50, Batch 1700/7628\n","Epoch 12/50, Batch 1800/7628\n","Epoch 12/50, Batch 1900/7628\n","Epoch 12/50, Batch 2000/7628\n","Epoch 12/50, Batch 2100/7628\n","Epoch 12/50, Batch 2200/7628\n","Epoch 12/50, Batch 2300/7628\n","Epoch 12/50, Batch 2400/7628\n","Epoch 12/50, Batch 2500/7628\n","Epoch 12/50, Batch 2600/7628\n","Epoch 12/50, Batch 2700/7628\n","Epoch 12/50, Batch 2800/7628\n","Epoch 12/50, Batch 2900/7628\n","Epoch 12/50, Batch 3000/7628\n","Epoch 12/50, Batch 3100/7628\n","Epoch 12/50, Batch 3200/7628\n","Epoch 12/50, Batch 3300/7628\n","Epoch 12/50, Batch 3400/7628\n","Epoch 12/50, Batch 3500/7628\n","Epoch 12/50, Batch 3600/7628\n","Epoch 12/50, Batch 3700/7628\n","Epoch 12/50, Batch 3800/7628\n","Epoch 12/50, Batch 3900/7628\n","Epoch 12/50, Batch 4000/7628\n","Epoch 12/50, Batch 4100/7628\n","Epoch 12/50, Batch 4200/7628\n","Epoch 12/50, Batch 4300/7628\n","Epoch 12/50, Batch 4400/7628\n","Epoch 12/50, Batch 4500/7628\n","Epoch 12/50, Batch 4600/7628\n","Epoch 12/50, Batch 4700/7628\n","Epoch 12/50, Batch 4800/7628\n","Epoch 12/50, Batch 4900/7628\n","Epoch 12/50, Batch 5000/7628\n","Epoch 12/50, Batch 5100/7628\n","Epoch 12/50, Batch 5200/7628\n","Epoch 12/50, Batch 5300/7628\n","Epoch 12/50, Batch 5400/7628\n","Epoch 12/50, Batch 5500/7628\n","Epoch 12/50, Batch 5600/7628\n","Epoch 12/50, Batch 5700/7628\n","Epoch 12/50, Batch 5800/7628\n","Epoch 12/50, Batch 5900/7628\n","Epoch 12/50, Batch 6000/7628\n","Epoch 12/50, Batch 6100/7628\n","Epoch 12/50, Batch 6200/7628\n","Epoch 12/50, Batch 6300/7628\n","Epoch 12/50, Batch 6400/7628\n","Epoch 12/50, Batch 6500/7628\n","Epoch 12/50, Batch 6600/7628\n","Epoch 12/50, Batch 6700/7628\n","Epoch 12/50, Batch 6800/7628\n","Epoch 12/50, Batch 6900/7628\n","Epoch 12/50, Batch 7000/7628\n","Epoch 12/50, Batch 7100/7628\n","Epoch 12/50, Batch 7200/7628\n","Epoch 12/50, Batch 7300/7628\n","Epoch 12/50, Batch 7400/7628\n","Epoch 12/50, Batch 7500/7628\n","Epoch 12/50, Batch 7600/7628\n","Epoch 12/50, Loss: 0.2769\n","No improvement for 1 epoch(s)\n","Epoch 13/50, Batch 0/7628\n","Epoch 13/50, Batch 100/7628\n","Epoch 13/50, Batch 200/7628\n","Epoch 13/50, Batch 300/7628\n","Epoch 13/50, Batch 400/7628\n","Epoch 13/50, Batch 500/7628\n","Epoch 13/50, Batch 600/7628\n","Epoch 13/50, Batch 700/7628\n","Epoch 13/50, Batch 800/7628\n","Epoch 13/50, Batch 900/7628\n","Epoch 13/50, Batch 1000/7628\n","Epoch 13/50, Batch 1100/7628\n","Epoch 13/50, Batch 1200/7628\n","Epoch 13/50, Batch 1300/7628\n","Epoch 13/50, Batch 1400/7628\n","Epoch 13/50, Batch 1500/7628\n","Epoch 13/50, Batch 1600/7628\n","Epoch 13/50, Batch 1700/7628\n","Epoch 13/50, Batch 1800/7628\n","Epoch 13/50, Batch 1900/7628\n","Epoch 13/50, Batch 2000/7628\n","Epoch 13/50, Batch 2100/7628\n","Epoch 13/50, Batch 2200/7628\n","Epoch 13/50, Batch 2300/7628\n","Epoch 13/50, Batch 2400/7628\n","Epoch 13/50, Batch 2500/7628\n","Epoch 13/50, Batch 2600/7628\n","Epoch 13/50, Batch 2700/7628\n","Epoch 13/50, Batch 2800/7628\n","Epoch 13/50, Batch 2900/7628\n","Epoch 13/50, Batch 3000/7628\n","Epoch 13/50, Batch 3100/7628\n","Epoch 13/50, Batch 3200/7628\n","Epoch 13/50, Batch 3300/7628\n","Epoch 13/50, Batch 3400/7628\n","Epoch 13/50, Batch 3500/7628\n","Epoch 13/50, Batch 3600/7628\n","Epoch 13/50, Batch 3700/7628\n","Epoch 13/50, Batch 3800/7628\n","Epoch 13/50, Batch 3900/7628\n","Epoch 13/50, Batch 4000/7628\n","Epoch 13/50, Batch 4100/7628\n","Epoch 13/50, Batch 4200/7628\n","Epoch 13/50, Batch 4300/7628\n","Epoch 13/50, Batch 4400/7628\n","Epoch 13/50, Batch 4500/7628\n","Epoch 13/50, Batch 4600/7628\n","Epoch 13/50, Batch 4700/7628\n","Epoch 13/50, Batch 4800/7628\n","Epoch 13/50, Batch 4900/7628\n","Epoch 13/50, Batch 5000/7628\n","Epoch 13/50, Batch 5100/7628\n","Epoch 13/50, Batch 5200/7628\n","Epoch 13/50, Batch 5300/7628\n","Epoch 13/50, Batch 5400/7628\n","Epoch 13/50, Batch 5500/7628\n","Epoch 13/50, Batch 5600/7628\n","Epoch 13/50, Batch 5700/7628\n","Epoch 13/50, Batch 5800/7628\n","Epoch 13/50, Batch 5900/7628\n","Epoch 13/50, Batch 6000/7628\n","Epoch 13/50, Batch 6100/7628\n","Epoch 13/50, Batch 6200/7628\n","Epoch 13/50, Batch 6300/7628\n","Epoch 13/50, Batch 6400/7628\n","Epoch 13/50, Batch 6500/7628\n","Epoch 13/50, Batch 6600/7628\n","Epoch 13/50, Batch 6700/7628\n","Epoch 13/50, Batch 6800/7628\n","Epoch 13/50, Batch 6900/7628\n","Epoch 13/50, Batch 7000/7628\n","Epoch 13/50, Batch 7100/7628\n","Epoch 13/50, Batch 7200/7628\n","Epoch 13/50, Batch 7300/7628\n","Epoch 13/50, Batch 7400/7628\n","Epoch 13/50, Batch 7500/7628\n","Epoch 13/50, Batch 7600/7628\n","Epoch 13/50, Loss: 0.2820\n","No improvement for 2 epoch(s)\n","Epoch 14/50, Batch 0/7628\n","Epoch 14/50, Batch 100/7628\n","Epoch 14/50, Batch 200/7628\n","Epoch 14/50, Batch 300/7628\n","Epoch 14/50, Batch 400/7628\n","Epoch 14/50, Batch 500/7628\n","Epoch 14/50, Batch 600/7628\n","Epoch 14/50, Batch 700/7628\n","Epoch 14/50, Batch 800/7628\n","Epoch 14/50, Batch 900/7628\n","Epoch 14/50, Batch 1000/7628\n","Epoch 14/50, Batch 1100/7628\n","Epoch 14/50, Batch 1200/7628\n","Epoch 14/50, Batch 1300/7628\n","Epoch 14/50, Batch 1400/7628\n","Epoch 14/50, Batch 1500/7628\n","Epoch 14/50, Batch 1600/7628\n","Epoch 14/50, Batch 1700/7628\n","Epoch 14/50, Batch 1800/7628\n","Epoch 14/50, Batch 1900/7628\n","Epoch 14/50, Batch 2000/7628\n","Epoch 14/50, Batch 2100/7628\n","Epoch 14/50, Batch 2200/7628\n","Epoch 14/50, Batch 2300/7628\n","Epoch 14/50, Batch 2400/7628\n","Epoch 14/50, Batch 2500/7628\n","Epoch 14/50, Batch 2600/7628\n","Epoch 14/50, Batch 2700/7628\n","Epoch 14/50, Batch 2800/7628\n","Epoch 14/50, Batch 2900/7628\n","Epoch 14/50, Batch 3000/7628\n","Epoch 14/50, Batch 3100/7628\n","Epoch 14/50, Batch 3200/7628\n","Epoch 14/50, Batch 3300/7628\n","Epoch 14/50, Batch 3400/7628\n","Epoch 14/50, Batch 3500/7628\n","Epoch 14/50, Batch 3600/7628\n","Epoch 14/50, Batch 3700/7628\n","Epoch 14/50, Batch 3800/7628\n","Epoch 14/50, Batch 3900/7628\n","Epoch 14/50, Batch 4000/7628\n","Epoch 14/50, Batch 4100/7628\n","Epoch 14/50, Batch 4200/7628\n","Epoch 14/50, Batch 4300/7628\n","Epoch 14/50, Batch 4400/7628\n","Epoch 14/50, Batch 4500/7628\n","Epoch 14/50, Batch 4600/7628\n","Epoch 14/50, Batch 4700/7628\n","Epoch 14/50, Batch 4800/7628\n","Epoch 14/50, Batch 4900/7628\n","Epoch 14/50, Batch 5000/7628\n","Epoch 14/50, Batch 5100/7628\n","Epoch 14/50, Batch 5200/7628\n","Epoch 14/50, Batch 5300/7628\n","Epoch 14/50, Batch 5400/7628\n","Epoch 14/50, Batch 5500/7628\n","Epoch 14/50, Batch 5600/7628\n","Epoch 14/50, Batch 5700/7628\n","Epoch 14/50, Batch 5800/7628\n","Epoch 14/50, Batch 5900/7628\n","Epoch 14/50, Batch 6000/7628\n","Epoch 14/50, Batch 6100/7628\n","Epoch 14/50, Batch 6200/7628\n","Epoch 14/50, Batch 6300/7628\n","Epoch 14/50, Batch 6400/7628\n","Epoch 14/50, Batch 6500/7628\n","Epoch 14/50, Batch 6600/7628\n","Epoch 14/50, Batch 6700/7628\n","Epoch 14/50, Batch 6800/7628\n","Epoch 14/50, Batch 6900/7628\n","Epoch 14/50, Batch 7000/7628\n","Epoch 14/50, Batch 7100/7628\n","Epoch 14/50, Batch 7200/7628\n","Epoch 14/50, Batch 7300/7628\n","Epoch 14/50, Batch 7400/7628\n","Epoch 14/50, Batch 7500/7628\n","Epoch 14/50, Batch 7600/7628\n","Epoch 14/50, Loss: 0.2735\n","Best model saved with loss: 0.2735\n","Epoch 15/50, Batch 0/7628\n","Epoch 15/50, Batch 100/7628\n","Epoch 15/50, Batch 200/7628\n","Epoch 15/50, Batch 300/7628\n","Epoch 15/50, Batch 400/7628\n","Epoch 15/50, Batch 500/7628\n","Epoch 15/50, Batch 600/7628\n","Epoch 15/50, Batch 700/7628\n","Epoch 15/50, Batch 800/7628\n","Epoch 15/50, Batch 900/7628\n","Epoch 15/50, Batch 1000/7628\n","Epoch 15/50, Batch 1100/7628\n","Epoch 15/50, Batch 1200/7628\n","Epoch 15/50, Batch 1300/7628\n","Epoch 15/50, Batch 1400/7628\n","Epoch 15/50, Batch 1500/7628\n","Epoch 15/50, Batch 1600/7628\n","Epoch 15/50, Batch 1700/7628\n","Epoch 15/50, Batch 1800/7628\n","Epoch 15/50, Batch 1900/7628\n","Epoch 15/50, Batch 2000/7628\n","Epoch 15/50, Batch 2100/7628\n","Epoch 15/50, Batch 2200/7628\n","Epoch 15/50, Batch 2300/7628\n","Epoch 15/50, Batch 2400/7628\n","Epoch 15/50, Batch 2500/7628\n","Epoch 15/50, Batch 2600/7628\n","Epoch 15/50, Batch 2700/7628\n","Epoch 15/50, Batch 2800/7628\n","Epoch 15/50, Batch 2900/7628\n","Epoch 15/50, Batch 3000/7628\n","Epoch 15/50, Batch 3100/7628\n","Epoch 15/50, Batch 3200/7628\n","Epoch 15/50, Batch 3300/7628\n","Epoch 15/50, Batch 3400/7628\n","Epoch 15/50, Batch 3500/7628\n","Epoch 15/50, Batch 3600/7628\n","Epoch 15/50, Batch 3700/7628\n","Epoch 15/50, Batch 3800/7628\n","Epoch 15/50, Batch 3900/7628\n","Epoch 15/50, Batch 4000/7628\n","Epoch 15/50, Batch 4100/7628\n","Epoch 15/50, Batch 4200/7628\n","Epoch 15/50, Batch 4300/7628\n","Epoch 15/50, Batch 4400/7628\n","Epoch 15/50, Batch 4500/7628\n","Epoch 15/50, Batch 4600/7628\n","Epoch 15/50, Batch 4700/7628\n","Epoch 15/50, Batch 4800/7628\n","Epoch 15/50, Batch 4900/7628\n","Epoch 15/50, Batch 5000/7628\n","Epoch 15/50, Batch 5100/7628\n","Epoch 15/50, Batch 5200/7628\n","Epoch 15/50, Batch 5300/7628\n","Epoch 15/50, Batch 5400/7628\n","Epoch 15/50, Batch 5500/7628\n","Epoch 15/50, Batch 5600/7628\n","Epoch 15/50, Batch 5700/7628\n","Epoch 15/50, Batch 5800/7628\n","Epoch 15/50, Batch 5900/7628\n","Epoch 15/50, Batch 6000/7628\n","Epoch 15/50, Batch 6100/7628\n","Epoch 15/50, Batch 6200/7628\n","Epoch 15/50, Batch 6300/7628\n","Epoch 15/50, Batch 6400/7628\n","Epoch 15/50, Batch 6500/7628\n","Epoch 15/50, Batch 6600/7628\n","Epoch 15/50, Batch 6700/7628\n","Epoch 15/50, Batch 6800/7628\n","Epoch 15/50, Batch 6900/7628\n","Epoch 15/50, Batch 7000/7628\n","Epoch 15/50, Batch 7100/7628\n","Epoch 15/50, Batch 7200/7628\n","Epoch 15/50, Batch 7300/7628\n","Epoch 15/50, Batch 7400/7628\n","Epoch 15/50, Batch 7500/7628\n","Epoch 15/50, Batch 7600/7628\n","Epoch 15/50, Loss: 0.2737\n","No improvement for 1 epoch(s)\n","Epoch 16/50, Batch 0/7628\n","Epoch 16/50, Batch 100/7628\n","Epoch 16/50, Batch 200/7628\n","Epoch 16/50, Batch 300/7628\n","Epoch 16/50, Batch 400/7628\n","Epoch 16/50, Batch 500/7628\n","Epoch 16/50, Batch 600/7628\n","Epoch 16/50, Batch 700/7628\n","Epoch 16/50, Batch 800/7628\n","Epoch 16/50, Batch 900/7628\n","Epoch 16/50, Batch 1000/7628\n","Epoch 16/50, Batch 1100/7628\n","Epoch 16/50, Batch 1200/7628\n","Epoch 16/50, Batch 1300/7628\n","Epoch 16/50, Batch 1400/7628\n","Epoch 16/50, Batch 1500/7628\n","Epoch 16/50, Batch 1600/7628\n","Epoch 16/50, Batch 1700/7628\n","Epoch 16/50, Batch 1800/7628\n","Epoch 16/50, Batch 1900/7628\n","Epoch 16/50, Batch 2000/7628\n","Epoch 16/50, Batch 2100/7628\n","Epoch 16/50, Batch 2200/7628\n","Epoch 16/50, Batch 2300/7628\n","Epoch 16/50, Batch 2400/7628\n","Epoch 16/50, Batch 2500/7628\n","Epoch 16/50, Batch 2600/7628\n","Epoch 16/50, Batch 2700/7628\n","Epoch 16/50, Batch 2800/7628\n","Epoch 16/50, Batch 2900/7628\n","Epoch 16/50, Batch 3000/7628\n","Epoch 16/50, Batch 3100/7628\n","Epoch 16/50, Batch 3200/7628\n","Epoch 16/50, Batch 3300/7628\n","Epoch 16/50, Batch 3400/7628\n","Epoch 16/50, Batch 3500/7628\n","Epoch 16/50, Batch 3600/7628\n","Epoch 16/50, Batch 3700/7628\n","Epoch 16/50, Batch 3800/7628\n","Epoch 16/50, Batch 3900/7628\n","Epoch 16/50, Batch 4000/7628\n","Epoch 16/50, Batch 4100/7628\n","Epoch 16/50, Batch 4200/7628\n","Epoch 16/50, Batch 4300/7628\n","Epoch 16/50, Batch 4400/7628\n","Epoch 16/50, Batch 4500/7628\n","Epoch 16/50, Batch 4600/7628\n","Epoch 16/50, Batch 4700/7628\n","Epoch 16/50, Batch 4800/7628\n","Epoch 16/50, Batch 4900/7628\n","Epoch 16/50, Batch 5000/7628\n","Epoch 16/50, Batch 5100/7628\n","Epoch 16/50, Batch 5200/7628\n","Epoch 16/50, Batch 5300/7628\n","Epoch 16/50, Batch 5400/7628\n","Epoch 16/50, Batch 5500/7628\n","Epoch 16/50, Batch 5600/7628\n","Epoch 16/50, Batch 5700/7628\n","Epoch 16/50, Batch 5800/7628\n","Epoch 16/50, Batch 5900/7628\n","Epoch 16/50, Batch 6000/7628\n","Epoch 16/50, Batch 6100/7628\n","Epoch 16/50, Batch 6200/7628\n","Epoch 16/50, Batch 6300/7628\n","Epoch 16/50, Batch 6400/7628\n","Epoch 16/50, Batch 6500/7628\n","Epoch 16/50, Batch 6600/7628\n","Epoch 16/50, Batch 6700/7628\n","Epoch 16/50, Batch 6800/7628\n","Epoch 16/50, Batch 6900/7628\n","Epoch 16/50, Batch 7000/7628\n","Epoch 16/50, Batch 7100/7628\n","Epoch 16/50, Batch 7200/7628\n","Epoch 16/50, Batch 7300/7628\n","Epoch 16/50, Batch 7400/7628\n","Epoch 16/50, Batch 7500/7628\n","Epoch 16/50, Batch 7600/7628\n","Epoch 16/50, Loss: 0.2816\n","No improvement for 2 epoch(s)\n","Epoch 17/50, Batch 0/7628\n","Epoch 17/50, Batch 100/7628\n","Epoch 17/50, Batch 200/7628\n","Epoch 17/50, Batch 300/7628\n","Epoch 17/50, Batch 400/7628\n","Epoch 17/50, Batch 500/7628\n","Epoch 17/50, Batch 600/7628\n","Epoch 17/50, Batch 700/7628\n","Epoch 17/50, Batch 800/7628\n","Epoch 17/50, Batch 900/7628\n","Epoch 17/50, Batch 1000/7628\n","Epoch 17/50, Batch 1100/7628\n","Epoch 17/50, Batch 1200/7628\n","Epoch 17/50, Batch 1300/7628\n","Epoch 17/50, Batch 1400/7628\n","Epoch 17/50, Batch 1500/7628\n","Epoch 17/50, Batch 1600/7628\n","Epoch 17/50, Batch 1700/7628\n","Epoch 17/50, Batch 1800/7628\n","Epoch 17/50, Batch 1900/7628\n","Epoch 17/50, Batch 2000/7628\n","Epoch 17/50, Batch 2100/7628\n","Epoch 17/50, Batch 2200/7628\n","Epoch 17/50, Batch 2300/7628\n","Epoch 17/50, Batch 2400/7628\n","Epoch 17/50, Batch 2500/7628\n","Epoch 17/50, Batch 2600/7628\n","Epoch 17/50, Batch 2700/7628\n","Epoch 17/50, Batch 2800/7628\n","Epoch 17/50, Batch 2900/7628\n","Epoch 17/50, Batch 3000/7628\n","Epoch 17/50, Batch 3100/7628\n","Epoch 17/50, Batch 3200/7628\n","Epoch 17/50, Batch 3300/7628\n","Epoch 17/50, Batch 3400/7628\n","Epoch 17/50, Batch 3500/7628\n","Epoch 17/50, Batch 3600/7628\n","Epoch 17/50, Batch 3700/7628\n","Epoch 17/50, Batch 3800/7628\n","Epoch 17/50, Batch 3900/7628\n","Epoch 17/50, Batch 4000/7628\n","Epoch 17/50, Batch 4100/7628\n","Epoch 17/50, Batch 4200/7628\n","Epoch 17/50, Batch 4300/7628\n","Epoch 17/50, Batch 4400/7628\n","Epoch 17/50, Batch 4500/7628\n","Epoch 17/50, Batch 4600/7628\n","Epoch 17/50, Batch 4700/7628\n","Epoch 17/50, Batch 4800/7628\n","Epoch 17/50, Batch 4900/7628\n","Epoch 17/50, Batch 5000/7628\n","Epoch 17/50, Batch 5100/7628\n","Epoch 17/50, Batch 5200/7628\n","Epoch 17/50, Batch 5300/7628\n","Epoch 17/50, Batch 5400/7628\n","Epoch 17/50, Batch 5500/7628\n","Epoch 17/50, Batch 5600/7628\n","Epoch 17/50, Batch 5700/7628\n","Epoch 17/50, Batch 5800/7628\n","Epoch 17/50, Batch 5900/7628\n","Epoch 17/50, Batch 6000/7628\n","Epoch 17/50, Batch 6100/7628\n","Epoch 17/50, Batch 6200/7628\n","Epoch 17/50, Batch 6300/7628\n","Epoch 17/50, Batch 6400/7628\n","Epoch 17/50, Batch 6500/7628\n","Epoch 17/50, Batch 6600/7628\n","Epoch 17/50, Batch 6700/7628\n","Epoch 17/50, Batch 6800/7628\n","Epoch 17/50, Batch 6900/7628\n","Epoch 17/50, Batch 7000/7628\n","Epoch 17/50, Batch 7100/7628\n","Epoch 17/50, Batch 7200/7628\n","Epoch 17/50, Batch 7300/7628\n","Epoch 17/50, Batch 7400/7628\n","Epoch 17/50, Batch 7500/7628\n","Epoch 17/50, Batch 7600/7628\n","Epoch 17/50, Loss: 0.2826\n","No improvement for 3 epoch(s)\n","Epoch 18/50, Batch 0/7628\n","Epoch 18/50, Batch 100/7628\n","Epoch 18/50, Batch 200/7628\n","Epoch 18/50, Batch 300/7628\n","Epoch 18/50, Batch 400/7628\n","Epoch 18/50, Batch 500/7628\n","Epoch 18/50, Batch 600/7628\n","Epoch 18/50, Batch 700/7628\n","Epoch 18/50, Batch 800/7628\n","Epoch 18/50, Batch 900/7628\n","Epoch 18/50, Batch 1000/7628\n","Epoch 18/50, Batch 1100/7628\n","Epoch 18/50, Batch 1200/7628\n","Epoch 18/50, Batch 1300/7628\n","Epoch 18/50, Batch 1400/7628\n","Epoch 18/50, Batch 1500/7628\n","Epoch 18/50, Batch 1600/7628\n","Epoch 18/50, Batch 1700/7628\n","Epoch 18/50, Batch 1800/7628\n","Epoch 18/50, Batch 1900/7628\n","Epoch 18/50, Batch 2000/7628\n","Epoch 18/50, Batch 2100/7628\n","Epoch 18/50, Batch 2200/7628\n","Epoch 18/50, Batch 2300/7628\n","Epoch 18/50, Batch 2400/7628\n","Epoch 18/50, Batch 2500/7628\n","Epoch 18/50, Batch 2600/7628\n","Epoch 18/50, Batch 2700/7628\n","Epoch 18/50, Batch 2800/7628\n","Epoch 18/50, Batch 2900/7628\n","Epoch 18/50, Batch 3000/7628\n","Epoch 18/50, Batch 3100/7628\n","Epoch 18/50, Batch 3200/7628\n","Epoch 18/50, Batch 3300/7628\n","Epoch 18/50, Batch 3400/7628\n","Epoch 18/50, Batch 3500/7628\n","Epoch 18/50, Batch 3600/7628\n","Epoch 18/50, Batch 3700/7628\n","Epoch 18/50, Batch 3800/7628\n","Epoch 18/50, Batch 3900/7628\n","Epoch 18/50, Batch 4000/7628\n","Epoch 18/50, Batch 4100/7628\n","Epoch 18/50, Batch 4200/7628\n","Epoch 18/50, Batch 4300/7628\n","Epoch 18/50, Batch 4400/7628\n","Epoch 18/50, Batch 4500/7628\n","Epoch 18/50, Batch 4600/7628\n","Epoch 18/50, Batch 4700/7628\n","Epoch 18/50, Batch 4800/7628\n","Epoch 18/50, Batch 4900/7628\n","Epoch 18/50, Batch 5000/7628\n","Epoch 18/50, Batch 5100/7628\n","Epoch 18/50, Batch 5200/7628\n","Epoch 18/50, Batch 5300/7628\n","Epoch 18/50, Batch 5400/7628\n","Epoch 18/50, Batch 5500/7628\n","Epoch 18/50, Batch 5600/7628\n","Epoch 18/50, Batch 5700/7628\n","Epoch 18/50, Batch 5800/7628\n","Epoch 18/50, Batch 5900/7628\n","Epoch 18/50, Batch 6000/7628\n","Epoch 18/50, Batch 6100/7628\n","Epoch 18/50, Batch 6200/7628\n","Epoch 18/50, Batch 6300/7628\n","Epoch 18/50, Batch 6400/7628\n","Epoch 18/50, Batch 6500/7628\n","Epoch 18/50, Batch 6600/7628\n","Epoch 18/50, Batch 6700/7628\n","Epoch 18/50, Batch 6800/7628\n","Epoch 18/50, Batch 6900/7628\n","Epoch 18/50, Batch 7000/7628\n","Epoch 18/50, Batch 7100/7628\n","Epoch 18/50, Batch 7200/7628\n","Epoch 18/50, Batch 7300/7628\n","Epoch 18/50, Batch 7400/7628\n","Epoch 18/50, Batch 7500/7628\n","Epoch 18/50, Batch 7600/7628\n","Epoch 18/50, Loss: 0.2724\n","Best model saved with loss: 0.2724\n","Epoch 19/50, Batch 0/7628\n","Epoch 19/50, Batch 100/7628\n","Epoch 19/50, Batch 200/7628\n","Epoch 19/50, Batch 300/7628\n","Epoch 19/50, Batch 400/7628\n","Epoch 19/50, Batch 500/7628\n","Epoch 19/50, Batch 600/7628\n","Epoch 19/50, Batch 700/7628\n","Epoch 19/50, Batch 800/7628\n","Epoch 19/50, Batch 900/7628\n","Epoch 19/50, Batch 1000/7628\n","Epoch 19/50, Batch 1100/7628\n","Epoch 19/50, Batch 1200/7628\n","Epoch 19/50, Batch 1300/7628\n","Epoch 19/50, Batch 1400/7628\n","Epoch 19/50, Batch 1500/7628\n","Epoch 19/50, Batch 1600/7628\n","Epoch 19/50, Batch 1700/7628\n","Epoch 19/50, Batch 1800/7628\n","Epoch 19/50, Batch 1900/7628\n","Epoch 19/50, Batch 2000/7628\n","Epoch 19/50, Batch 2100/7628\n","Epoch 19/50, Batch 2200/7628\n","Epoch 19/50, Batch 2300/7628\n","Epoch 19/50, Batch 2400/7628\n","Epoch 19/50, Batch 2500/7628\n","Epoch 19/50, Batch 2600/7628\n","Epoch 19/50, Batch 2700/7628\n","Epoch 19/50, Batch 2800/7628\n","Epoch 19/50, Batch 2900/7628\n","Epoch 19/50, Batch 3000/7628\n","Epoch 19/50, Batch 3100/7628\n","Epoch 19/50, Batch 3200/7628\n","Epoch 19/50, Batch 3300/7628\n","Epoch 19/50, Batch 3400/7628\n","Epoch 19/50, Batch 3500/7628\n","Epoch 19/50, Batch 3600/7628\n","Epoch 19/50, Batch 3700/7628\n","Epoch 19/50, Batch 3800/7628\n","Epoch 19/50, Batch 3900/7628\n","Epoch 19/50, Batch 4000/7628\n","Epoch 19/50, Batch 4100/7628\n","Epoch 19/50, Batch 4200/7628\n","Epoch 19/50, Batch 4300/7628\n","Epoch 19/50, Batch 4400/7628\n","Epoch 19/50, Batch 4500/7628\n","Epoch 19/50, Batch 4600/7628\n","Epoch 19/50, Batch 4700/7628\n","Epoch 19/50, Batch 4800/7628\n","Epoch 19/50, Batch 4900/7628\n","Epoch 19/50, Batch 5000/7628\n","Epoch 19/50, Batch 5100/7628\n","Epoch 19/50, Batch 5200/7628\n","Epoch 19/50, Batch 5300/7628\n","Epoch 19/50, Batch 5400/7628\n","Epoch 19/50, Batch 5500/7628\n","Epoch 19/50, Batch 5600/7628\n","Epoch 19/50, Batch 5700/7628\n","Epoch 19/50, Batch 5800/7628\n","Epoch 19/50, Batch 5900/7628\n","Epoch 19/50, Batch 6000/7628\n","Epoch 19/50, Batch 6100/7628\n","Epoch 19/50, Batch 6200/7628\n","Epoch 19/50, Batch 6300/7628\n","Epoch 19/50, Batch 6400/7628\n","Epoch 19/50, Batch 6500/7628\n","Epoch 19/50, Batch 6600/7628\n","Epoch 19/50, Batch 6700/7628\n","Epoch 19/50, Batch 6800/7628\n","Epoch 19/50, Batch 6900/7628\n","Epoch 19/50, Batch 7000/7628\n","Epoch 19/50, Batch 7100/7628\n","Epoch 19/50, Batch 7200/7628\n","Epoch 19/50, Batch 7300/7628\n","Epoch 19/50, Batch 7400/7628\n","Epoch 19/50, Batch 7500/7628\n","Epoch 19/50, Batch 7600/7628\n","Epoch 19/50, Loss: 0.2727\n","No improvement for 1 epoch(s)\n","Epoch 20/50, Batch 0/7628\n","Epoch 20/50, Batch 100/7628\n","Epoch 20/50, Batch 200/7628\n","Epoch 20/50, Batch 300/7628\n","Epoch 20/50, Batch 400/7628\n","Epoch 20/50, Batch 500/7628\n","Epoch 20/50, Batch 600/7628\n","Epoch 20/50, Batch 700/7628\n","Epoch 20/50, Batch 800/7628\n","Epoch 20/50, Batch 900/7628\n","Epoch 20/50, Batch 1000/7628\n","Epoch 20/50, Batch 1100/7628\n","Epoch 20/50, Batch 1200/7628\n","Epoch 20/50, Batch 1300/7628\n","Epoch 20/50, Batch 1400/7628\n","Epoch 20/50, Batch 1500/7628\n","Epoch 20/50, Batch 1600/7628\n","Epoch 20/50, Batch 1700/7628\n","Epoch 20/50, Batch 1800/7628\n","Epoch 20/50, Batch 1900/7628\n","Epoch 20/50, Batch 2000/7628\n","Epoch 20/50, Batch 2100/7628\n","Epoch 20/50, Batch 2200/7628\n","Epoch 20/50, Batch 2300/7628\n","Epoch 20/50, Batch 2400/7628\n","Epoch 20/50, Batch 2500/7628\n","Epoch 20/50, Batch 2600/7628\n","Epoch 20/50, Batch 2700/7628\n","Epoch 20/50, Batch 2800/7628\n","Epoch 20/50, Batch 2900/7628\n","Epoch 20/50, Batch 3000/7628\n","Epoch 20/50, Batch 3100/7628\n","Epoch 20/50, Batch 3200/7628\n","Epoch 20/50, Batch 3300/7628\n","Epoch 20/50, Batch 3400/7628\n","Epoch 20/50, Batch 3500/7628\n","Epoch 20/50, Batch 3600/7628\n","Epoch 20/50, Batch 3700/7628\n","Epoch 20/50, Batch 3800/7628\n","Epoch 20/50, Batch 3900/7628\n","Epoch 20/50, Batch 4000/7628\n","Epoch 20/50, Batch 4100/7628\n","Epoch 20/50, Batch 4200/7628\n","Epoch 20/50, Batch 4300/7628\n","Epoch 20/50, Batch 4400/7628\n","Epoch 20/50, Batch 4500/7628\n","Epoch 20/50, Batch 4600/7628\n","Epoch 20/50, Batch 4700/7628\n","Epoch 20/50, Batch 4800/7628\n","Epoch 20/50, Batch 4900/7628\n","Epoch 20/50, Batch 5000/7628\n","Epoch 20/50, Batch 5100/7628\n","Epoch 20/50, Batch 5200/7628\n","Epoch 20/50, Batch 5300/7628\n","Epoch 20/50, Batch 5400/7628\n","Epoch 20/50, Batch 5500/7628\n","Epoch 20/50, Batch 5600/7628\n","Epoch 20/50, Batch 5700/7628\n","Epoch 20/50, Batch 5800/7628\n","Epoch 20/50, Batch 5900/7628\n","Epoch 20/50, Batch 6000/7628\n","Epoch 20/50, Batch 6100/7628\n","Epoch 20/50, Batch 6200/7628\n","Epoch 20/50, Batch 6300/7628\n","Epoch 20/50, Batch 6400/7628\n","Epoch 20/50, Batch 6500/7628\n","Epoch 20/50, Batch 6600/7628\n","Epoch 20/50, Batch 6700/7628\n","Epoch 20/50, Batch 6800/7628\n","Epoch 20/50, Batch 6900/7628\n","Epoch 20/50, Batch 7000/7628\n","Epoch 20/50, Batch 7100/7628\n","Epoch 20/50, Batch 7200/7628\n","Epoch 20/50, Batch 7300/7628\n","Epoch 20/50, Batch 7400/7628\n","Epoch 20/50, Batch 7500/7628\n","Epoch 20/50, Batch 7600/7628\n","Epoch 20/50, Loss: 0.3775\n","No improvement for 2 epoch(s)\n","Epoch 21/50, Batch 0/7628\n","Epoch 21/50, Batch 100/7628\n","Epoch 21/50, Batch 200/7628\n","Epoch 21/50, Batch 300/7628\n","Epoch 21/50, Batch 400/7628\n","Epoch 21/50, Batch 500/7628\n","Epoch 21/50, Batch 600/7628\n","Epoch 21/50, Batch 700/7628\n","Epoch 21/50, Batch 800/7628\n","Epoch 21/50, Batch 900/7628\n","Epoch 21/50, Batch 1000/7628\n","Epoch 21/50, Batch 1100/7628\n","Epoch 21/50, Batch 1200/7628\n","Epoch 21/50, Batch 1300/7628\n","Epoch 21/50, Batch 1400/7628\n","Epoch 21/50, Batch 1500/7628\n","Epoch 21/50, Batch 1600/7628\n","Epoch 21/50, Batch 1700/7628\n","Epoch 21/50, Batch 1800/7628\n","Epoch 21/50, Batch 1900/7628\n","Epoch 21/50, Batch 2000/7628\n","Epoch 21/50, Batch 2100/7628\n","Epoch 21/50, Batch 2200/7628\n","Epoch 21/50, Batch 2300/7628\n","Epoch 21/50, Batch 2400/7628\n","Epoch 21/50, Batch 2500/7628\n","Epoch 21/50, Batch 2600/7628\n","Epoch 21/50, Batch 2700/7628\n","Epoch 21/50, Batch 2800/7628\n","Epoch 21/50, Batch 2900/7628\n","Epoch 21/50, Batch 3000/7628\n","Epoch 21/50, Batch 3100/7628\n","Epoch 21/50, Batch 3200/7628\n","Epoch 21/50, Batch 3300/7628\n","Epoch 21/50, Batch 3400/7628\n","Epoch 21/50, Batch 3500/7628\n","Epoch 21/50, Batch 3600/7628\n","Epoch 21/50, Batch 3700/7628\n","Epoch 21/50, Batch 3800/7628\n","Epoch 21/50, Batch 3900/7628\n","Epoch 21/50, Batch 4000/7628\n","Epoch 21/50, Batch 4100/7628\n","Epoch 21/50, Batch 4200/7628\n","Epoch 21/50, Batch 4300/7628\n","Epoch 21/50, Batch 4400/7628\n","Epoch 21/50, Batch 4500/7628\n","Epoch 21/50, Batch 4600/7628\n","Epoch 21/50, Batch 4700/7628\n","Epoch 21/50, Batch 4800/7628\n","Epoch 21/50, Batch 4900/7628\n","Epoch 21/50, Batch 5000/7628\n","Epoch 21/50, Batch 5100/7628\n","Epoch 21/50, Batch 5200/7628\n","Epoch 21/50, Batch 5300/7628\n","Epoch 21/50, Batch 5400/7628\n","Epoch 21/50, Batch 5500/7628\n","Epoch 21/50, Batch 5600/7628\n","Epoch 21/50, Batch 5700/7628\n","Epoch 21/50, Batch 5800/7628\n","Epoch 21/50, Batch 5900/7628\n","Epoch 21/50, Batch 6000/7628\n","Epoch 21/50, Batch 6100/7628\n","Epoch 21/50, Batch 6200/7628\n","Epoch 21/50, Batch 6300/7628\n","Epoch 21/50, Batch 6400/7628\n","Epoch 21/50, Batch 6500/7628\n","Epoch 21/50, Batch 6600/7628\n","Epoch 21/50, Batch 6700/7628\n","Epoch 21/50, Batch 6800/7628\n","Epoch 21/50, Batch 6900/7628\n","Epoch 21/50, Batch 7000/7628\n","Epoch 21/50, Batch 7100/7628\n","Epoch 21/50, Batch 7200/7628\n","Epoch 21/50, Batch 7300/7628\n","Epoch 21/50, Batch 7400/7628\n","Epoch 21/50, Batch 7500/7628\n","Epoch 21/50, Batch 7600/7628\n","Epoch 21/50, Loss: 0.3337\n","No improvement for 3 epoch(s)\n","Epoch 22/50, Batch 0/7628\n","Epoch 22/50, Batch 100/7628\n","Epoch 22/50, Batch 200/7628\n","Epoch 22/50, Batch 300/7628\n","Epoch 22/50, Batch 400/7628\n","Epoch 22/50, Batch 500/7628\n","Epoch 22/50, Batch 600/7628\n","Epoch 22/50, Batch 700/7628\n","Epoch 22/50, Batch 800/7628\n","Epoch 22/50, Batch 900/7628\n","Epoch 22/50, Batch 1000/7628\n","Epoch 22/50, Batch 1100/7628\n","Epoch 22/50, Batch 1200/7628\n","Epoch 22/50, Batch 1300/7628\n","Epoch 22/50, Batch 1400/7628\n","Epoch 22/50, Batch 1500/7628\n","Epoch 22/50, Batch 1600/7628\n","Epoch 22/50, Batch 1700/7628\n","Epoch 22/50, Batch 1800/7628\n","Epoch 22/50, Batch 1900/7628\n","Epoch 22/50, Batch 2000/7628\n","Epoch 22/50, Batch 2100/7628\n","Epoch 22/50, Batch 2200/7628\n","Epoch 22/50, Batch 2300/7628\n","Epoch 22/50, Batch 2400/7628\n","Epoch 22/50, Batch 2500/7628\n","Epoch 22/50, Batch 2600/7628\n","Epoch 22/50, Batch 2700/7628\n","Epoch 22/50, Batch 2800/7628\n","Epoch 22/50, Batch 2900/7628\n","Epoch 22/50, Batch 3000/7628\n","Epoch 22/50, Batch 3100/7628\n","Epoch 22/50, Batch 3200/7628\n","Epoch 22/50, Batch 3300/7628\n","Epoch 22/50, Batch 3400/7628\n","Epoch 22/50, Batch 3500/7628\n","Epoch 22/50, Batch 3600/7628\n","Epoch 22/50, Batch 3700/7628\n","Epoch 22/50, Batch 3800/7628\n","Epoch 22/50, Batch 3900/7628\n","Epoch 22/50, Batch 4000/7628\n","Epoch 22/50, Batch 4100/7628\n","Epoch 22/50, Batch 4200/7628\n","Epoch 22/50, Batch 4300/7628\n","Epoch 22/50, Batch 4400/7628\n","Epoch 22/50, Batch 4500/7628\n","Epoch 22/50, Batch 4600/7628\n","Epoch 22/50, Batch 4700/7628\n","Epoch 22/50, Batch 4800/7628\n","Epoch 22/50, Batch 4900/7628\n","Epoch 22/50, Batch 5000/7628\n","Epoch 22/50, Batch 5100/7628\n","Epoch 22/50, Batch 5200/7628\n","Epoch 22/50, Batch 5300/7628\n","Epoch 22/50, Batch 5400/7628\n","Epoch 22/50, Batch 5500/7628\n","Epoch 22/50, Batch 5600/7628\n","Epoch 22/50, Batch 5700/7628\n","Epoch 22/50, Batch 5800/7628\n","Epoch 22/50, Batch 5900/7628\n","Epoch 22/50, Batch 6000/7628\n","Epoch 22/50, Batch 6100/7628\n","Epoch 22/50, Batch 6200/7628\n","Epoch 22/50, Batch 6300/7628\n","Epoch 22/50, Batch 6400/7628\n","Epoch 22/50, Batch 6500/7628\n","Epoch 22/50, Batch 6600/7628\n","Epoch 22/50, Batch 6700/7628\n","Epoch 22/50, Batch 6800/7628\n","Epoch 22/50, Batch 6900/7628\n","Epoch 22/50, Batch 7000/7628\n","Epoch 22/50, Batch 7100/7628\n","Epoch 22/50, Batch 7200/7628\n","Epoch 22/50, Batch 7300/7628\n","Epoch 22/50, Batch 7400/7628\n","Epoch 22/50, Batch 7500/7628\n","Epoch 22/50, Batch 7600/7628\n","Epoch 22/50, Loss: 0.2746\n","No improvement for 4 epoch(s)\n","Epoch 23/50, Batch 0/7628\n","Epoch 23/50, Batch 100/7628\n","Epoch 23/50, Batch 200/7628\n","Epoch 23/50, Batch 300/7628\n","Epoch 23/50, Batch 400/7628\n","Epoch 23/50, Batch 500/7628\n","Epoch 23/50, Batch 600/7628\n","Epoch 23/50, Batch 700/7628\n","Epoch 23/50, Batch 800/7628\n","Epoch 23/50, Batch 900/7628\n","Epoch 23/50, Batch 1000/7628\n","Epoch 23/50, Batch 1100/7628\n","Epoch 23/50, Batch 1200/7628\n","Epoch 23/50, Batch 1300/7628\n","Epoch 23/50, Batch 1400/7628\n","Epoch 23/50, Batch 1500/7628\n","Epoch 23/50, Batch 1600/7628\n","Epoch 23/50, Batch 1700/7628\n","Epoch 23/50, Batch 1800/7628\n","Epoch 23/50, Batch 1900/7628\n","Epoch 23/50, Batch 2000/7628\n","Epoch 23/50, Batch 2100/7628\n","Epoch 23/50, Batch 2200/7628\n","Epoch 23/50, Batch 2300/7628\n","Epoch 23/50, Batch 2400/7628\n","Epoch 23/50, Batch 2500/7628\n","Epoch 23/50, Batch 2600/7628\n","Epoch 23/50, Batch 2700/7628\n","Epoch 23/50, Batch 2800/7628\n","Epoch 23/50, Batch 2900/7628\n","Epoch 23/50, Batch 3000/7628\n","Epoch 23/50, Batch 3100/7628\n","Epoch 23/50, Batch 3200/7628\n","Epoch 23/50, Batch 3300/7628\n","Epoch 23/50, Batch 3400/7628\n","Epoch 23/50, Batch 3500/7628\n","Epoch 23/50, Batch 3600/7628\n","Epoch 23/50, Batch 3700/7628\n","Epoch 23/50, Batch 3800/7628\n","Epoch 23/50, Batch 3900/7628\n","Epoch 23/50, Batch 4000/7628\n","Epoch 23/50, Batch 4100/7628\n","Epoch 23/50, Batch 4200/7628\n","Epoch 23/50, Batch 4300/7628\n","Epoch 23/50, Batch 4400/7628\n","Epoch 23/50, Batch 4500/7628\n","Epoch 23/50, Batch 4600/7628\n","Epoch 23/50, Batch 4700/7628\n","Epoch 23/50, Batch 4800/7628\n","Epoch 23/50, Batch 4900/7628\n","Epoch 23/50, Batch 5000/7628\n","Epoch 23/50, Batch 5100/7628\n","Epoch 23/50, Batch 5200/7628\n","Epoch 23/50, Batch 5300/7628\n","Epoch 23/50, Batch 5400/7628\n","Epoch 23/50, Batch 5500/7628\n","Epoch 23/50, Batch 5600/7628\n","Epoch 23/50, Batch 5700/7628\n","Epoch 23/50, Batch 5800/7628\n","Epoch 23/50, Batch 5900/7628\n","Epoch 23/50, Batch 6000/7628\n","Epoch 23/50, Batch 6100/7628\n","Epoch 23/50, Batch 6200/7628\n","Epoch 23/50, Batch 6300/7628\n","Epoch 23/50, Batch 6400/7628\n","Epoch 23/50, Batch 6500/7628\n","Epoch 23/50, Batch 6600/7628\n","Epoch 23/50, Batch 6700/7628\n","Epoch 23/50, Batch 6800/7628\n","Epoch 23/50, Batch 6900/7628\n","Epoch 23/50, Batch 7000/7628\n","Epoch 23/50, Batch 7100/7628\n","Epoch 23/50, Batch 7200/7628\n","Epoch 23/50, Batch 7300/7628\n","Epoch 23/50, Batch 7400/7628\n","Epoch 23/50, Batch 7500/7628\n","Epoch 23/50, Batch 7600/7628\n","Epoch 23/50, Loss: 0.2754\n","No improvement for 5 epoch(s)\n","Early stopping triggered after 23 epochs\n","Best model loaded!\n","Model saved successfully!\n"]}]},{"cell_type":"markdown","source":["## Stage 3: Fine Ranking"],"metadata":{"id":"MqdbRKcMzaT6"}},{"cell_type":"markdown","source":["A sophisticated multi-task model (e.g., MMOE) precisely predicts multiple objectives—click probability, purchase likelihood, and expected GMV—using rich features and deep neural architectures, generating granular relevance scores for each candidate item."],"metadata":{"id":"sdOxhzzBzd6x"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from typing import List, Tuple\n","import numpy as np\n","\n","class Expert(nn.Module):\n","\n","    def __init__(self, input_dim: int, hidden_dim: int = 128):\n","        \"\"\"\n","        Initialize expert network.\n","\n","        Args:\n","            input_dim: Input feature dimension\n","            hidden_dim: Hidden layer dimension\n","        \"\"\"\n","\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.net(x)\n","\n","\n","class Gate(nn.Module):\n","\n","    def __init__(self, input_dim: int, num_experts: int):\n","        \"\"\"\n","        Initialize gate network.\n","        Args:\n","            input_dim: Input feature dimension\n","            num_experts: Number of experts\n","        \"\"\"\n","\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, num_experts),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.net(x)\n","\n","\n","class Tower(nn.Module):\n","\n","    def __init__(self, input_dim: int, task_type: str = 'binary'):\n","        \"\"\"\n","        Initialize tower network.\n","        Args:\n","            input_dim: Input dimension from experts\n","            task_type: Task type ('binary' or 'regression')\n","        \"\"\"\n","\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1),\n","            nn.Sigmoid() if task_type == 'binary' else nn.Identity()\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.net(x)"],"metadata":{"id":"MILuGyLOzYjG","executionInfo":{"status":"ok","timestamp":1770772187242,"user_tz":-60,"elapsed":15,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["class MMOE(nn.Module):\n","\n","    def __init__(self, input_dim: int, num_experts: int = 4, hidden_dim: int = 128):\n","        \"\"\"\n","        Initialize MMOE model.\n","        Args:\n","            input_dim: Input feature dimension\n","            num_experts: Number of expert networks\n","            hidden_dim: Hidden dimension for experts\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n","        self.gate_purchase = Gate(input_dim, num_experts)\n","        self.gate_gmv = Gate(input_dim, num_experts)\n","        self.tower_purchase = Tower(hidden_dim, 'binary')\n","        self.tower_gmv = Tower(hidden_dim, 'regression')\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Forward pass through MMOE.\n","        Args:\n","            x: Input features [batch_size, input_dim]\n","        Returns:\n","            Tuple of (purchase_prob, gmv_pred) [batch_size, 1] each\n","        \"\"\"\n","\n","        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)\n","\n","        gate_weights_purchase = self.gate_purchase(x).unsqueeze(2)\n","        purchase_input = (expert_outputs * gate_weights_purchase).sum(dim=1)\n","        purchase_prob = self.tower_purchase(purchase_input)\n","\n","        gate_weights_gmv = self.gate_gmv(x).unsqueeze(2)\n","        gmv_input = (expert_outputs * gate_weights_gmv).sum(dim=1)\n","        gmv_pred = self.tower_gmv(gmv_input)\n","\n","        return purchase_prob, gmv_pred\n","\n","    def predict(self, features: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:\n","        \"\"\"\n","        Predict scores for candidates.\n","        Args:\n","            features: Input features [num_candidates, input_dim]\n","        Returns:\n","            Tuple of (purchase_probs, gmv_preds) [num_candidates] each\n","        \"\"\"\n","\n","        self.eval()\n","        with torch.no_grad():\n","            purchase_prob, gmv_pred = self.forward(features)\n","        return purchase_prob.squeeze().cpu().numpy(), gmv_pred.squeeze().cpu().numpy()\n","\n","    def rank(self, candidate_ids: List[int], features: torch.Tensor,\n","             purchase_weight: float = 0.5, gmv_weight: float = 0.5,\n","             top_k: int = 20) -> List[int]:\n","        \"\"\"\n","        Rank candidates and return top-K.\n","        Args:\n","            candidate_ids: List of candidate item IDs\n","            features: Features for all candidates [num_candidates, input_dim]\n","            purchase_weight: Weight for purchase probability\n","            gmv_weight: Weight for GMV prediction\n","            top_k: Number of items to return\n","        Returns:\n","            Top-K candidate IDs after ranking\n","        \"\"\"\n","\n","        purchase_probs, gmv_preds = self.predict(features)\n","\n","        gmv_normalized = (gmv_preds - gmv_preds.min()) / (gmv_preds.max() - gmv_preds.min() + 1e-8)\n","        scores = purchase_weight * purchase_probs + gmv_weight * gmv_normalized\n","        ranked_indices = np.argsort(scores)[::-1][:top_k]\n","\n","        return [candidate_ids[i] for i in ranked_indices]\n","\n","\n","def train_mmoe(model: MMOE, train_loader, val_loader, num_epochs: int = 10,\n","               purchase_weight: float = 0.5, gmv_weight: float = 0.5,\n","               device: str = 'cuda'):\n","    \"\"\"\n","    Train MMOE model.\n","    Args:\n","        model: MMOE model instance\n","        train_loader: Training data loader\n","        val_loader: Validation data loader\n","        num_epochs: Number of training epochs\n","        purchase_weight: Weight for purchase task loss\n","        gmv_weight: Weight for GMV task loss\n","        device: Device to train on\n","    \"\"\"\n","\n","    model = model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    bce_loss = nn.BCELoss()\n","    mse_loss = nn.MSELoss()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        train_purchase_loss = 0.0\n","        train_gmv_loss = 0.0\n","\n","        for features, purchase_labels, gmv_labels in train_loader:\n","            features = features.to(device)\n","            purchase_labels = purchase_labels.to(device)\n","            gmv_labels = gmv_labels.to(device)\n","\n","            purchase_prob, gmv_pred = model(features)\n","\n","            loss_purchase = bce_loss(purchase_prob.squeeze(), purchase_labels.float())\n","            loss_gmv = mse_loss(gmv_pred.squeeze(), gmv_labels.float())\n","            loss = purchase_weight * loss_purchase + gmv_weight * loss_gmv\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            train_purchase_loss += loss_purchase.item()\n","            train_gmv_loss += loss_gmv.item()\n","\n","        model.eval()\n","        val_loss = 0.0\n","        val_purchase_loss = 0.0\n","        val_gmv_loss = 0.0\n","\n","        with torch.no_grad():\n","            for features, purchase_labels, gmv_labels in val_loader:\n","                features = features.to(device)\n","                purchase_labels = purchase_labels.to(device)\n","                gmv_labels = gmv_labels.to(device)\n","\n","                purchase_prob, gmv_pred = model(features)\n","\n","                loss_purchase = bce_loss(purchase_prob.squeeze(), purchase_labels.float())\n","                loss_gmv = mse_loss(gmv_pred.squeeze(), gmv_labels.float())\n","                loss = purchase_weight * loss_purchase + gmv_weight * loss_gmv\n","\n","                val_loss += loss.item()\n","                val_purchase_loss += loss_purchase.item()\n","                val_gmv_loss += loss_gmv.item()\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(f\"Train - Total: {train_loss/len(train_loader):.4f}, Purchase: {train_purchase_loss/len(train_loader):.4f}, GMV: {train_gmv_loss/len(train_loader):.4f}\")\n","        print(f\"Val   - Total: {val_loss/len(val_loader):.4f}, Purchase: {val_purchase_loss/len(val_loader):.4f}, GMV: {val_gmv_loss/len(val_loader):.4f}\")\n","        print(\"-\" * 80)"],"metadata":{"id":"056siOyUNs90","executionInfo":{"status":"ok","timestamp":1770772189798,"user_tz":-60,"elapsed":7,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import psycopg2\n","import os\n","\n","# 0. Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# 1. Load training data from database\n","try:\n","    with psycopg2.connect(\n","        host=os.getenv('DB_HOST'),\n","        port=os.getenv('DB_PORT'),\n","        dbname=os.getenv('DB_NAME'),\n","        user=os.getenv('DB_USER'),\n","        password=os.getenv('DB_PASSWORD'),\n","        sslmode='require'\n","    ) as conn:\n","        with conn.cursor() as cursor:\n","            cursor.execute(\"\"\"\n","            SELECT \"ClientID\", \"ProductID\", \"PurchaseCount\", \"TotalSpent\"\n","            FROM user_item_interactions\n","            \"\"\")\n","            interaction_data = cursor.fetchall()\n","\n","            cursor.execute(\"\"\"\n","            SELECT \"ClientID\", \"TotalPurchases\", \"TotalSpendEuro\", \"AvgOrderValue\",\n","                   \"DaysSinceLastPurchase\", \"PurchaseFrequency\", \"UniqueProductsBought\",\n","                   \"TotalQuantity\", \"Age\"\n","            FROM clients\n","            \"\"\")\n","            user_features_data = cursor.fetchall()\n","\n","            cursor.execute(\"\"\"\n","            SELECT \"ProductID\", \"TotalSales\", \"TotalQuantitySold\", \"Sales7d\",\n","                   \"Sales30d\", \"AvgPrice\", \"TotalRevenue\", \"UniqueBuyers\",\n","                   \"AvgQuantityPerOrder\", \"TotalStockQuantity\", \"StockCountries\"\n","            FROM products\n","            \"\"\")\n","            item_features_data = cursor.fetchall()\n","\n","            print(\"Training data loaded successfully!\")\n","except Exception as e:\n","    print(f\"Error loading training data: {e}\")\n","    raise\n","\n","# 2. Prepare features\n","user_feature_dict = {uid: [0 if x is None else float(x) for x in features]\n","                     for uid, *features in user_features_data}\n","item_feature_dict = {iid: [0 if x is None else float(x) for x in features]\n","                     for iid, *features in item_features_data}\n","\n","user_feature_dim = len(user_features_data[0]) - 1\n","item_feature_dim = len(item_features_data[0]) - 1\n","feature_dim = user_feature_dim + item_feature_dim\n","\n","print(f\"User feature dimension: {user_feature_dim}\")\n","print(f\"Item feature dimension: {item_feature_dim}\")\n","print(f\"Total feature dimension: {feature_dim}\")\n","\n","# 3. Prepare training samples\n","print(\"Preparing training samples...\")\n","training_samples = []\n","\n","for user_id, item_id, purchase_count, total_spent in interaction_data:\n","    if user_id not in user_feature_dict or item_id not in item_feature_dict:\n","        continue\n","\n","    user_feat = user_feature_dict[user_id]\n","    item_feat = item_feature_dict[item_id]\n","    combined_feat = user_feat + item_feat\n","\n","    purchase_label = 1 if purchase_count > 0 else 0\n","    gmv_value = 0 if total_spent is None else float(total_spent)\n","\n","    training_samples.append((combined_feat, purchase_label, gmv_value))\n","\n","print(f\"Total training samples: {len(training_samples)}\")\n","\n","# 4. Split train and validation sets\n","np.random.shuffle(training_samples)\n","split_idx = int(0.8 * len(training_samples))\n","train_samples = training_samples[:split_idx]\n","val_samples = training_samples[split_idx:]\n","\n","print(f\"Train samples: {len(train_samples)}\")\n","print(f\"Validation samples: {len(val_samples)}\")\n","\n","# 5. Prepare tensors\n","features_train = torch.FloatTensor([feat for feat, _, _ in train_samples])\n","purchase_labels_train = torch.FloatTensor([label for _, label, _ in train_samples])\n","gmv_labels_train = torch.FloatTensor([gmv for _, _, gmv in train_samples])\n","\n","train_dataset = TensorDataset(features_train, purchase_labels_train, gmv_labels_train)\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n","\n","features_val = torch.FloatTensor([feat for feat, _, _ in val_samples])\n","purchase_labels_val = torch.FloatTensor([label for _, label, _ in val_samples])\n","gmv_labels_val = torch.FloatTensor([gmv for _, _, gmv in val_samples])\n","\n","val_dataset = TensorDataset(features_val, purchase_labels_val, gmv_labels_val)\n","val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n","\n","# 6. Initialize model\n","model = MMOE(input_dim=feature_dim, num_experts=4, hidden_dim=128)\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","bce_loss = nn.BCELoss()\n","mse_loss = nn.MSELoss()\n","\n","# 7. Training loop\n","epochs = 50\n","patience = 5\n","best_loss = float('inf')\n","patience_counter = 0\n","purchase_weight = 0.5\n","gmv_weight = 0.5\n","\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    train_purchase_loss = 0.0\n","    train_gmv_loss = 0.0\n","\n","    for features, purchase_labels, gmv_labels in train_loader:\n","        features = features.to(device)\n","        purchase_labels = purchase_labels.to(device)\n","        gmv_labels = gmv_labels.to(device)\n","\n","        purchase_prob, gmv_pred = model(features)\n","\n","        loss_purchase = bce_loss(purchase_prob.squeeze(), purchase_labels.float())\n","        loss_gmv = mse_loss(gmv_pred.squeeze(), gmv_labels.float())\n","        loss = purchase_weight * loss_purchase + gmv_weight * loss_gmv\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        train_purchase_loss += loss_purchase.item()\n","        train_gmv_loss += loss_gmv.item()\n","\n","    model.eval()\n","    val_loss = 0.0\n","    val_purchase_loss = 0.0\n","    val_gmv_loss = 0.0\n","\n","    with torch.no_grad():\n","        for features, purchase_labels, gmv_labels in val_loader:\n","            features = features.to(device)\n","            purchase_labels = purchase_labels.to(device)\n","            gmv_labels = gmv_labels.to(device)\n","\n","            purchase_prob, gmv_pred = model(features)\n","\n","            loss_purchase = bce_loss(purchase_prob.squeeze(), purchase_labels.float())\n","            loss_gmv = mse_loss(gmv_pred.squeeze(), gmv_labels.float())\n","            loss = purchase_weight * loss_purchase + gmv_weight * loss_gmv\n","\n","            val_loss += loss.item()\n","            val_purchase_loss += loss_purchase.item()\n","            val_gmv_loss += loss_gmv.item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    print(f\"Train - Total: {train_loss/len(train_loader):.4f}, Purchase: {train_purchase_loss/len(train_loader):.4f}, GMV: {train_gmv_loss/len(train_loader):.4f}\")\n","    print(f\"Val   - Total: {avg_val_loss:.4f}, Purchase: {val_purchase_loss/len(val_loader):.4f}, GMV: {val_gmv_loss/len(val_loader):.4f}\")\n","\n","    # Early stopping\n","    if avg_val_loss < best_loss:\n","        best_loss = avg_val_loss\n","        patience_counter = 0\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'feature_dim': feature_dim,\n","            'num_experts': 4,\n","            'hidden_dim': 128\n","        }, 'mmoe_model_best.pth')\n","        print(f\"Best model saved with loss: {best_loss:.4f}\")\n","    else:\n","        patience_counter += 1\n","        print(f\"No improvement for {patience_counter} epoch(s)\")\n","\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n","            break\n","\n","    print(\"-\" * 80)\n","\n","# 8. Load best model\n","checkpoint = torch.load('mmoe_model_best.pth')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(\"Best model loaded!\")\n","\n","# 9. Save final model\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'feature_dim': feature_dim,\n","    'num_experts': 4,\n","    'hidden_dim': 128\n","}, 'mmoe_model.pth')\n","print(\"Model saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FY6bSrqOAqP","executionInfo":{"status":"ok","timestamp":1770772943809,"user_tz":-60,"elapsed":218710,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"b730073d-a7b3-45a2-de58-740479ad3ce8"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Training data loaded successfully!\n","User feature dimension: 8\n","Item feature dimension: 10\n","Total feature dimension: 18\n","Preparing training samples...\n","Total training samples: 976274\n","Train samples: 781019\n","Validation samples: 195255\n","Epoch 1/50\n","Train - Total: 65809.1628, Purchase: 0.0000, GMV: 131618.3256\n","Val   - Total: 10299.8049, Purchase: 0.0000, GMV: 20599.6099\n","Best model saved with loss: 10299.8049\n","--------------------------------------------------------------------------------\n","Epoch 2/50\n","Train - Total: 5840.9278, Purchase: 0.0000, GMV: 11681.8557\n","Val   - Total: 3910.7425, Purchase: 0.0000, GMV: 7821.4850\n","Best model saved with loss: 3910.7425\n","--------------------------------------------------------------------------------\n","Epoch 3/50\n","Train - Total: 6602.3654, Purchase: 0.0000, GMV: 13204.7308\n","Val   - Total: 4636.6761, Purchase: 0.0000, GMV: 9273.3522\n","No improvement for 1 epoch(s)\n","--------------------------------------------------------------------------------\n","Epoch 4/50\n","Train - Total: 41207.0761, Purchase: 0.0000, GMV: 82414.1522\n","Val   - Total: 8458.7378, Purchase: 0.0000, GMV: 16917.4756\n","No improvement for 2 epoch(s)\n","--------------------------------------------------------------------------------\n","Epoch 5/50\n","Train - Total: 17700.2873, Purchase: 0.0000, GMV: 35400.5746\n","Val   - Total: 7311.1915, Purchase: 0.0000, GMV: 14622.3829\n","No improvement for 3 epoch(s)\n","--------------------------------------------------------------------------------\n","Epoch 6/50\n","Train - Total: 77020.1774, Purchase: 0.0000, GMV: 154040.3547\n","Val   - Total: 5206.2829, Purchase: 0.0000, GMV: 10412.5657\n","No improvement for 4 epoch(s)\n","--------------------------------------------------------------------------------\n","Epoch 7/50\n","Train - Total: 9101.3969, Purchase: 0.0000, GMV: 18202.7939\n","Val   - Total: 9658.5114, Purchase: 0.0000, GMV: 19317.0228\n","No improvement for 5 epoch(s)\n","Early stopping triggered after 7 epochs\n","Best model loaded!\n","Model saved successfully!\n"]}]},{"cell_type":"markdown","source":["## Stage 4: Re-ranking"],"metadata":{"id":"dQ1DC3A9z9K4"}},{"cell_type":"markdown","source":["Post-processing refines the ranked list by applying diversity constraints, business rules, and fairness policies through algorithms like MMR or DPP, transforming a purely relevance-driven ordering into a well-balanced and engaging presentation that maximizes both overall user experience and strategic business objectives."],"metadata":{"id":"oAR-yVrj0Blt"}},{"cell_type":"code","source":["from typing import List, Dict\n","import psycopg2\n","\n","class Reranking:\n","\n","    def __init__(self, db_config: Dict[str, str]):\n","        \"\"\"\n","        Initialize reranking module.\n","\n","        Args:\n","            db_config: Database connection configuration\n","        \"\"\"\n","\n","        self.db_config = db_config\n","        self.product_stocks = {}\n","        self.country_stocks = {}\n","\n","    def load_stock_data(self):\n","        \"\"\"\n","        Load stock information from database.\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    # Load total stock per product\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ProductID\", \"TotalStockQuantity\"\n","                    FROM products\n","                    WHERE \"TotalStockQuantity\" > 0\n","                    \"\"\")\n","                    self.product_stocks = {product_id: stock for product_id, stock in cursor.fetchall()}\n","\n","                    # Load country-specific stock\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ProductID\", \"StoreCountry\", \"Quantity\"\n","                    FROM stocks\n","                    WHERE \"Quantity\" > 0\n","                    \"\"\")\n","                    for product_id, country, quantity in cursor.fetchall():\n","                        if product_id not in self.country_stocks:\n","                            self.country_stocks[product_id] = {}\n","                        self.country_stocks[product_id][country] = quantity\n","\n","                    print(\"Stock data loaded successfully!\")\n","        except Exception as e:\n","            print(f\"Error loading stock data: {e}\")\n","\n","    def rerank(self, candidate_ids: List[int], scores: List[float],\n","               user_country: str, stock_weight: float = 0.3,\n","               country_weight: float = 0.4) -> List[int]:\n","        \"\"\"\n","        Rerank candidates based on stock availability and country preference.\n","        Args:\n","            candidate_ids: List of candidate item IDs\n","            scores: Original ranking scores for each candidate\n","            user_country: User's country code\n","            stock_weight: Weight for total stock quantity\n","            country_weight: Weight for country-specific stock\n","        Returns:\n","            Reranked list of item IDs\n","        \"\"\"\n","\n","        reranked_items = []\n","\n","        for idx, item_id in enumerate(candidate_ids):\n","            original_score = scores[idx]\n","\n","            # Stock boost\n","            stock_boost = 0.0\n","            if item_id in self.product_stocks:\n","                total_stock = self.product_stocks[item_id]\n","                stock_boost = min(total_stock / 100.0, 1.0) * stock_weight\n","\n","            # Country stock boost\n","            country_boost = 0.0\n","            if item_id in self.country_stocks and user_country in self.country_stocks[item_id]:\n","                country_stock = self.country_stocks[item_id][user_country]\n","                country_boost = min(country_stock / 50.0, 1.0) * country_weight\n","\n","            # Combined score\n","            final_score = original_score + stock_boost + country_boost\n","\n","            reranked_items.append((item_id, final_score))\n","\n","        # Sort by final score descending\n","        reranked_items.sort(key=lambda x: x[1], reverse=True)\n","\n","        return [item_id for item_id, score in reranked_items]"],"metadata":{"id":"gH32zVk10A3y","executionInfo":{"status":"ok","timestamp":1770775621608,"user_tz":-60,"elapsed":42,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: End-End Pipeline"],"metadata":{"id":"lz9Z2RhReEgA"}},{"cell_type":"code","source":["class RecommendationPipeline:\n","\n","    def __init__(self, db_config: Dict[str, str]):\n","        \"\"\"\n","        Initialize recommendation pipeline.\n","        Args:\n","            db_config: Database connection configuration dictionary\n","        \"\"\"\n","\n","        self.db_config = db_config\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        self._load_models()\n","        self.item_embeddings, self.item_id_to_idx = self._load_item_embeddings()\n","\n","    def _load_models(self):\n","        \"\"\"Load all trained models.\"\"\"\n","\n","        print(\"Loading models...\")\n","\n","        # Load two-tower model\n","        two_tower_checkpoint = torch.load('two_tower_model.pth', map_location=self.device)\n","        self.two_tower = TwoTowerRecall(\n","            user_feature_dim=8,\n","            item_feature_dim=10,\n","            embedding_dim=64\n","        )\n","        self.two_tower.user_tower.load_state_dict(two_tower_checkpoint['user_tower'])\n","        self.two_tower.item_tower.load_state_dict(two_tower_checkpoint['item_tower'])\n","        self.two_tower.user_tower = self.two_tower.user_tower.to(self.device)\n","        self.two_tower.item_tower = self.two_tower.item_tower.to(self.device)\n","\n","        # Load ItemCF model\n","        self.item_cf = ItemCF(self.db_config)\n","        self.item_cf.load_from_db()\n","\n","        # Load UserCF model\n","        self.user_cf = UserCF(self.db_config)\n","        self.user_cf.load_from_db()\n","\n","        # Load global popular model\n","        self.global_popular = GlobalPopular(self.db_config)\n","        self.global_popular.load_from_db()\n","\n","        # Load category popular model\n","        self.category_popular = CategoryPopular(self.db_config)\n","        self.category_popular.load_from_db()\n","\n","        # Load coarse ranking model\n","        coarse_checkpoint = torch.load('coarse_ranking_model.pth', map_location=self.device)\n","        self.coarse_ranker = CoarseRanking(\n","            feature_dim=coarse_checkpoint['feature_dim'],\n","            hidden_dim=64\n","        )\n","        self.coarse_ranker.model.load_state_dict(coarse_checkpoint['model'])\n","        self.coarse_ranker.model = self.coarse_ranker.model.to(self.device)\n","\n","        # Load MMOE model\n","        mmoe_checkpoint = torch.load('mmoe_model.pth', map_location=self.device)\n","        self.fine_ranker = MMOE(\n","            input_dim=mmoe_checkpoint['feature_dim'],\n","            num_experts=4,\n","            hidden_dim=128\n","        )\n","        self.fine_ranker.load_state_dict(mmoe_checkpoint['model_state_dict'])\n","        self.fine_ranker = self.fine_ranker.to(self.device)\n","\n","        # Load reranking model\n","        self.reranker = Reranking(self.db_config)\n","        self.reranker.load_stock_data()\n","\n","        print(\"All models loaded successfully!\")\n","\n","    def _load_item_embeddings(self):\n","        \"\"\"Load item embeddings from database.\"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute('SELECT \"ProductID\", \"Embedding\" FROM item_embeddings')\n","                    item_data = cursor.fetchall()\n","\n","                    item_id_to_idx = {pid: i for i, (pid, _) in enumerate(item_data)}\n","                    item_embeddings = np.array([emb for _, emb in item_data])\n","\n","                    print(\"Item embeddings loaded successfully!\")\n","                    return item_embeddings, item_id_to_idx\n","        except Exception as e:\n","            print(f\"Error loading item embeddings: {e}\")\n","            return np.array([]), {}\n","\n","    def _get_user_features(self, user_id: int) -> List[float]:\n","        \"\"\"\n","        Get user features from database.\n","        Args:\n","            user_id: User ID\n","        Returns:\n","            List of user feature values\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"TotalPurchases\", \"TotalSpendEuro\", \"AvgOrderValue\",\n","                           \"DaysSinceLastPurchase\", \"PurchaseFrequency\",\n","                           \"UniqueProductsBought\", \"TotalQuantity\", \"Age\"\n","                    FROM clients WHERE \"ClientID\" = %s\n","                    \"\"\", (user_id,))\n","\n","                    result = cursor.fetchone()\n","                    return [0 if x is None else float(x) for x in result] if result else [0] * 8\n","        except Exception as e:\n","            print(f\"Error loading user features: {e}\")\n","            return [0] * 8\n","\n","    def _get_item_features(self, item_ids: List[int]) -> torch.Tensor:\n","        \"\"\"\n","        Get item features from database.\n","        Args:\n","            item_ids: List of item IDs\n","        Returns:\n","            Tensor of item features [num_items, item_feature_dim]\n","        \"\"\"\n","\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"TotalSales\", \"TotalQuantitySold\", \"Sales7d\", \"Sales30d\",\n","                           \"AvgPrice\", \"TotalRevenue\", \"UniqueBuyers\",\n","                           \"AvgQuantityPerOrder\", \"TotalStockQuantity\", \"StockCountries\"\n","                    FROM products WHERE \"ProductID\" = ANY(%s)\n","                    \"\"\", (item_ids,))\n","\n","                    features = [[0 if x is None else float(x) for x in row] for row in cursor.fetchall()]\n","                    return torch.tensor(features, dtype=torch.float32)\n","        except Exception as e:\n","            print(f\"Error loading item features: {e}\")\n","            return torch.zeros(len(item_ids), 10)\n","\n","    def _get_combined_features(self, user_id: int, item_ids: List[int]) -> torch.Tensor:\n","        \"\"\"\n","        Get combined user-item features for ranking.\n","        Args:\n","            user_id: User ID\n","            item_ids: List of item IDs\n","        Returns:\n","            Combined feature tensor [num_items, user_feature_dim + item_feature_dim]\n","        \"\"\"\n","\n","        user_features = self._get_user_features(user_id)\n","        item_features = self._get_item_features(item_ids)\n","\n","        user_tensor = torch.tensor([user_features] * len(item_ids), dtype=torch.float32)\n","        combined = torch.cat([user_tensor, item_features], dim=1)\n","\n","        return combined.to(self.device)\n","\n","    def recommend(self, user_id: int, top_k: int = 20) -> List[int]:\n","        \"\"\"\n","        Generate recommendations for a user.\n","        Args:\n","            user_id: User ID\n","            top_k: Number of final recommendations\n","        Returns:\n","            List of recommended item IDs\n","        \"\"\"\n","\n","        print(f\"\\n{'='*50}\")\n","        print(f\"Generating recommendations for User {user_id}\")\n","        print(f\"{'='*50}\\n\")\n","\n","        # Load user data from database\n","        try:\n","            with psycopg2.connect(**self.db_config) as conn:\n","                with conn.cursor() as cursor:\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ClientCountry\", \"TopCategory1\", \"TopCategory2\", \"TopCategory3\"\n","                    FROM clients WHERE \"ClientID\" = %s\n","                    \"\"\", (user_id,))\n","                    result = cursor.fetchone()\n","\n","                    if not result:\n","                        print(\"User not found!\")\n","                        return []\n","\n","                    user_country = result[0]\n","                    user_categories = [cat for cat in result[1:] if cat]\n","\n","                    cursor.execute(\"\"\"\n","                    SELECT \"ProductID\" FROM user_item_interactions WHERE \"ClientID\" = %s\n","                    \"\"\", (user_id,))\n","                    user_purchased = [row[0] for row in cursor.fetchall()]\n","        except Exception as e:\n","            print(f\"Error loading user data: {e}\")\n","            return []\n","\n","        # Get user features for two-tower\n","        user_features = self._get_user_features(user_id)\n","\n","        # Stage 1: Multi-channel recall\n","        print(\"Stage 1: Multi-channel Recall\")\n","        print(\"-\" * 50)\n","\n","        candidates = set()\n","\n","        # Recall from two-tower model\n","        user_tensor = torch.FloatTensor([user_features]).to(self.device)\n","        user_embedding = self.two_tower.encode_users(user_tensor)[0]\n","        two_tower_idx = self.two_tower.recall(user_embedding, self.item_embeddings, top_k=500)\n","        two_tower_items = [list(self.item_id_to_idx.keys())[idx] for idx in two_tower_idx]\n","        candidates.update(two_tower_items)\n","        print(f\"TwoTower recalled: {len(two_tower_items)} candidates\")\n","\n","        # Recall from ItemCF model\n","        itemcf_items = self.item_cf.recall(user_purchased, top_k=300)\n","        candidates.update(itemcf_items)\n","        print(f\"ItemCF recalled: {len(itemcf_items)} candidates\")\n","\n","        # Recall from UserCF model\n","        usercf_items = self.user_cf.recall(user_id, user_purchased, top_k=200)\n","        candidates.update(usercf_items)\n","        print(f\"UserCF recalled: {len(usercf_items)} candidates\")\n","\n","        # Recall from global popular\n","        global_items = self.global_popular.recall(top_k=200)\n","        candidates.update(global_items)\n","        print(f\"GlobalPopular recalled: {len(global_items)} candidates\")\n","\n","        # Recall from category popular\n","        category_items = self.category_popular.recall(user_categories, top_k=200)\n","        candidates.update(category_items)\n","        print(f\"CategoryPopular recalled: {len(category_items)} candidates\")\n","\n","        # Remove purchased items\n","        candidates = list(candidates - set(user_purchased))\n","        print(f\"Total unique candidates: {len(candidates)}\")\n","\n","        # Stage 2: Coarse ranking\n","        print(\"\\nStage 2: Coarse Ranking\")\n","        print(\"-\" * 50)\n","\n","        candidate_features = self._get_combined_features(user_id, candidates)\n","        coarse_candidates = self.coarse_ranker.rank(candidates, candidate_features, top_k=500)\n","        print(f\"Coarse ranking kept: {len(coarse_candidates)} candidates\")\n","\n","        # Stage 3: Fine ranking\n","        print(\"\\nStage 3: Fine Ranking\")\n","        print(\"-\" * 50)\n","\n","        fine_features = self._get_combined_features(user_id, coarse_candidates)\n","        purchase_probs, gmv_preds = self.fine_ranker.predict(fine_features)\n","\n","        gmv_normalized = (gmv_preds - gmv_preds.min()) / (gmv_preds.max() - gmv_preds.min() + 1e-8)\n","        scores = 0.5 * purchase_probs + 0.5 * gmv_normalized\n","\n","        ranked_indices = np.argsort(scores)[::-1][:100]\n","        fine_candidates = [coarse_candidates[i] for i in ranked_indices]\n","        fine_scores = [scores[i] for i in ranked_indices]\n","        print(f\"Fine ranking kept: {len(fine_candidates)} candidates\")\n","\n","        # Stage 4: Reranking\n","        print(\"\\nStage 4: Reranking\")\n","        print(\"-\" * 50)\n","\n","        final_items = self.reranker.rerank(fine_candidates, fine_scores, user_country)[:top_k]\n","        print(f\"Reranking kept: {len(final_items)} candidates\")\n","\n","        print(f\"\\n{'='*50}\")\n","        print(f\"Final recommendations: {final_items}\")\n","        print(f\"{'='*50}\\n\")\n","\n","        return final_items"],"metadata":{"id":"vlpI2O4keG0Y","executionInfo":{"status":"ok","timestamp":1770778063154,"user_tz":-60,"elapsed":21,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","if __name__ == \"__main__\":\n","    db_config = {\n","        'host': os.getenv('DB_HOST'),\n","        'port': os.getenv('DB_PORT'),\n","        'dbname': os.getenv('DB_NAME'),\n","        'user': os.getenv('DB_USER'),\n","        'password': os.getenv('DB_PASSWORD'),\n","        'sslmode': 'require'\n","    }\n","\n","    # Initialize pipeline (loads all models automatically)\n","    pipeline = RecommendationPipeline(db_config)\n","\n","    # Generate recommendations\n","    user_id = 2820558652430377474\n","    recommendations = pipeline.recommend(user_id, top_k=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejVKUSdbell_","executionInfo":{"status":"ok","timestamp":1770778110170,"user_tz":-60,"elapsed":45948,"user":{"displayName":"Yuhong Li","userId":"05007242337654446538"}},"outputId":"0b36e623-e72d-4fc2-a04d-fa233ee17bc9"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading models...\n","Item similarity loaded successfully! Total items: 6873\n","User similarity loaded successfully! Total users: 43766\n","Loaded 1000 popular items!\n","Loaded popular items for 16 categories!\n","Stock data loaded successfully!\n","All models loaded successfully!\n","Item embeddings loaded successfully!\n","\n","==================================================\n","Generating recommendations for User 2820558652430377474\n","==================================================\n","\n","Stage 1: Multi-channel Recall\n","--------------------------------------------------\n","TwoTower recalled: 500 candidates\n","ItemCF recalled: 0 candidates\n","UserCF recalled: 0 candidates\n","GlobalPopular recalled: 200 candidates\n","CategoryPopular recalled: 200 candidates\n","Total unique candidates: 880\n","\n","Stage 2: Coarse Ranking\n","--------------------------------------------------\n","Coarse ranking kept: 500 candidates\n","\n","Stage 3: Fine Ranking\n","--------------------------------------------------\n","Fine ranking kept: 100 candidates\n","\n","Stage 4: Reranking\n","--------------------------------------------------\n","Reranking kept: 20 candidates\n","\n","==================================================\n","Final recommendations: [2574451597753930800, 6804252697652906339, 5929926235917173905, 1607208898116947879, 4978781552851483666, 7191926660451464548, 2808526412659237093, 3810987776110928266, 6395953806864235238, 8863088790624591833, 726448763633718668, 4158341377090031082, 4628479610125335225, 9042774209429765554, 1899868537842038681, 4149518240723963838, 4211864033374941039, 2065347138986255224, 2658511236566114333, 6443490232743848883]\n","==================================================\n","\n"]}]}]}